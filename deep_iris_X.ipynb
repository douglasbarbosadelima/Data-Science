{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "deep_iris.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douglasbarbosadelima/Data-Science/blob/master/deep_iris_X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFFlrJEyR_iN",
        "colab_type": "text"
      },
      "source": [
        "Classificação Multiclasse com Iris e TF 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ehhQFEK449e",
        "colab_type": "text"
      },
      "source": [
        "Parte 1: montando e calibrando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLY0Ut32SMMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "989c73c4-dde1-464b-b604-562179ffeba5"
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0 #com GPU"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.8)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.17.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.27.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (45.1.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/61/a836d1d71a87ca4e7034a9f9ac06eab288c403ecc611432a508edd908f8c/google_auth-1.11.1-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed google-auth-1.11.1 tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jte8_L_eTN26",
        "colab_type": "code",
        "outputId": "31c29712-4225-4553-e873-100a95398923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1vkzIf8Y3eP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07d9d275-ad82-45cc-df9e-c9d43a8d0233"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X = iris.data\n",
        "X"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq3WFyBxY3eW",
        "colab_type": "code",
        "outputId": "eba0677d-4a5d-414d-bcc5-12499d87b27c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = iris.target\n",
        "y[:10]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9z8YRhTY3ee",
        "colab_type": "code",
        "outputId": "7553a6b0-2d1a-4375-c728-6e86ab044d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehot = OneHotEncoder()\n",
        "y.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEHyEASzY3eh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ac92277-4621-4a1c-af6f-7e1132802d7e"
      },
      "source": [
        "y = y.reshape((150, 1))\n",
        "y"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2],\n",
              "       [2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMpv9Vg4Y3el",
        "colab_type": "code",
        "outputId": "0cd5be30-363c-4931-d557-9433e29b3e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y = onehot.fit_transform(y).toarray()\n",
        "y[:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKeMwtJbY3eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PbP4TDkY3e3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#crie o modelo com duas hidden layers de 4 perceptrons, relu e 3 outputs\n",
        "#com softmax \n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(3, activation='relu', input_shape=(4,)),\n",
        "  tf.keras.layers.Dense(3, activation='relu'),\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#continue o código"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmzH-Rh42JN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "28902e1e-1654-48e0-f34d-32504cd0b6d9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 6         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 9         \n",
            "=================================================================\n",
            "Total params: 25\n",
            "Trainable params: 25\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXrntr4DZvvQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b8c41bb6-38a3-4dc6-8a6f-b706094d6096"
      },
      "source": [
        "ephocs = 1000\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "hist=model.fit(x=X_train,y=y_train, validation_data=(X_test, y_test),epochs=ephocs)\n",
        "hist"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 45 samples\n",
            "Epoch 1/1000\n",
            "105/105 [==============================] - 1s 5ms/sample - loss: 2.8720 - accuracy: 0.3333 - val_loss: 3.2687 - val_accuracy: 0.3333\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 2.7836 - accuracy: 0.3333 - val_loss: 3.1764 - val_accuracy: 0.3333\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 2.7013 - accuracy: 0.3333 - val_loss: 3.0831 - val_accuracy: 0.3556\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 2.6134 - accuracy: 0.3333 - val_loss: 2.9882 - val_accuracy: 0.3556\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 0s 320us/sample - loss: 2.5346 - accuracy: 0.3619 - val_loss: 2.8893 - val_accuracy: 0.4000\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 2.4400 - accuracy: 0.3619 - val_loss: 2.7915 - val_accuracy: 0.4000\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 0s 303us/sample - loss: 2.3535 - accuracy: 0.4190 - val_loss: 2.6891 - val_accuracy: 0.4444\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 2.2660 - accuracy: 0.5333 - val_loss: 2.5860 - val_accuracy: 0.5111\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 2.1822 - accuracy: 0.5905 - val_loss: 2.4820 - val_accuracy: 0.5556\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 2.0993 - accuracy: 0.6095 - val_loss: 2.3801 - val_accuracy: 0.5556\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 2.0089 - accuracy: 0.6571 - val_loss: 2.2813 - val_accuracy: 0.5556\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 1.9248 - accuracy: 0.6571 - val_loss: 2.1824 - val_accuracy: 0.5556\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 0s 312us/sample - loss: 1.8391 - accuracy: 0.6286 - val_loss: 2.0827 - val_accuracy: 0.5111\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 0s 351us/sample - loss: 1.7621 - accuracy: 0.5143 - val_loss: 1.9826 - val_accuracy: 0.4889\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 1.6821 - accuracy: 0.4571 - val_loss: 1.8887 - val_accuracy: 0.4000\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 1.6067 - accuracy: 0.3810 - val_loss: 1.7987 - val_accuracy: 0.3778\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 1.5334 - accuracy: 0.3524 - val_loss: 1.7127 - val_accuracy: 0.3556\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 1.4674 - accuracy: 0.3333 - val_loss: 1.6321 - val_accuracy: 0.3333\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 1.3999 - accuracy: 0.3333 - val_loss: 1.5579 - val_accuracy: 0.3333\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 1.3384 - accuracy: 0.3333 - val_loss: 1.4895 - val_accuracy: 0.3333\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 1.2850 - accuracy: 0.3333 - val_loss: 1.4259 - val_accuracy: 0.3333\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 1.2360 - accuracy: 0.3333 - val_loss: 1.3659 - val_accuracy: 0.3333\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 1.1914 - accuracy: 0.3333 - val_loss: 1.3121 - val_accuracy: 0.3333\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 1.1487 - accuracy: 0.3333 - val_loss: 1.2621 - val_accuracy: 0.3333\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 1.1139 - accuracy: 0.3333 - val_loss: 1.2132 - val_accuracy: 0.3333\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 1.0781 - accuracy: 0.3333 - val_loss: 1.1674 - val_accuracy: 0.3333\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 1.0466 - accuracy: 0.3333 - val_loss: 1.1252 - val_accuracy: 0.3333\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 1.0196 - accuracy: 0.3333 - val_loss: 1.0881 - val_accuracy: 0.3333\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.9998 - accuracy: 0.3333 - val_loss: 1.0576 - val_accuracy: 0.3333\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 0.9881 - accuracy: 0.3333 - val_loss: 1.0368 - val_accuracy: 0.3333\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.9778 - accuracy: 0.3333 - val_loss: 1.0212 - val_accuracy: 0.3333\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.9702 - accuracy: 0.3333 - val_loss: 1.0087 - val_accuracy: 0.3333\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.9654 - accuracy: 0.3333 - val_loss: 0.9980 - val_accuracy: 0.3333\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.9608 - accuracy: 0.3333 - val_loss: 0.9903 - val_accuracy: 0.3333\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 0s 303us/sample - loss: 0.9579 - accuracy: 0.3333 - val_loss: 0.9841 - val_accuracy: 0.3333\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 0s 297us/sample - loss: 0.9543 - accuracy: 0.3333 - val_loss: 0.9791 - val_accuracy: 0.3333\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.9514 - accuracy: 0.3333 - val_loss: 0.9742 - val_accuracy: 0.3333\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.9489 - accuracy: 0.3429 - val_loss: 0.9701 - val_accuracy: 0.3333\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 0s 346us/sample - loss: 0.9464 - accuracy: 0.3429 - val_loss: 0.9672 - val_accuracy: 0.3333\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 0s 324us/sample - loss: 0.9438 - accuracy: 0.3429 - val_loss: 0.9650 - val_accuracy: 0.3333\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.9414 - accuracy: 0.3524 - val_loss: 0.9620 - val_accuracy: 0.3333\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.9386 - accuracy: 0.3524 - val_loss: 0.9591 - val_accuracy: 0.3333\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.9362 - accuracy: 0.3524 - val_loss: 0.9564 - val_accuracy: 0.3333\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.9338 - accuracy: 0.3619 - val_loss: 0.9536 - val_accuracy: 0.3333\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 0.9319 - accuracy: 0.3714 - val_loss: 0.9507 - val_accuracy: 0.3556\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.9294 - accuracy: 0.3714 - val_loss: 0.9489 - val_accuracy: 0.3556\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.9272 - accuracy: 0.3714 - val_loss: 0.9469 - val_accuracy: 0.3556\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.9249 - accuracy: 0.3714 - val_loss: 0.9450 - val_accuracy: 0.3556\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.9228 - accuracy: 0.3714 - val_loss: 0.9428 - val_accuracy: 0.3556\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.9206 - accuracy: 0.3905 - val_loss: 0.9413 - val_accuracy: 0.3556\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.9184 - accuracy: 0.4000 - val_loss: 0.9396 - val_accuracy: 0.3778\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.9162 - accuracy: 0.4095 - val_loss: 0.9386 - val_accuracy: 0.3778\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.9142 - accuracy: 0.4190 - val_loss: 0.9373 - val_accuracy: 0.3778\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.9120 - accuracy: 0.4571 - val_loss: 0.9366 - val_accuracy: 0.4000\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 0s 310us/sample - loss: 0.9100 - accuracy: 0.4667 - val_loss: 0.9350 - val_accuracy: 0.4000\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.9078 - accuracy: 0.4857 - val_loss: 0.9327 - val_accuracy: 0.4667\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.9059 - accuracy: 0.4952 - val_loss: 0.9309 - val_accuracy: 0.4889\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.9042 - accuracy: 0.5048 - val_loss: 0.9287 - val_accuracy: 0.4889\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.9023 - accuracy: 0.5048 - val_loss: 0.9274 - val_accuracy: 0.4889\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.9003 - accuracy: 0.5238 - val_loss: 0.9257 - val_accuracy: 0.4889\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.8985 - accuracy: 0.5333 - val_loss: 0.9243 - val_accuracy: 0.4889\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.8967 - accuracy: 0.5333 - val_loss: 0.9221 - val_accuracy: 0.4889\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 0.8950 - accuracy: 0.5524 - val_loss: 0.9207 - val_accuracy: 0.4889\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.8935 - accuracy: 0.5524 - val_loss: 0.9204 - val_accuracy: 0.4889\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 0s 316us/sample - loss: 0.8920 - accuracy: 0.5524 - val_loss: 0.9202 - val_accuracy: 0.4889\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.8905 - accuracy: 0.5429 - val_loss: 0.9194 - val_accuracy: 0.5111\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.8890 - accuracy: 0.5429 - val_loss: 0.9178 - val_accuracy: 0.5111\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 0.8876 - accuracy: 0.5429 - val_loss: 0.9163 - val_accuracy: 0.5111\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 0s 313us/sample - loss: 0.8864 - accuracy: 0.5429 - val_loss: 0.9153 - val_accuracy: 0.5111\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 0s 371us/sample - loss: 0.8850 - accuracy: 0.5429 - val_loss: 0.9146 - val_accuracy: 0.5111\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 0s 429us/sample - loss: 0.8838 - accuracy: 0.5429 - val_loss: 0.9133 - val_accuracy: 0.5111\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 0s 320us/sample - loss: 0.8827 - accuracy: 0.5429 - val_loss: 0.9111 - val_accuracy: 0.5111\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 0s 321us/sample - loss: 0.8816 - accuracy: 0.5429 - val_loss: 0.9108 - val_accuracy: 0.5111\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 0s 325us/sample - loss: 0.8803 - accuracy: 0.5429 - val_loss: 0.9104 - val_accuracy: 0.5111\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 0s 342us/sample - loss: 0.8791 - accuracy: 0.5429 - val_loss: 0.9105 - val_accuracy: 0.4667\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 0s 363us/sample - loss: 0.8780 - accuracy: 0.5429 - val_loss: 0.9114 - val_accuracy: 0.4667\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 0.8772 - accuracy: 0.5333 - val_loss: 0.9119 - val_accuracy: 0.4667\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 0s 314us/sample - loss: 0.8762 - accuracy: 0.5333 - val_loss: 0.9109 - val_accuracy: 0.4667\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 0s 327us/sample - loss: 0.8750 - accuracy: 0.5333 - val_loss: 0.9086 - val_accuracy: 0.4667\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.8739 - accuracy: 0.5333 - val_loss: 0.9060 - val_accuracy: 0.4667\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 0s 354us/sample - loss: 0.8725 - accuracy: 0.5429 - val_loss: 0.9039 - val_accuracy: 0.4667\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.8713 - accuracy: 0.5429 - val_loss: 0.9016 - val_accuracy: 0.5111\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 0s 323us/sample - loss: 0.8702 - accuracy: 0.5429 - val_loss: 0.9002 - val_accuracy: 0.5111\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 0s 316us/sample - loss: 0.8690 - accuracy: 0.5429 - val_loss: 0.8997 - val_accuracy: 0.5111\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 0s 311us/sample - loss: 0.8679 - accuracy: 0.5429 - val_loss: 0.8984 - val_accuracy: 0.5111\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 0.8667 - accuracy: 0.5429 - val_loss: 0.8961 - val_accuracy: 0.5111\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.8655 - accuracy: 0.5524 - val_loss: 0.8939 - val_accuracy: 0.5111\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.8645 - accuracy: 0.5524 - val_loss: 0.8931 - val_accuracy: 0.5111\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 0s 349us/sample - loss: 0.8635 - accuracy: 0.5429 - val_loss: 0.8930 - val_accuracy: 0.5111\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.8621 - accuracy: 0.5429 - val_loss: 0.8926 - val_accuracy: 0.5111\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 0s 311us/sample - loss: 0.8612 - accuracy: 0.5429 - val_loss: 0.8924 - val_accuracy: 0.5111\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 0s 299us/sample - loss: 0.8599 - accuracy: 0.5429 - val_loss: 0.8903 - val_accuracy: 0.5111\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 0.8587 - accuracy: 0.5429 - val_loss: 0.8892 - val_accuracy: 0.5111\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.8577 - accuracy: 0.5429 - val_loss: 0.8864 - val_accuracy: 0.5111\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.8564 - accuracy: 0.5524 - val_loss: 0.8850 - val_accuracy: 0.5111\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 0s 299us/sample - loss: 0.8553 - accuracy: 0.5524 - val_loss: 0.8840 - val_accuracy: 0.5111\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 0.8540 - accuracy: 0.5429 - val_loss: 0.8846 - val_accuracy: 0.5111\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.8531 - accuracy: 0.5429 - val_loss: 0.8855 - val_accuracy: 0.4889\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 0s 379us/sample - loss: 0.8519 - accuracy: 0.5429 - val_loss: 0.8848 - val_accuracy: 0.4889\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 0s 326us/sample - loss: 0.8508 - accuracy: 0.5429 - val_loss: 0.8827 - val_accuracy: 0.5111\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 0s 383us/sample - loss: 0.8496 - accuracy: 0.5429 - val_loss: 0.8809 - val_accuracy: 0.5111\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.8486 - accuracy: 0.5429 - val_loss: 0.8807 - val_accuracy: 0.5111\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.8472 - accuracy: 0.5429 - val_loss: 0.8797 - val_accuracy: 0.4889\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.8460 - accuracy: 0.5429 - val_loss: 0.8781 - val_accuracy: 0.5111\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.8448 - accuracy: 0.5429 - val_loss: 0.8754 - val_accuracy: 0.5111\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.8437 - accuracy: 0.5429 - val_loss: 0.8733 - val_accuracy: 0.5111\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.8424 - accuracy: 0.5524 - val_loss: 0.8718 - val_accuracy: 0.5111\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.8413 - accuracy: 0.5524 - val_loss: 0.8705 - val_accuracy: 0.5111\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.8399 - accuracy: 0.5524 - val_loss: 0.8672 - val_accuracy: 0.5111\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.8390 - accuracy: 0.5524 - val_loss: 0.8645 - val_accuracy: 0.5111\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 0s 327us/sample - loss: 0.8380 - accuracy: 0.5619 - val_loss: 0.8628 - val_accuracy: 0.5111\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 0s 335us/sample - loss: 0.8370 - accuracy: 0.5619 - val_loss: 0.8620 - val_accuracy: 0.5111\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 0.8357 - accuracy: 0.5619 - val_loss: 0.8607 - val_accuracy: 0.5111\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 0s 316us/sample - loss: 0.8346 - accuracy: 0.5619 - val_loss: 0.8592 - val_accuracy: 0.5111\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.8335 - accuracy: 0.5619 - val_loss: 0.8592 - val_accuracy: 0.5111\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.8321 - accuracy: 0.5619 - val_loss: 0.8577 - val_accuracy: 0.5111\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.8309 - accuracy: 0.5619 - val_loss: 0.8560 - val_accuracy: 0.5111\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.8299 - accuracy: 0.5619 - val_loss: 0.8547 - val_accuracy: 0.5111\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.8285 - accuracy: 0.5619 - val_loss: 0.8552 - val_accuracy: 0.5111\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 0s 291us/sample - loss: 0.8273 - accuracy: 0.5524 - val_loss: 0.8570 - val_accuracy: 0.5111\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 0s 313us/sample - loss: 0.8262 - accuracy: 0.5429 - val_loss: 0.8592 - val_accuracy: 0.5111\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.8249 - accuracy: 0.5429 - val_loss: 0.8583 - val_accuracy: 0.5111\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.8237 - accuracy: 0.5429 - val_loss: 0.8577 - val_accuracy: 0.4889\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.8226 - accuracy: 0.5429 - val_loss: 0.8572 - val_accuracy: 0.4889\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.8220 - accuracy: 0.5429 - val_loss: 0.8544 - val_accuracy: 0.5111\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.8201 - accuracy: 0.5429 - val_loss: 0.8546 - val_accuracy: 0.4889\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.8191 - accuracy: 0.5429 - val_loss: 0.8530 - val_accuracy: 0.5111\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.8181 - accuracy: 0.5429 - val_loss: 0.8510 - val_accuracy: 0.5111\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.8167 - accuracy: 0.5429 - val_loss: 0.8496 - val_accuracy: 0.5111\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 0s 318us/sample - loss: 0.8152 - accuracy: 0.5429 - val_loss: 0.8450 - val_accuracy: 0.5111\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 0.8141 - accuracy: 0.5524 - val_loss: 0.8412 - val_accuracy: 0.5111\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.8129 - accuracy: 0.5619 - val_loss: 0.8382 - val_accuracy: 0.5111\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 0s 320us/sample - loss: 0.8120 - accuracy: 0.5810 - val_loss: 0.8368 - val_accuracy: 0.5111\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 0s 469us/sample - loss: 0.8107 - accuracy: 0.5810 - val_loss: 0.8371 - val_accuracy: 0.5111\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 0s 345us/sample - loss: 0.8098 - accuracy: 0.5619 - val_loss: 0.8382 - val_accuracy: 0.5111\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.8082 - accuracy: 0.5619 - val_loss: 0.8373 - val_accuracy: 0.5111\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 0s 310us/sample - loss: 0.8071 - accuracy: 0.5619 - val_loss: 0.8369 - val_accuracy: 0.5111\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.8059 - accuracy: 0.5524 - val_loss: 0.8369 - val_accuracy: 0.5111\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.8048 - accuracy: 0.5524 - val_loss: 0.8384 - val_accuracy: 0.5111\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.8037 - accuracy: 0.5429 - val_loss: 0.8368 - val_accuracy: 0.5111\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 0s 291us/sample - loss: 0.8025 - accuracy: 0.5524 - val_loss: 0.8341 - val_accuracy: 0.5111\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.8014 - accuracy: 0.5524 - val_loss: 0.8319 - val_accuracy: 0.5111\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.8002 - accuracy: 0.5619 - val_loss: 0.8298 - val_accuracy: 0.5111\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.7992 - accuracy: 0.5619 - val_loss: 0.8274 - val_accuracy: 0.5111\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.7978 - accuracy: 0.5619 - val_loss: 0.8276 - val_accuracy: 0.5111\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.7968 - accuracy: 0.5524 - val_loss: 0.8284 - val_accuracy: 0.5111\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.7956 - accuracy: 0.5524 - val_loss: 0.8276 - val_accuracy: 0.5111\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.7943 - accuracy: 0.5524 - val_loss: 0.8245 - val_accuracy: 0.5111\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 0s 340us/sample - loss: 0.7933 - accuracy: 0.5619 - val_loss: 0.8216 - val_accuracy: 0.5111\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.7923 - accuracy: 0.5619 - val_loss: 0.8182 - val_accuracy: 0.5111\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.7914 - accuracy: 0.5810 - val_loss: 0.8155 - val_accuracy: 0.5111\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.7900 - accuracy: 0.5810 - val_loss: 0.8152 - val_accuracy: 0.5111\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.7888 - accuracy: 0.5810 - val_loss: 0.8168 - val_accuracy: 0.5111\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.7878 - accuracy: 0.5619 - val_loss: 0.8195 - val_accuracy: 0.5111\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 0s 310us/sample - loss: 0.7864 - accuracy: 0.5524 - val_loss: 0.8193 - val_accuracy: 0.5111\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.7853 - accuracy: 0.5429 - val_loss: 0.8198 - val_accuracy: 0.5111\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 0s 316us/sample - loss: 0.7843 - accuracy: 0.5429 - val_loss: 0.8179 - val_accuracy: 0.5111\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 0s 317us/sample - loss: 0.7834 - accuracy: 0.5524 - val_loss: 0.8136 - val_accuracy: 0.5111\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.7821 - accuracy: 0.5619 - val_loss: 0.8134 - val_accuracy: 0.5111\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 0.7809 - accuracy: 0.5619 - val_loss: 0.8101 - val_accuracy: 0.5111\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.7797 - accuracy: 0.5619 - val_loss: 0.8085 - val_accuracy: 0.5111\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 0.7788 - accuracy: 0.5619 - val_loss: 0.8095 - val_accuracy: 0.5111\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.7772 - accuracy: 0.5619 - val_loss: 0.8101 - val_accuracy: 0.5111\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 0s 453us/sample - loss: 0.7761 - accuracy: 0.5619 - val_loss: 0.8099 - val_accuracy: 0.5111\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 0.7752 - accuracy: 0.5524 - val_loss: 0.8088 - val_accuracy: 0.5111\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.7740 - accuracy: 0.5619 - val_loss: 0.8070 - val_accuracy: 0.5111\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.7728 - accuracy: 0.5619 - val_loss: 0.8050 - val_accuracy: 0.5111\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.7717 - accuracy: 0.5619 - val_loss: 0.8030 - val_accuracy: 0.5111\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 0s 297us/sample - loss: 0.7706 - accuracy: 0.5619 - val_loss: 0.8005 - val_accuracy: 0.5111\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.7696 - accuracy: 0.5714 - val_loss: 0.7971 - val_accuracy: 0.5111\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.7685 - accuracy: 0.5810 - val_loss: 0.7947 - val_accuracy: 0.5111\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 0s 331us/sample - loss: 0.7676 - accuracy: 0.5810 - val_loss: 0.7924 - val_accuracy: 0.5111\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.7667 - accuracy: 0.5810 - val_loss: 0.7913 - val_accuracy: 0.5111\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.7656 - accuracy: 0.5810 - val_loss: 0.7906 - val_accuracy: 0.5111\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 0s 311us/sample - loss: 0.7645 - accuracy: 0.5810 - val_loss: 0.7893 - val_accuracy: 0.5111\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.7632 - accuracy: 0.5810 - val_loss: 0.7896 - val_accuracy: 0.5111\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.7619 - accuracy: 0.5810 - val_loss: 0.7900 - val_accuracy: 0.5111\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.7610 - accuracy: 0.5714 - val_loss: 0.7903 - val_accuracy: 0.5111\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.7598 - accuracy: 0.5714 - val_loss: 0.7903 - val_accuracy: 0.5111\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.7589 - accuracy: 0.5714 - val_loss: 0.7909 - val_accuracy: 0.5111\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.7580 - accuracy: 0.5619 - val_loss: 0.7916 - val_accuracy: 0.5111\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.7568 - accuracy: 0.5619 - val_loss: 0.7906 - val_accuracy: 0.5111\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.7558 - accuracy: 0.5619 - val_loss: 0.7879 - val_accuracy: 0.5111\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.7546 - accuracy: 0.5619 - val_loss: 0.7864 - val_accuracy: 0.5111\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.7536 - accuracy: 0.5619 - val_loss: 0.7858 - val_accuracy: 0.5111\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 0s 291us/sample - loss: 0.7525 - accuracy: 0.5619 - val_loss: 0.7844 - val_accuracy: 0.5111\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 0s 292us/sample - loss: 0.7513 - accuracy: 0.5714 - val_loss: 0.7821 - val_accuracy: 0.5111\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 0s 328us/sample - loss: 0.7505 - accuracy: 0.5714 - val_loss: 0.7804 - val_accuracy: 0.5111\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.7491 - accuracy: 0.5810 - val_loss: 0.7771 - val_accuracy: 0.5111\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.7482 - accuracy: 0.5810 - val_loss: 0.7751 - val_accuracy: 0.5111\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.7471 - accuracy: 0.5810 - val_loss: 0.7746 - val_accuracy: 0.5111\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.7459 - accuracy: 0.5810 - val_loss: 0.7725 - val_accuracy: 0.5111\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.7452 - accuracy: 0.5810 - val_loss: 0.7703 - val_accuracy: 0.5111\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.7441 - accuracy: 0.5810 - val_loss: 0.7697 - val_accuracy: 0.5111\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.7430 - accuracy: 0.5810 - val_loss: 0.7697 - val_accuracy: 0.5111\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 0s 411us/sample - loss: 0.7420 - accuracy: 0.5810 - val_loss: 0.7703 - val_accuracy: 0.5111\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 0s 291us/sample - loss: 0.7409 - accuracy: 0.5810 - val_loss: 0.7696 - val_accuracy: 0.5111\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.7399 - accuracy: 0.5810 - val_loss: 0.7692 - val_accuracy: 0.5111\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.7389 - accuracy: 0.5810 - val_loss: 0.7680 - val_accuracy: 0.5111\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.7379 - accuracy: 0.5810 - val_loss: 0.7672 - val_accuracy: 0.5111\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.7370 - accuracy: 0.5714 - val_loss: 0.7671 - val_accuracy: 0.5111\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.7359 - accuracy: 0.5714 - val_loss: 0.7653 - val_accuracy: 0.5111\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.7348 - accuracy: 0.5810 - val_loss: 0.7628 - val_accuracy: 0.5111\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.7339 - accuracy: 0.5810 - val_loss: 0.7595 - val_accuracy: 0.5111\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.7330 - accuracy: 0.5810 - val_loss: 0.7576 - val_accuracy: 0.5111\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 0s 299us/sample - loss: 0.7321 - accuracy: 0.5810 - val_loss: 0.7579 - val_accuracy: 0.5111\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.7307 - accuracy: 0.5810 - val_loss: 0.7591 - val_accuracy: 0.5111\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.7300 - accuracy: 0.5810 - val_loss: 0.7589 - val_accuracy: 0.5111\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.7289 - accuracy: 0.5810 - val_loss: 0.7571 - val_accuracy: 0.5111\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.7280 - accuracy: 0.5810 - val_loss: 0.7549 - val_accuracy: 0.5111\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 0s 299us/sample - loss: 0.7267 - accuracy: 0.5810 - val_loss: 0.7556 - val_accuracy: 0.5111\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 0s 341us/sample - loss: 0.7261 - accuracy: 0.5810 - val_loss: 0.7581 - val_accuracy: 0.5111\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.7259 - accuracy: 0.5714 - val_loss: 0.7602 - val_accuracy: 0.5111\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.7244 - accuracy: 0.5714 - val_loss: 0.7602 - val_accuracy: 0.5111\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 0s 315us/sample - loss: 0.7240 - accuracy: 0.5619 - val_loss: 0.7603 - val_accuracy: 0.5111\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 0.7226 - accuracy: 0.5714 - val_loss: 0.7562 - val_accuracy: 0.5111\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.7217 - accuracy: 0.5810 - val_loss: 0.7483 - val_accuracy: 0.5111\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.7203 - accuracy: 0.5810 - val_loss: 0.7443 - val_accuracy: 0.5111\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.7198 - accuracy: 0.5905 - val_loss: 0.7435 - val_accuracy: 0.5111\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.7188 - accuracy: 0.5905 - val_loss: 0.7426 - val_accuracy: 0.5111\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.7181 - accuracy: 0.5905 - val_loss: 0.7414 - val_accuracy: 0.5111\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.7166 - accuracy: 0.5810 - val_loss: 0.7443 - val_accuracy: 0.5111\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.7150 - accuracy: 0.5810 - val_loss: 0.7480 - val_accuracy: 0.5111\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.7144 - accuracy: 0.5714 - val_loss: 0.7510 - val_accuracy: 0.5111\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.7140 - accuracy: 0.5714 - val_loss: 0.7521 - val_accuracy: 0.5111\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.7136 - accuracy: 0.5429 - val_loss: 0.7529 - val_accuracy: 0.5111\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 0s 362us/sample - loss: 0.7132 - accuracy: 0.5333 - val_loss: 0.7505 - val_accuracy: 0.5111\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 0s 324us/sample - loss: 0.7116 - accuracy: 0.5619 - val_loss: 0.7428 - val_accuracy: 0.5111\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.7095 - accuracy: 0.5714 - val_loss: 0.7392 - val_accuracy: 0.5111\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.7089 - accuracy: 0.5810 - val_loss: 0.7351 - val_accuracy: 0.5111\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 0s 292us/sample - loss: 0.7082 - accuracy: 0.5810 - val_loss: 0.7340 - val_accuracy: 0.5111\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.7073 - accuracy: 0.5810 - val_loss: 0.7337 - val_accuracy: 0.5111\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.7065 - accuracy: 0.5810 - val_loss: 0.7356 - val_accuracy: 0.5111\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.7051 - accuracy: 0.5810 - val_loss: 0.7348 - val_accuracy: 0.5111\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.7044 - accuracy: 0.5810 - val_loss: 0.7318 - val_accuracy: 0.5111\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.7036 - accuracy: 0.5810 - val_loss: 0.7325 - val_accuracy: 0.5111\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 0s 318us/sample - loss: 0.7024 - accuracy: 0.5810 - val_loss: 0.7328 - val_accuracy: 0.5111\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.7015 - accuracy: 0.5810 - val_loss: 0.7331 - val_accuracy: 0.5111\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.7009 - accuracy: 0.5714 - val_loss: 0.7340 - val_accuracy: 0.5111\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.7006 - accuracy: 0.5714 - val_loss: 0.7345 - val_accuracy: 0.5111\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 0.6992 - accuracy: 0.5714 - val_loss: 0.7308 - val_accuracy: 0.5111\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.6980 - accuracy: 0.5810 - val_loss: 0.7285 - val_accuracy: 0.5111\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.6971 - accuracy: 0.5810 - val_loss: 0.7259 - val_accuracy: 0.5111\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 0s 307us/sample - loss: 0.6972 - accuracy: 0.5810 - val_loss: 0.7231 - val_accuracy: 0.5111\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 0s 327us/sample - loss: 0.6959 - accuracy: 0.5810 - val_loss: 0.7260 - val_accuracy: 0.5111\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 0s 342us/sample - loss: 0.6949 - accuracy: 0.5810 - val_loss: 0.7281 - val_accuracy: 0.5111\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.6941 - accuracy: 0.5714 - val_loss: 0.7277 - val_accuracy: 0.5111\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 0s 292us/sample - loss: 0.6930 - accuracy: 0.5714 - val_loss: 0.7242 - val_accuracy: 0.5111\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 0s 324us/sample - loss: 0.6922 - accuracy: 0.5810 - val_loss: 0.7209 - val_accuracy: 0.5111\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 0.6914 - accuracy: 0.5810 - val_loss: 0.7202 - val_accuracy: 0.5111\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 0.6906 - accuracy: 0.5810 - val_loss: 0.7175 - val_accuracy: 0.5111\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 0s 314us/sample - loss: 0.6904 - accuracy: 0.5810 - val_loss: 0.7147 - val_accuracy: 0.5111\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 0s 303us/sample - loss: 0.6897 - accuracy: 0.5810 - val_loss: 0.7172 - val_accuracy: 0.5111\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 0s 333us/sample - loss: 0.6881 - accuracy: 0.5810 - val_loss: 0.7185 - val_accuracy: 0.5111\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.6874 - accuracy: 0.5810 - val_loss: 0.7193 - val_accuracy: 0.5111\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.6865 - accuracy: 0.5714 - val_loss: 0.7190 - val_accuracy: 0.5111\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.6855 - accuracy: 0.5714 - val_loss: 0.7172 - val_accuracy: 0.5111\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.6848 - accuracy: 0.5810 - val_loss: 0.7147 - val_accuracy: 0.5111\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.6840 - accuracy: 0.5810 - val_loss: 0.7156 - val_accuracy: 0.5111\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.6831 - accuracy: 0.5810 - val_loss: 0.7157 - val_accuracy: 0.5111\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.6822 - accuracy: 0.5714 - val_loss: 0.7146 - val_accuracy: 0.5111\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.6814 - accuracy: 0.5714 - val_loss: 0.7125 - val_accuracy: 0.5111\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.6808 - accuracy: 0.5810 - val_loss: 0.7118 - val_accuracy: 0.5111\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.6798 - accuracy: 0.5714 - val_loss: 0.7127 - val_accuracy: 0.5111\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.6794 - accuracy: 0.5619 - val_loss: 0.7124 - val_accuracy: 0.5111\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.6785 - accuracy: 0.5714 - val_loss: 0.7082 - val_accuracy: 0.5111\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.6775 - accuracy: 0.5810 - val_loss: 0.7080 - val_accuracy: 0.5111\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.6766 - accuracy: 0.5810 - val_loss: 0.7088 - val_accuracy: 0.5111\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.6760 - accuracy: 0.5714 - val_loss: 0.7112 - val_accuracy: 0.5111\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.6755 - accuracy: 0.5619 - val_loss: 0.7105 - val_accuracy: 0.5111\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.6747 - accuracy: 0.5714 - val_loss: 0.7072 - val_accuracy: 0.5111\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 0s 350us/sample - loss: 0.6736 - accuracy: 0.5714 - val_loss: 0.7047 - val_accuracy: 0.5111\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.6729 - accuracy: 0.5810 - val_loss: 0.7043 - val_accuracy: 0.5111\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.6721 - accuracy: 0.5810 - val_loss: 0.7024 - val_accuracy: 0.5111\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.6713 - accuracy: 0.5810 - val_loss: 0.6994 - val_accuracy: 0.5111\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.6710 - accuracy: 0.5810 - val_loss: 0.6988 - val_accuracy: 0.5111\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.6697 - accuracy: 0.5810 - val_loss: 0.7014 - val_accuracy: 0.5111\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.6690 - accuracy: 0.5714 - val_loss: 0.7048 - val_accuracy: 0.5111\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 0s 333us/sample - loss: 0.6698 - accuracy: 0.5619 - val_loss: 0.7066 - val_accuracy: 0.5111\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.6679 - accuracy: 0.5619 - val_loss: 0.6998 - val_accuracy: 0.5111\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.6666 - accuracy: 0.5810 - val_loss: 0.6934 - val_accuracy: 0.5111\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.6668 - accuracy: 0.5905 - val_loss: 0.6903 - val_accuracy: 0.5111\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.6664 - accuracy: 0.5905 - val_loss: 0.6900 - val_accuracy: 0.5111\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.6653 - accuracy: 0.5905 - val_loss: 0.6920 - val_accuracy: 0.5111\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.6640 - accuracy: 0.5810 - val_loss: 0.6935 - val_accuracy: 0.5111\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.6633 - accuracy: 0.5810 - val_loss: 0.6953 - val_accuracy: 0.5111\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 0s 331us/sample - loss: 0.6627 - accuracy: 0.6000 - val_loss: 0.6960 - val_accuracy: 0.7556\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.6622 - accuracy: 0.8476 - val_loss: 0.6952 - val_accuracy: 0.7556\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.6614 - accuracy: 0.8381 - val_loss: 0.6933 - val_accuracy: 0.7556\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.6603 - accuracy: 0.8286 - val_loss: 0.6891 - val_accuracy: 0.7556\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.6599 - accuracy: 0.8190 - val_loss: 0.6867 - val_accuracy: 0.7556\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.6593 - accuracy: 0.8286 - val_loss: 0.6864 - val_accuracy: 0.7556\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.6585 - accuracy: 0.8286 - val_loss: 0.6889 - val_accuracy: 0.7556\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 0s 365us/sample - loss: 0.6575 - accuracy: 0.8476 - val_loss: 0.6932 - val_accuracy: 0.7556\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.6578 - accuracy: 0.8286 - val_loss: 0.6965 - val_accuracy: 0.7556\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.6577 - accuracy: 0.8286 - val_loss: 0.6962 - val_accuracy: 0.7556\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.6568 - accuracy: 0.8286 - val_loss: 0.6897 - val_accuracy: 0.7556\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.6550 - accuracy: 0.8286 - val_loss: 0.6847 - val_accuracy: 0.7556\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.6542 - accuracy: 0.8286 - val_loss: 0.6823 - val_accuracy: 0.7556\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.6536 - accuracy: 0.8286 - val_loss: 0.6809 - val_accuracy: 0.7556\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.6533 - accuracy: 0.8000 - val_loss: 0.6775 - val_accuracy: 0.7333\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.6524 - accuracy: 0.7905 - val_loss: 0.6779 - val_accuracy: 0.7333\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 0s 326us/sample - loss: 0.6524 - accuracy: 0.7905 - val_loss: 0.6798 - val_accuracy: 0.7333\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.6507 - accuracy: 0.7905 - val_loss: 0.6784 - val_accuracy: 0.7333\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.6499 - accuracy: 0.7905 - val_loss: 0.6792 - val_accuracy: 0.7333\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.6492 - accuracy: 0.7905 - val_loss: 0.6800 - val_accuracy: 0.7333\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.6485 - accuracy: 0.7810 - val_loss: 0.6819 - val_accuracy: 0.7333\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.6484 - accuracy: 0.7905 - val_loss: 0.6823 - val_accuracy: 0.7333\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.6474 - accuracy: 0.7905 - val_loss: 0.6795 - val_accuracy: 0.7333\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.6467 - accuracy: 0.7905 - val_loss: 0.6775 - val_accuracy: 0.7333\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 0s 352us/sample - loss: 0.6459 - accuracy: 0.7905 - val_loss: 0.6768 - val_accuracy: 0.7333\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.6452 - accuracy: 0.7905 - val_loss: 0.6772 - val_accuracy: 0.7333\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.6447 - accuracy: 0.7905 - val_loss: 0.6752 - val_accuracy: 0.7333\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.6437 - accuracy: 0.7905 - val_loss: 0.6758 - val_accuracy: 0.7333\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.6432 - accuracy: 0.7905 - val_loss: 0.6776 - val_accuracy: 0.7333\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.6430 - accuracy: 0.7810 - val_loss: 0.6774 - val_accuracy: 0.7333\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.6421 - accuracy: 0.7810 - val_loss: 0.6712 - val_accuracy: 0.7333\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.6407 - accuracy: 0.7905 - val_loss: 0.6674 - val_accuracy: 0.7111\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.6405 - accuracy: 0.7810 - val_loss: 0.6635 - val_accuracy: 0.7111\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 0s 292us/sample - loss: 0.6405 - accuracy: 0.7905 - val_loss: 0.6622 - val_accuracy: 0.7111\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.6396 - accuracy: 0.7905 - val_loss: 0.6656 - val_accuracy: 0.7111\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.6386 - accuracy: 0.7905 - val_loss: 0.6675 - val_accuracy: 0.7333\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.6376 - accuracy: 0.7905 - val_loss: 0.6670 - val_accuracy: 0.7333\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.6368 - accuracy: 0.7905 - val_loss: 0.6679 - val_accuracy: 0.7333\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.6361 - accuracy: 0.7905 - val_loss: 0.6673 - val_accuracy: 0.7333\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.6355 - accuracy: 0.7905 - val_loss: 0.6650 - val_accuracy: 0.7333\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.6348 - accuracy: 0.7905 - val_loss: 0.6642 - val_accuracy: 0.7333\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 0s 412us/sample - loss: 0.6340 - accuracy: 0.7905 - val_loss: 0.6636 - val_accuracy: 0.7333\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.6333 - accuracy: 0.7905 - val_loss: 0.6652 - val_accuracy: 0.7333\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.6327 - accuracy: 0.7905 - val_loss: 0.6649 - val_accuracy: 0.7333\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.6320 - accuracy: 0.7905 - val_loss: 0.6646 - val_accuracy: 0.7333\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.6315 - accuracy: 0.7905 - val_loss: 0.6642 - val_accuracy: 0.7333\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.6307 - accuracy: 0.8095 - val_loss: 0.6595 - val_accuracy: 0.7556\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.6298 - accuracy: 0.8095 - val_loss: 0.6579 - val_accuracy: 0.7778\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.6292 - accuracy: 0.8095 - val_loss: 0.6579 - val_accuracy: 0.7778\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.6286 - accuracy: 0.8190 - val_loss: 0.6537 - val_accuracy: 0.8000\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.6276 - accuracy: 0.8190 - val_loss: 0.6517 - val_accuracy: 0.8000\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.6269 - accuracy: 0.8190 - val_loss: 0.6483 - val_accuracy: 0.8222\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.6268 - accuracy: 0.8190 - val_loss: 0.6464 - val_accuracy: 0.8444\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.6258 - accuracy: 0.8190 - val_loss: 0.6483 - val_accuracy: 0.8222\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.6244 - accuracy: 0.8286 - val_loss: 0.6531 - val_accuracy: 0.8444\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.6247 - accuracy: 0.8476 - val_loss: 0.6596 - val_accuracy: 0.8000\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.6240 - accuracy: 0.8667 - val_loss: 0.6607 - val_accuracy: 0.8000\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.6232 - accuracy: 0.8762 - val_loss: 0.6584 - val_accuracy: 0.8222\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 0s 340us/sample - loss: 0.6221 - accuracy: 0.8667 - val_loss: 0.6516 - val_accuracy: 0.8444\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.6210 - accuracy: 0.8571 - val_loss: 0.6476 - val_accuracy: 0.8667\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.6200 - accuracy: 0.8476 - val_loss: 0.6459 - val_accuracy: 0.8444\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.6191 - accuracy: 0.8476 - val_loss: 0.6447 - val_accuracy: 0.8444\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.6183 - accuracy: 0.8476 - val_loss: 0.6452 - val_accuracy: 0.8667\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.6174 - accuracy: 0.8571 - val_loss: 0.6476 - val_accuracy: 0.8667\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.6176 - accuracy: 0.8667 - val_loss: 0.6510 - val_accuracy: 0.8667\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.6163 - accuracy: 0.8762 - val_loss: 0.6445 - val_accuracy: 0.8667\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.6148 - accuracy: 0.8762 - val_loss: 0.6383 - val_accuracy: 0.8667\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.6151 - accuracy: 0.8571 - val_loss: 0.6306 - val_accuracy: 0.8667\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.6145 - accuracy: 0.8476 - val_loss: 0.6284 - val_accuracy: 0.8889\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.6142 - accuracy: 0.8476 - val_loss: 0.6255 - val_accuracy: 0.8889\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.6137 - accuracy: 0.8476 - val_loss: 0.6252 - val_accuracy: 0.8889\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.6125 - accuracy: 0.8667 - val_loss: 0.6280 - val_accuracy: 0.8889\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.6106 - accuracy: 0.8762 - val_loss: 0.6325 - val_accuracy: 0.8889\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.6087 - accuracy: 0.9143 - val_loss: 0.6361 - val_accuracy: 0.8667\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.6080 - accuracy: 0.9143 - val_loss: 0.6393 - val_accuracy: 0.8667\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 0s 439us/sample - loss: 0.6080 - accuracy: 0.9238 - val_loss: 0.6424 - val_accuracy: 0.8667\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 0s 340us/sample - loss: 0.6075 - accuracy: 0.9238 - val_loss: 0.6429 - val_accuracy: 0.8667\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 0s 291us/sample - loss: 0.6068 - accuracy: 0.9238 - val_loss: 0.6433 - val_accuracy: 0.8444\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.6059 - accuracy: 0.9238 - val_loss: 0.6420 - val_accuracy: 0.8667\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.6046 - accuracy: 0.9238 - val_loss: 0.6369 - val_accuracy: 0.8889\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.6032 - accuracy: 0.9238 - val_loss: 0.6320 - val_accuracy: 0.8667\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.6017 - accuracy: 0.9238 - val_loss: 0.6300 - val_accuracy: 0.8667\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.6007 - accuracy: 0.9238 - val_loss: 0.6279 - val_accuracy: 0.8667\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.6002 - accuracy: 0.9238 - val_loss: 0.6278 - val_accuracy: 0.8889\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.5991 - accuracy: 0.9238 - val_loss: 0.6209 - val_accuracy: 0.8889\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.5977 - accuracy: 0.9238 - val_loss: 0.6171 - val_accuracy: 0.8889\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 0s 238us/sample - loss: 0.5962 - accuracy: 0.9238 - val_loss: 0.6168 - val_accuracy: 0.9111\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.5952 - accuracy: 0.9238 - val_loss: 0.6152 - val_accuracy: 0.9111\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.5943 - accuracy: 0.9238 - val_loss: 0.6169 - val_accuracy: 0.9111\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.5928 - accuracy: 0.9333 - val_loss: 0.6165 - val_accuracy: 0.9111\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 0s 339us/sample - loss: 0.5920 - accuracy: 0.9333 - val_loss: 0.6171 - val_accuracy: 0.8889\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 0s 341us/sample - loss: 0.5906 - accuracy: 0.9333 - val_loss: 0.6155 - val_accuracy: 0.9111\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.5895 - accuracy: 0.9333 - val_loss: 0.6134 - val_accuracy: 0.9111\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.5882 - accuracy: 0.9333 - val_loss: 0.6118 - val_accuracy: 0.9111\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.5869 - accuracy: 0.9333 - val_loss: 0.6105 - val_accuracy: 0.9111\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.5856 - accuracy: 0.9333 - val_loss: 0.6090 - val_accuracy: 0.9111\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.5846 - accuracy: 0.9333 - val_loss: 0.6071 - val_accuracy: 0.9111\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.5833 - accuracy: 0.9333 - val_loss: 0.6064 - val_accuracy: 0.9111\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.5816 - accuracy: 0.9333 - val_loss: 0.6013 - val_accuracy: 0.9111\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 0.5808 - accuracy: 0.9333 - val_loss: 0.5952 - val_accuracy: 0.9111\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 0s 238us/sample - loss: 0.5792 - accuracy: 0.9333 - val_loss: 0.5904 - val_accuracy: 0.9111\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.5781 - accuracy: 0.9333 - val_loss: 0.5885 - val_accuracy: 0.9333\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.5765 - accuracy: 0.9333 - val_loss: 0.5893 - val_accuracy: 0.9333\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.5750 - accuracy: 0.9333 - val_loss: 0.5910 - val_accuracy: 0.9333\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.5732 - accuracy: 0.9429 - val_loss: 0.5913 - val_accuracy: 0.9333\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.5717 - accuracy: 0.9524 - val_loss: 0.5910 - val_accuracy: 0.9333\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.5708 - accuracy: 0.9524 - val_loss: 0.5905 - val_accuracy: 0.9333\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.5684 - accuracy: 0.9524 - val_loss: 0.5845 - val_accuracy: 0.9333\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.5667 - accuracy: 0.9524 - val_loss: 0.5800 - val_accuracy: 0.9333\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 0.5654 - accuracy: 0.9524 - val_loss: 0.5761 - val_accuracy: 0.9333\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.5637 - accuracy: 0.9524 - val_loss: 0.5756 - val_accuracy: 0.9333\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 0s 229us/sample - loss: 0.5623 - accuracy: 0.9524 - val_loss: 0.5753 - val_accuracy: 0.9333\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.5603 - accuracy: 0.9524 - val_loss: 0.5695 - val_accuracy: 0.9333\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.5590 - accuracy: 0.9524 - val_loss: 0.5649 - val_accuracy: 0.9333\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.5573 - accuracy: 0.9524 - val_loss: 0.5643 - val_accuracy: 0.9333\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.5551 - accuracy: 0.9524 - val_loss: 0.5636 - val_accuracy: 0.9333\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.5525 - accuracy: 0.9524 - val_loss: 0.5666 - val_accuracy: 0.9333\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.5507 - accuracy: 0.9619 - val_loss: 0.5695 - val_accuracy: 0.9333\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.5496 - accuracy: 0.9619 - val_loss: 0.5695 - val_accuracy: 0.9333\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.5479 - accuracy: 0.9619 - val_loss: 0.5633 - val_accuracy: 0.9333\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.5450 - accuracy: 0.9619 - val_loss: 0.5592 - val_accuracy: 0.9333\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.5430 - accuracy: 0.9619 - val_loss: 0.5544 - val_accuracy: 0.9333\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.5407 - accuracy: 0.9619 - val_loss: 0.5525 - val_accuracy: 0.9333\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 0.5388 - accuracy: 0.9714 - val_loss: 0.5518 - val_accuracy: 0.9333\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.5366 - accuracy: 0.9714 - val_loss: 0.5512 - val_accuracy: 0.9333\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.5347 - accuracy: 0.9714 - val_loss: 0.5500 - val_accuracy: 0.9333\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 0s 292us/sample - loss: 0.5326 - accuracy: 0.9714 - val_loss: 0.5438 - val_accuracy: 0.9333\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.5308 - accuracy: 0.9714 - val_loss: 0.5390 - val_accuracy: 0.9333\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.5280 - accuracy: 0.9714 - val_loss: 0.5376 - val_accuracy: 0.9333\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.5257 - accuracy: 0.9714 - val_loss: 0.5371 - val_accuracy: 0.9333\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.5235 - accuracy: 0.9714 - val_loss: 0.5381 - val_accuracy: 0.9333\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.5217 - accuracy: 0.9714 - val_loss: 0.5406 - val_accuracy: 0.9333\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 0s 291us/sample - loss: 0.5200 - accuracy: 0.9714 - val_loss: 0.5397 - val_accuracy: 0.9333\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.5174 - accuracy: 0.9714 - val_loss: 0.5338 - val_accuracy: 0.9333\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.5146 - accuracy: 0.9714 - val_loss: 0.5280 - val_accuracy: 0.9333\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.5119 - accuracy: 0.9714 - val_loss: 0.5239 - val_accuracy: 0.9333\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.5096 - accuracy: 0.9714 - val_loss: 0.5213 - val_accuracy: 0.9333\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.5071 - accuracy: 0.9714 - val_loss: 0.5191 - val_accuracy: 0.9333\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 0s 313us/sample - loss: 0.5051 - accuracy: 0.9714 - val_loss: 0.5178 - val_accuracy: 0.9333\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 0s 291us/sample - loss: 0.5019 - accuracy: 0.9714 - val_loss: 0.5120 - val_accuracy: 0.9333\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.4996 - accuracy: 0.9714 - val_loss: 0.5043 - val_accuracy: 0.9333\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.4968 - accuracy: 0.9714 - val_loss: 0.5000 - val_accuracy: 0.9333\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.4945 - accuracy: 0.9714 - val_loss: 0.4957 - val_accuracy: 0.9556\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.4919 - accuracy: 0.9714 - val_loss: 0.4941 - val_accuracy: 0.9556\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 0s 305us/sample - loss: 0.4889 - accuracy: 0.9714 - val_loss: 0.4924 - val_accuracy: 0.9333\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.4862 - accuracy: 0.9714 - val_loss: 0.4928 - val_accuracy: 0.9333\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.4840 - accuracy: 0.9714 - val_loss: 0.4921 - val_accuracy: 0.9333\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.4808 - accuracy: 0.9714 - val_loss: 0.4835 - val_accuracy: 0.9556\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.4783 - accuracy: 0.9714 - val_loss: 0.4754 - val_accuracy: 0.9556\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 0s 299us/sample - loss: 0.4771 - accuracy: 0.9714 - val_loss: 0.4705 - val_accuracy: 0.9556\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 0.4743 - accuracy: 0.9714 - val_loss: 0.4693 - val_accuracy: 0.9556\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.4709 - accuracy: 0.9714 - val_loss: 0.4710 - val_accuracy: 0.9556\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.4674 - accuracy: 0.9714 - val_loss: 0.4703 - val_accuracy: 0.9556\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.4644 - accuracy: 0.9714 - val_loss: 0.4668 - val_accuracy: 0.9556\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.4615 - accuracy: 0.9714 - val_loss: 0.4660 - val_accuracy: 0.9556\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.4588 - accuracy: 0.9714 - val_loss: 0.4665 - val_accuracy: 0.9333\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.4562 - accuracy: 0.9714 - val_loss: 0.4667 - val_accuracy: 0.9333\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.4535 - accuracy: 0.9714 - val_loss: 0.4636 - val_accuracy: 0.9333\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.4508 - accuracy: 0.9714 - val_loss: 0.4581 - val_accuracy: 0.9333\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.4481 - accuracy: 0.9714 - val_loss: 0.4534 - val_accuracy: 0.9333\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.4450 - accuracy: 0.9714 - val_loss: 0.4516 - val_accuracy: 0.9333\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.4422 - accuracy: 0.9714 - val_loss: 0.4483 - val_accuracy: 0.9556\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.4394 - accuracy: 0.9714 - val_loss: 0.4436 - val_accuracy: 0.9556\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.4366 - accuracy: 0.9714 - val_loss: 0.4411 - val_accuracy: 0.9556\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 0s 300us/sample - loss: 0.4340 - accuracy: 0.9714 - val_loss: 0.4400 - val_accuracy: 0.9556\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.4316 - accuracy: 0.9714 - val_loss: 0.4386 - val_accuracy: 0.9556\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.4283 - accuracy: 0.9714 - val_loss: 0.4316 - val_accuracy: 0.9556\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.4257 - accuracy: 0.9714 - val_loss: 0.4261 - val_accuracy: 0.9556\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.4231 - accuracy: 0.9714 - val_loss: 0.4202 - val_accuracy: 0.9556\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.4209 - accuracy: 0.9714 - val_loss: 0.4170 - val_accuracy: 0.9556\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.4183 - accuracy: 0.9714 - val_loss: 0.4178 - val_accuracy: 0.9556\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.4144 - accuracy: 0.9714 - val_loss: 0.4191 - val_accuracy: 0.9556\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 0s 387us/sample - loss: 0.4121 - accuracy: 0.9714 - val_loss: 0.4213 - val_accuracy: 0.9556\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.4096 - accuracy: 0.9714 - val_loss: 0.4223 - val_accuracy: 0.9333\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.4074 - accuracy: 0.9714 - val_loss: 0.4211 - val_accuracy: 0.9333\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.4046 - accuracy: 0.9714 - val_loss: 0.4171 - val_accuracy: 0.9333\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.4018 - accuracy: 0.9714 - val_loss: 0.4122 - val_accuracy: 0.9556\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.3989 - accuracy: 0.9714 - val_loss: 0.4035 - val_accuracy: 0.9556\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 0s 349us/sample - loss: 0.3957 - accuracy: 0.9714 - val_loss: 0.3966 - val_accuracy: 0.9556\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.3939 - accuracy: 0.9714 - val_loss: 0.3894 - val_accuracy: 0.9556\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.3910 - accuracy: 0.9714 - val_loss: 0.3869 - val_accuracy: 0.9556\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.3884 - accuracy: 0.9714 - val_loss: 0.3876 - val_accuracy: 0.9556\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.3852 - accuracy: 0.9714 - val_loss: 0.3880 - val_accuracy: 0.9556\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.3828 - accuracy: 0.9714 - val_loss: 0.3896 - val_accuracy: 0.9556\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 0.3800 - accuracy: 0.9714 - val_loss: 0.3895 - val_accuracy: 0.9556\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 0s 331us/sample - loss: 0.3778 - accuracy: 0.9714 - val_loss: 0.3877 - val_accuracy: 0.9556\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.3753 - accuracy: 0.9714 - val_loss: 0.3829 - val_accuracy: 0.9556\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.3728 - accuracy: 0.9714 - val_loss: 0.3781 - val_accuracy: 0.9556\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.3700 - accuracy: 0.9714 - val_loss: 0.3747 - val_accuracy: 0.9556\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.3675 - accuracy: 0.9714 - val_loss: 0.3710 - val_accuracy: 0.9556\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.3651 - accuracy: 0.9714 - val_loss: 0.3652 - val_accuracy: 0.9556\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.3631 - accuracy: 0.9714 - val_loss: 0.3608 - val_accuracy: 0.9556\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.3606 - accuracy: 0.9714 - val_loss: 0.3592 - val_accuracy: 0.9556\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.3579 - accuracy: 0.9714 - val_loss: 0.3580 - val_accuracy: 0.9556\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.3554 - accuracy: 0.9714 - val_loss: 0.3597 - val_accuracy: 0.9556\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.3530 - accuracy: 0.9714 - val_loss: 0.3599 - val_accuracy: 0.9556\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.3509 - accuracy: 0.9714 - val_loss: 0.3573 - val_accuracy: 0.9556\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.3485 - accuracy: 0.9714 - val_loss: 0.3525 - val_accuracy: 0.9556\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.3460 - accuracy: 0.9714 - val_loss: 0.3497 - val_accuracy: 0.9556\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 0s 344us/sample - loss: 0.3437 - accuracy: 0.9714 - val_loss: 0.3447 - val_accuracy: 0.9556\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.3414 - accuracy: 0.9714 - val_loss: 0.3420 - val_accuracy: 0.9556\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.3391 - accuracy: 0.9714 - val_loss: 0.3382 - val_accuracy: 0.9556\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.3367 - accuracy: 0.9714 - val_loss: 0.3328 - val_accuracy: 0.9556\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.3357 - accuracy: 0.9714 - val_loss: 0.3292 - val_accuracy: 0.9556\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.3337 - accuracy: 0.9714 - val_loss: 0.3286 - val_accuracy: 0.9556\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.3312 - accuracy: 0.9714 - val_loss: 0.3279 - val_accuracy: 0.9556\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.3290 - accuracy: 0.9714 - val_loss: 0.3256 - val_accuracy: 0.9556\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.3269 - accuracy: 0.9714 - val_loss: 0.3243 - val_accuracy: 0.9556\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.3247 - accuracy: 0.9714 - val_loss: 0.3225 - val_accuracy: 0.9556\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.3226 - accuracy: 0.9714 - val_loss: 0.3222 - val_accuracy: 0.9556\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.3202 - accuracy: 0.9714 - val_loss: 0.3235 - val_accuracy: 0.9556\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.3188 - accuracy: 0.9714 - val_loss: 0.3248 - val_accuracy: 0.9556\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.3174 - accuracy: 0.9714 - val_loss: 0.3248 - val_accuracy: 0.9556\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 0s 352us/sample - loss: 0.3155 - accuracy: 0.9714 - val_loss: 0.3233 - val_accuracy: 0.9556\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.3135 - accuracy: 0.9714 - val_loss: 0.3187 - val_accuracy: 0.9556\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.3118 - accuracy: 0.9714 - val_loss: 0.3144 - val_accuracy: 0.9556\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.3096 - accuracy: 0.9714 - val_loss: 0.3130 - val_accuracy: 0.9556\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.3081 - accuracy: 0.9714 - val_loss: 0.3106 - val_accuracy: 0.9556\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.3063 - accuracy: 0.9714 - val_loss: 0.3111 - val_accuracy: 0.9556\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 0.3048 - accuracy: 0.9714 - val_loss: 0.3091 - val_accuracy: 0.9556\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.3030 - accuracy: 0.9714 - val_loss: 0.3080 - val_accuracy: 0.9556\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.3014 - accuracy: 0.9714 - val_loss: 0.3071 - val_accuracy: 0.9556\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.3002 - accuracy: 0.9714 - val_loss: 0.3053 - val_accuracy: 0.9556\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.2979 - accuracy: 0.9714 - val_loss: 0.2985 - val_accuracy: 0.9556\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 0s 325us/sample - loss: 0.2964 - accuracy: 0.9714 - val_loss: 0.2951 - val_accuracy: 0.9556\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.2949 - accuracy: 0.9714 - val_loss: 0.2934 - val_accuracy: 0.9556\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.2934 - accuracy: 0.9714 - val_loss: 0.2925 - val_accuracy: 0.9556\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.2918 - accuracy: 0.9714 - val_loss: 0.2928 - val_accuracy: 0.9556\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.2902 - accuracy: 0.9714 - val_loss: 0.2919 - val_accuracy: 0.9556\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.2886 - accuracy: 0.9714 - val_loss: 0.2938 - val_accuracy: 0.9556\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.2876 - accuracy: 0.9714 - val_loss: 0.2941 - val_accuracy: 0.9556\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.2859 - accuracy: 0.9714 - val_loss: 0.2917 - val_accuracy: 0.9556\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.2845 - accuracy: 0.9714 - val_loss: 0.2892 - val_accuracy: 0.9556\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.2828 - accuracy: 0.9714 - val_loss: 0.2865 - val_accuracy: 0.9556\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 0s 323us/sample - loss: 0.2816 - accuracy: 0.9714 - val_loss: 0.2848 - val_accuracy: 0.9556\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.2798 - accuracy: 0.9714 - val_loss: 0.2847 - val_accuracy: 0.9556\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.2787 - accuracy: 0.9714 - val_loss: 0.2840 - val_accuracy: 0.9556\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.2774 - accuracy: 0.9714 - val_loss: 0.2855 - val_accuracy: 0.9556\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.2766 - accuracy: 0.9714 - val_loss: 0.2875 - val_accuracy: 0.9556\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.2756 - accuracy: 0.9714 - val_loss: 0.2882 - val_accuracy: 0.9556\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.2745 - accuracy: 0.9714 - val_loss: 0.2856 - val_accuracy: 0.9556\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.2728 - accuracy: 0.9714 - val_loss: 0.2801 - val_accuracy: 0.9556\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.2717 - accuracy: 0.9714 - val_loss: 0.2745 - val_accuracy: 0.9556\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.2700 - accuracy: 0.9714 - val_loss: 0.2715 - val_accuracy: 0.9556\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.2684 - accuracy: 0.9714 - val_loss: 0.2714 - val_accuracy: 0.9556\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.2673 - accuracy: 0.9714 - val_loss: 0.2714 - val_accuracy: 0.9556\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.2662 - accuracy: 0.9714 - val_loss: 0.2693 - val_accuracy: 0.9556\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 0s 303us/sample - loss: 0.2651 - accuracy: 0.9714 - val_loss: 0.2697 - val_accuracy: 0.9556\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 0s 324us/sample - loss: 0.2638 - accuracy: 0.9714 - val_loss: 0.2676 - val_accuracy: 0.9556\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.2626 - accuracy: 0.9714 - val_loss: 0.2644 - val_accuracy: 0.9778\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.2614 - accuracy: 0.9714 - val_loss: 0.2632 - val_accuracy: 0.9778\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.2604 - accuracy: 0.9714 - val_loss: 0.2604 - val_accuracy: 0.9778\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.2594 - accuracy: 0.9714 - val_loss: 0.2594 - val_accuracy: 0.9778\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.2582 - accuracy: 0.9714 - val_loss: 0.2583 - val_accuracy: 0.9778\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.2569 - accuracy: 0.9714 - val_loss: 0.2592 - val_accuracy: 0.9778\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.2564 - accuracy: 0.9714 - val_loss: 0.2639 - val_accuracy: 0.9556\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 0.2549 - accuracy: 0.9714 - val_loss: 0.2639 - val_accuracy: 0.9556\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.2540 - accuracy: 0.9714 - val_loss: 0.2646 - val_accuracy: 0.9556\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.2530 - accuracy: 0.9714 - val_loss: 0.2616 - val_accuracy: 0.9556\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.2517 - accuracy: 0.9714 - val_loss: 0.2579 - val_accuracy: 0.9556\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.2513 - accuracy: 0.9714 - val_loss: 0.2538 - val_accuracy: 0.9778\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.2494 - accuracy: 0.9714 - val_loss: 0.2542 - val_accuracy: 0.9556\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.2486 - accuracy: 0.9714 - val_loss: 0.2540 - val_accuracy: 0.9556\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.2473 - accuracy: 0.9714 - val_loss: 0.2503 - val_accuracy: 0.9778\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 0s 313us/sample - loss: 0.2464 - accuracy: 0.9714 - val_loss: 0.2486 - val_accuracy: 0.9778\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.2456 - accuracy: 0.9714 - val_loss: 0.2468 - val_accuracy: 0.9778\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.2447 - accuracy: 0.9714 - val_loss: 0.2445 - val_accuracy: 0.9778\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.2437 - accuracy: 0.9810 - val_loss: 0.2439 - val_accuracy: 0.9778\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 0s 226us/sample - loss: 0.2424 - accuracy: 0.9714 - val_loss: 0.2461 - val_accuracy: 0.9778\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.2419 - accuracy: 0.9714 - val_loss: 0.2488 - val_accuracy: 0.9556\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.2410 - accuracy: 0.9714 - val_loss: 0.2503 - val_accuracy: 0.9556\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.2396 - accuracy: 0.9714 - val_loss: 0.2504 - val_accuracy: 0.9556\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.2398 - accuracy: 0.9714 - val_loss: 0.2508 - val_accuracy: 0.9556\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.2379 - accuracy: 0.9714 - val_loss: 0.2466 - val_accuracy: 0.9556\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.2364 - accuracy: 0.9714 - val_loss: 0.2415 - val_accuracy: 0.9778\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.2368 - accuracy: 0.9714 - val_loss: 0.2364 - val_accuracy: 0.9778\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.2350 - accuracy: 0.9810 - val_loss: 0.2367 - val_accuracy: 0.9778\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.2338 - accuracy: 0.9714 - val_loss: 0.2380 - val_accuracy: 0.9778\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 0s 291us/sample - loss: 0.2328 - accuracy: 0.9714 - val_loss: 0.2383 - val_accuracy: 0.9778\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 0s 305us/sample - loss: 0.2323 - accuracy: 0.9714 - val_loss: 0.2389 - val_accuracy: 0.9556\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.2316 - accuracy: 0.9714 - val_loss: 0.2355 - val_accuracy: 0.9778\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.2305 - accuracy: 0.9714 - val_loss: 0.2341 - val_accuracy: 0.9778\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 0.2295 - accuracy: 0.9714 - val_loss: 0.2307 - val_accuracy: 0.9778\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.2289 - accuracy: 0.9810 - val_loss: 0.2295 - val_accuracy: 0.9778\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.2281 - accuracy: 0.9714 - val_loss: 0.2316 - val_accuracy: 0.9778\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.2271 - accuracy: 0.9714 - val_loss: 0.2305 - val_accuracy: 0.9778\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.2259 - accuracy: 0.9714 - val_loss: 0.2271 - val_accuracy: 0.9778\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.2256 - accuracy: 0.9810 - val_loss: 0.2241 - val_accuracy: 0.9778\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.2252 - accuracy: 0.9810 - val_loss: 0.2242 - val_accuracy: 0.9778\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.2242 - accuracy: 0.9810 - val_loss: 0.2235 - val_accuracy: 0.9778\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.2231 - accuracy: 0.9810 - val_loss: 0.2258 - val_accuracy: 0.9778\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.2216 - accuracy: 0.9714 - val_loss: 0.2285 - val_accuracy: 0.9778\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.2214 - accuracy: 0.9714 - val_loss: 0.2315 - val_accuracy: 0.9556\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.2209 - accuracy: 0.9714 - val_loss: 0.2325 - val_accuracy: 0.9556\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 0.2203 - accuracy: 0.9714 - val_loss: 0.2328 - val_accuracy: 0.9556\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.2196 - accuracy: 0.9714 - val_loss: 0.2317 - val_accuracy: 0.9556\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.2189 - accuracy: 0.9714 - val_loss: 0.2300 - val_accuracy: 0.9556\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.2179 - accuracy: 0.9714 - val_loss: 0.2302 - val_accuracy: 0.9556\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.2177 - accuracy: 0.9714 - val_loss: 0.2292 - val_accuracy: 0.9556\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.2163 - accuracy: 0.9714 - val_loss: 0.2222 - val_accuracy: 0.9778\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.2149 - accuracy: 0.9714 - val_loss: 0.2190 - val_accuracy: 0.9778\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.2143 - accuracy: 0.9714 - val_loss: 0.2183 - val_accuracy: 0.9778\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.2139 - accuracy: 0.9714 - val_loss: 0.2193 - val_accuracy: 0.9778\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.2127 - accuracy: 0.9714 - val_loss: 0.2163 - val_accuracy: 0.9778\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.2122 - accuracy: 0.9714 - val_loss: 0.2149 - val_accuracy: 0.9778\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.2112 - accuracy: 0.9714 - val_loss: 0.2171 - val_accuracy: 0.9778\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.2107 - accuracy: 0.9714 - val_loss: 0.2203 - val_accuracy: 0.9556\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.2102 - accuracy: 0.9714 - val_loss: 0.2226 - val_accuracy: 0.9556\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.2098 - accuracy: 0.9714 - val_loss: 0.2211 - val_accuracy: 0.9556\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.2090 - accuracy: 0.9714 - val_loss: 0.2207 - val_accuracy: 0.9556\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.2085 - accuracy: 0.9714 - val_loss: 0.2196 - val_accuracy: 0.9556\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.2069 - accuracy: 0.9714 - val_loss: 0.2131 - val_accuracy: 0.9778\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 0s 316us/sample - loss: 0.2059 - accuracy: 0.9714 - val_loss: 0.2085 - val_accuracy: 0.9778\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.2063 - accuracy: 0.9810 - val_loss: 0.2043 - val_accuracy: 0.9778\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 0s 228us/sample - loss: 0.2058 - accuracy: 0.9810 - val_loss: 0.2028 - val_accuracy: 0.9778\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 0.2058 - accuracy: 0.9810 - val_loss: 0.2018 - val_accuracy: 0.9778\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.2047 - accuracy: 0.9810 - val_loss: 0.2027 - val_accuracy: 0.9778\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.2035 - accuracy: 0.9810 - val_loss: 0.2047 - val_accuracy: 0.9778\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.2027 - accuracy: 0.9714 - val_loss: 0.2089 - val_accuracy: 0.9778\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.2016 - accuracy: 0.9714 - val_loss: 0.2104 - val_accuracy: 0.9778\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.2006 - accuracy: 0.9714 - val_loss: 0.2065 - val_accuracy: 0.9778\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.2000 - accuracy: 0.9714 - val_loss: 0.2034 - val_accuracy: 0.9778\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.1994 - accuracy: 0.9714 - val_loss: 0.2019 - val_accuracy: 0.9778\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1989 - accuracy: 0.9714 - val_loss: 0.2019 - val_accuracy: 0.9778\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1983 - accuracy: 0.9810 - val_loss: 0.2003 - val_accuracy: 0.9778\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.1981 - accuracy: 0.9810 - val_loss: 0.1995 - val_accuracy: 0.9778\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.1965 - accuracy: 0.9810 - val_loss: 0.2028 - val_accuracy: 0.9778\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.1957 - accuracy: 0.9714 - val_loss: 0.2064 - val_accuracy: 0.9778\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 0s 308us/sample - loss: 0.1961 - accuracy: 0.9714 - val_loss: 0.2107 - val_accuracy: 0.9556\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1961 - accuracy: 0.9714 - val_loss: 0.2106 - val_accuracy: 0.9556\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.1955 - accuracy: 0.9714 - val_loss: 0.2079 - val_accuracy: 0.9556\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.1942 - accuracy: 0.9714 - val_loss: 0.2007 - val_accuracy: 0.9778\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.1926 - accuracy: 0.9714 - val_loss: 0.1948 - val_accuracy: 0.9778\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.1932 - accuracy: 0.9810 - val_loss: 0.1919 - val_accuracy: 0.9778\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1925 - accuracy: 0.9810 - val_loss: 0.1914 - val_accuracy: 0.9778\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.1920 - accuracy: 0.9810 - val_loss: 0.1907 - val_accuracy: 0.9778\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.1912 - accuracy: 0.9810 - val_loss: 0.1918 - val_accuracy: 0.9778\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1901 - accuracy: 0.9810 - val_loss: 0.1933 - val_accuracy: 0.9778\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.1894 - accuracy: 0.9714 - val_loss: 0.1951 - val_accuracy: 0.9778\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.1894 - accuracy: 0.9714 - val_loss: 0.1941 - val_accuracy: 0.9778\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.1887 - accuracy: 0.9714 - val_loss: 0.1996 - val_accuracy: 0.9556\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.1885 - accuracy: 0.9714 - val_loss: 0.2039 - val_accuracy: 0.9556\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 0s 332us/sample - loss: 0.1884 - accuracy: 0.9714 - val_loss: 0.2047 - val_accuracy: 0.9556\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.1881 - accuracy: 0.9714 - val_loss: 0.2036 - val_accuracy: 0.9556\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.1871 - accuracy: 0.9714 - val_loss: 0.1990 - val_accuracy: 0.9556\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.1858 - accuracy: 0.9714 - val_loss: 0.1933 - val_accuracy: 0.9778\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.1846 - accuracy: 0.9714 - val_loss: 0.1871 - val_accuracy: 0.9778\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.1842 - accuracy: 0.9810 - val_loss: 0.1844 - val_accuracy: 0.9778\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.1845 - accuracy: 0.9810 - val_loss: 0.1815 - val_accuracy: 0.9778\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.1846 - accuracy: 0.9810 - val_loss: 0.1809 - val_accuracy: 0.9778\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.1842 - accuracy: 0.9810 - val_loss: 0.1811 - val_accuracy: 0.9778\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.1824 - accuracy: 0.9810 - val_loss: 0.1855 - val_accuracy: 0.9778\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 0s 365us/sample - loss: 0.1814 - accuracy: 0.9714 - val_loss: 0.1940 - val_accuracy: 0.9556\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.1822 - accuracy: 0.9714 - val_loss: 0.1982 - val_accuracy: 0.9556\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1818 - accuracy: 0.9714 - val_loss: 0.1952 - val_accuracy: 0.9556\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.1807 - accuracy: 0.9714 - val_loss: 0.1895 - val_accuracy: 0.9778\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.1795 - accuracy: 0.9714 - val_loss: 0.1859 - val_accuracy: 0.9778\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 0.1788 - accuracy: 0.9714 - val_loss: 0.1848 - val_accuracy: 0.9778\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.1785 - accuracy: 0.9714 - val_loss: 0.1834 - val_accuracy: 0.9778\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.1779 - accuracy: 0.9714 - val_loss: 0.1824 - val_accuracy: 0.9778\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.1776 - accuracy: 0.9714 - val_loss: 0.1849 - val_accuracy: 0.9778\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.1769 - accuracy: 0.9714 - val_loss: 0.1857 - val_accuracy: 0.9778\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.1765 - accuracy: 0.9714 - val_loss: 0.1848 - val_accuracy: 0.9778\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.1760 - accuracy: 0.9714 - val_loss: 0.1857 - val_accuracy: 0.9778\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.1759 - accuracy: 0.9714 - val_loss: 0.1829 - val_accuracy: 0.9778\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 0s 313us/sample - loss: 0.1748 - accuracy: 0.9714 - val_loss: 0.1823 - val_accuracy: 0.9778\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.1744 - accuracy: 0.9714 - val_loss: 0.1816 - val_accuracy: 0.9778\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.1738 - accuracy: 0.9714 - val_loss: 0.1801 - val_accuracy: 0.9778\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.1733 - accuracy: 0.9714 - val_loss: 0.1790 - val_accuracy: 0.9778\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.1727 - accuracy: 0.9714 - val_loss: 0.1781 - val_accuracy: 0.9778\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.1725 - accuracy: 0.9714 - val_loss: 0.1768 - val_accuracy: 0.9778\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.1717 - accuracy: 0.9714 - val_loss: 0.1776 - val_accuracy: 0.9778\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 0.1713 - accuracy: 0.9714 - val_loss: 0.1779 - val_accuracy: 0.9778\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.1713 - accuracy: 0.9714 - val_loss: 0.1791 - val_accuracy: 0.9778\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.1706 - accuracy: 0.9714 - val_loss: 0.1779 - val_accuracy: 0.9778\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 0.1699 - accuracy: 0.9714 - val_loss: 0.1750 - val_accuracy: 0.9778\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.1693 - accuracy: 0.9714 - val_loss: 0.1729 - val_accuracy: 0.9778\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.1691 - accuracy: 0.9810 - val_loss: 0.1709 - val_accuracy: 0.9778\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.1687 - accuracy: 0.9810 - val_loss: 0.1712 - val_accuracy: 0.9778\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.1682 - accuracy: 0.9714 - val_loss: 0.1719 - val_accuracy: 0.9778\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.1676 - accuracy: 0.9714 - val_loss: 0.1724 - val_accuracy: 0.9778\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.1671 - accuracy: 0.9714 - val_loss: 0.1718 - val_accuracy: 0.9778\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 0s 324us/sample - loss: 0.1669 - accuracy: 0.9714 - val_loss: 0.1728 - val_accuracy: 0.9778\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.1662 - accuracy: 0.9714 - val_loss: 0.1731 - val_accuracy: 0.9778\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.1662 - accuracy: 0.9714 - val_loss: 0.1753 - val_accuracy: 0.9778\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1657 - accuracy: 0.9714 - val_loss: 0.1746 - val_accuracy: 0.9778\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 0.1654 - accuracy: 0.9714 - val_loss: 0.1753 - val_accuracy: 0.9778\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.1650 - accuracy: 0.9714 - val_loss: 0.1732 - val_accuracy: 0.9778\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.1644 - accuracy: 0.9714 - val_loss: 0.1707 - val_accuracy: 0.9778\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1636 - accuracy: 0.9714 - val_loss: 0.1694 - val_accuracy: 0.9778\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 0s 303us/sample - loss: 0.1635 - accuracy: 0.9714 - val_loss: 0.1678 - val_accuracy: 0.9778\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.1627 - accuracy: 0.9714 - val_loss: 0.1682 - val_accuracy: 0.9778\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 0s 234us/sample - loss: 0.1624 - accuracy: 0.9714 - val_loss: 0.1725 - val_accuracy: 0.9778\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1629 - accuracy: 0.9714 - val_loss: 0.1785 - val_accuracy: 0.9556\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1632 - accuracy: 0.9714 - val_loss: 0.1820 - val_accuracy: 0.9556\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.1643 - accuracy: 0.9714 - val_loss: 0.1823 - val_accuracy: 0.9556\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1628 - accuracy: 0.9714 - val_loss: 0.1740 - val_accuracy: 0.9778\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 0s 316us/sample - loss: 0.1607 - accuracy: 0.9714 - val_loss: 0.1684 - val_accuracy: 0.9778\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1596 - accuracy: 0.9714 - val_loss: 0.1647 - val_accuracy: 0.9778\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.1587 - accuracy: 0.9810 - val_loss: 0.1599 - val_accuracy: 0.9778\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.1593 - accuracy: 0.9810 - val_loss: 0.1565 - val_accuracy: 0.9778\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1595 - accuracy: 0.9810 - val_loss: 0.1571 - val_accuracy: 0.9778\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.1588 - accuracy: 0.9810 - val_loss: 0.1578 - val_accuracy: 0.9778\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.1583 - accuracy: 0.9810 - val_loss: 0.1566 - val_accuracy: 0.9778\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.1580 - accuracy: 0.9810 - val_loss: 0.1561 - val_accuracy: 0.9778\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.1574 - accuracy: 0.9810 - val_loss: 0.1574 - val_accuracy: 0.9778\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 0s 303us/sample - loss: 0.1566 - accuracy: 0.9810 - val_loss: 0.1592 - val_accuracy: 0.9778\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 0s 358us/sample - loss: 0.1560 - accuracy: 0.9714 - val_loss: 0.1616 - val_accuracy: 0.9778\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 0s 323us/sample - loss: 0.1565 - accuracy: 0.9714 - val_loss: 0.1640 - val_accuracy: 0.9778\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.1556 - accuracy: 0.9714 - val_loss: 0.1617 - val_accuracy: 0.9778\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.1551 - accuracy: 0.9714 - val_loss: 0.1597 - val_accuracy: 0.9778\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.1546 - accuracy: 0.9714 - val_loss: 0.1583 - val_accuracy: 0.9778\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.1542 - accuracy: 0.9714 - val_loss: 0.1567 - val_accuracy: 0.9778\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.1539 - accuracy: 0.9810 - val_loss: 0.1556 - val_accuracy: 0.9778\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.1535 - accuracy: 0.9810 - val_loss: 0.1559 - val_accuracy: 0.9778\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.1532 - accuracy: 0.9714 - val_loss: 0.1583 - val_accuracy: 0.9778\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.1534 - accuracy: 0.9714 - val_loss: 0.1597 - val_accuracy: 0.9778\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.1539 - accuracy: 0.9714 - val_loss: 0.1558 - val_accuracy: 0.9778\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.1520 - accuracy: 0.9714 - val_loss: 0.1560 - val_accuracy: 0.9778\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.1518 - accuracy: 0.9714 - val_loss: 0.1575 - val_accuracy: 0.9778\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.1513 - accuracy: 0.9714 - val_loss: 0.1568 - val_accuracy: 0.9778\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.1509 - accuracy: 0.9714 - val_loss: 0.1557 - val_accuracy: 0.9778\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.1504 - accuracy: 0.9714 - val_loss: 0.1531 - val_accuracy: 0.9778\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.1502 - accuracy: 0.9810 - val_loss: 0.1523 - val_accuracy: 0.9778\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.1497 - accuracy: 0.9810 - val_loss: 0.1541 - val_accuracy: 0.9778\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 0.1496 - accuracy: 0.9714 - val_loss: 0.1580 - val_accuracy: 0.9778\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.1493 - accuracy: 0.9714 - val_loss: 0.1596 - val_accuracy: 0.9778\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.1495 - accuracy: 0.9714 - val_loss: 0.1584 - val_accuracy: 0.9778\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.1488 - accuracy: 0.9714 - val_loss: 0.1568 - val_accuracy: 0.9778\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.1481 - accuracy: 0.9714 - val_loss: 0.1526 - val_accuracy: 0.9778\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.1474 - accuracy: 0.9714 - val_loss: 0.1480 - val_accuracy: 0.9778\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.1474 - accuracy: 0.9810 - val_loss: 0.1460 - val_accuracy: 0.9778\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.1475 - accuracy: 0.9810 - val_loss: 0.1465 - val_accuracy: 0.9778\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.1466 - accuracy: 0.9810 - val_loss: 0.1494 - val_accuracy: 0.9778\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.1463 - accuracy: 0.9714 - val_loss: 0.1517 - val_accuracy: 0.9778\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.1461 - accuracy: 0.9714 - val_loss: 0.1530 - val_accuracy: 0.9778\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.1460 - accuracy: 0.9714 - val_loss: 0.1541 - val_accuracy: 0.9778\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.1459 - accuracy: 0.9714 - val_loss: 0.1518 - val_accuracy: 0.9778\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.1451 - accuracy: 0.9714 - val_loss: 0.1506 - val_accuracy: 0.9778\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.1448 - accuracy: 0.9714 - val_loss: 0.1499 - val_accuracy: 0.9778\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.1444 - accuracy: 0.9714 - val_loss: 0.1498 - val_accuracy: 0.9778\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.1441 - accuracy: 0.9714 - val_loss: 0.1489 - val_accuracy: 0.9778\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.1438 - accuracy: 0.9714 - val_loss: 0.1453 - val_accuracy: 0.9778\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.1432 - accuracy: 0.9810 - val_loss: 0.1417 - val_accuracy: 0.9778\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.1437 - accuracy: 0.9810 - val_loss: 0.1402 - val_accuracy: 0.9778\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.1434 - accuracy: 0.9810 - val_loss: 0.1419 - val_accuracy: 0.9778\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.1425 - accuracy: 0.9810 - val_loss: 0.1459 - val_accuracy: 0.9778\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 0s 324us/sample - loss: 0.1418 - accuracy: 0.9714 - val_loss: 0.1496 - val_accuracy: 0.9778\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.1422 - accuracy: 0.9714 - val_loss: 0.1543 - val_accuracy: 0.9778\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.1440 - accuracy: 0.9714 - val_loss: 0.1567 - val_accuracy: 0.9556\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.1424 - accuracy: 0.9714 - val_loss: 0.1479 - val_accuracy: 0.9778\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1408 - accuracy: 0.9714 - val_loss: 0.1443 - val_accuracy: 0.9778\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.1410 - accuracy: 0.9714 - val_loss: 0.1404 - val_accuracy: 0.9778\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.1409 - accuracy: 0.9810 - val_loss: 0.1403 - val_accuracy: 0.9778\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1394 - accuracy: 0.9714 - val_loss: 0.1459 - val_accuracy: 0.9778\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 0s 291us/sample - loss: 0.1421 - accuracy: 0.9714 - val_loss: 0.1519 - val_accuracy: 0.9778\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.1395 - accuracy: 0.9714 - val_loss: 0.1439 - val_accuracy: 0.9778\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 0s 359us/sample - loss: 0.1403 - accuracy: 0.9714 - val_loss: 0.1360 - val_accuracy: 0.9778\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.1396 - accuracy: 0.9714 - val_loss: 0.1343 - val_accuracy: 0.9778\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.1399 - accuracy: 0.9714 - val_loss: 0.1329 - val_accuracy: 0.9778\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 0s 368us/sample - loss: 0.1401 - accuracy: 0.9714 - val_loss: 0.1331 - val_accuracy: 0.9778\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.1391 - accuracy: 0.9714 - val_loss: 0.1351 - val_accuracy: 0.9778\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.1378 - accuracy: 0.9810 - val_loss: 0.1408 - val_accuracy: 0.9778\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.1376 - accuracy: 0.9714 - val_loss: 0.1477 - val_accuracy: 0.9778\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 0s 300us/sample - loss: 0.1387 - accuracy: 0.9714 - val_loss: 0.1529 - val_accuracy: 0.9556\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.1392 - accuracy: 0.9714 - val_loss: 0.1533 - val_accuracy: 0.9556\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 0s 305us/sample - loss: 0.1385 - accuracy: 0.9714 - val_loss: 0.1470 - val_accuracy: 0.9778\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.1369 - accuracy: 0.9714 - val_loss: 0.1440 - val_accuracy: 0.9778\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.1364 - accuracy: 0.9714 - val_loss: 0.1401 - val_accuracy: 0.9778\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.1361 - accuracy: 0.9714 - val_loss: 0.1386 - val_accuracy: 0.9778\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1359 - accuracy: 0.9714 - val_loss: 0.1386 - val_accuracy: 0.9778\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.1350 - accuracy: 0.9714 - val_loss: 0.1434 - val_accuracy: 0.9778\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.1357 - accuracy: 0.9714 - val_loss: 0.1459 - val_accuracy: 0.9778\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1359 - accuracy: 0.9714 - val_loss: 0.1444 - val_accuracy: 0.9778\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.1352 - accuracy: 0.9714 - val_loss: 0.1363 - val_accuracy: 0.9778\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.1339 - accuracy: 0.9714 - val_loss: 0.1345 - val_accuracy: 0.9778\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1338 - accuracy: 0.9810 - val_loss: 0.1344 - val_accuracy: 0.9778\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.1335 - accuracy: 0.9810 - val_loss: 0.1352 - val_accuracy: 0.9778\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.1332 - accuracy: 0.9714 - val_loss: 0.1376 - val_accuracy: 0.9778\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.1329 - accuracy: 0.9714 - val_loss: 0.1383 - val_accuracy: 0.9778\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1328 - accuracy: 0.9714 - val_loss: 0.1379 - val_accuracy: 0.9778\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.1326 - accuracy: 0.9714 - val_loss: 0.1365 - val_accuracy: 0.9778\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.1321 - accuracy: 0.9714 - val_loss: 0.1319 - val_accuracy: 0.9778\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 0s 338us/sample - loss: 0.1326 - accuracy: 0.9810 - val_loss: 0.1299 - val_accuracy: 0.9778\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.1319 - accuracy: 0.9810 - val_loss: 0.1299 - val_accuracy: 0.9778\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.1316 - accuracy: 0.9810 - val_loss: 0.1299 - val_accuracy: 0.9778\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 0s 234us/sample - loss: 0.1314 - accuracy: 0.9810 - val_loss: 0.1318 - val_accuracy: 0.9778\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1309 - accuracy: 0.9810 - val_loss: 0.1334 - val_accuracy: 0.9778\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.1311 - accuracy: 0.9714 - val_loss: 0.1356 - val_accuracy: 0.9778\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 0s 376us/sample - loss: 0.1308 - accuracy: 0.9714 - val_loss: 0.1333 - val_accuracy: 0.9778\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.1303 - accuracy: 0.9714 - val_loss: 0.1339 - val_accuracy: 0.9778\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.1298 - accuracy: 0.9714 - val_loss: 0.1332 - val_accuracy: 0.9778\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.1297 - accuracy: 0.9714 - val_loss: 0.1322 - val_accuracy: 0.9778\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.1295 - accuracy: 0.9714 - val_loss: 0.1324 - val_accuracy: 0.9778\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.1292 - accuracy: 0.9714 - val_loss: 0.1320 - val_accuracy: 0.9778\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.1295 - accuracy: 0.9714 - val_loss: 0.1305 - val_accuracy: 0.9778\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.1288 - accuracy: 0.9714 - val_loss: 0.1339 - val_accuracy: 0.9778\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.1288 - accuracy: 0.9714 - val_loss: 0.1341 - val_accuracy: 0.9778\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.1284 - accuracy: 0.9714 - val_loss: 0.1337 - val_accuracy: 0.9778\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 0s 310us/sample - loss: 0.1283 - accuracy: 0.9714 - val_loss: 0.1331 - val_accuracy: 0.9778\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.1279 - accuracy: 0.9714 - val_loss: 0.1332 - val_accuracy: 0.9778\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.1280 - accuracy: 0.9714 - val_loss: 0.1334 - val_accuracy: 0.9778\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.1277 - accuracy: 0.9714 - val_loss: 0.1335 - val_accuracy: 0.9778\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.1272 - accuracy: 0.9714 - val_loss: 0.1304 - val_accuracy: 0.9778\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.1269 - accuracy: 0.9714 - val_loss: 0.1283 - val_accuracy: 0.9778\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.1264 - accuracy: 0.9714 - val_loss: 0.1270 - val_accuracy: 0.9778\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1262 - accuracy: 0.9714 - val_loss: 0.1271 - val_accuracy: 0.9778\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 0.1263 - accuracy: 0.9714 - val_loss: 0.1269 - val_accuracy: 0.9778\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.1258 - accuracy: 0.9714 - val_loss: 0.1287 - val_accuracy: 0.9778\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1256 - accuracy: 0.9714 - val_loss: 0.1274 - val_accuracy: 0.9778\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.1253 - accuracy: 0.9714 - val_loss: 0.1288 - val_accuracy: 0.9778\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.1253 - accuracy: 0.9714 - val_loss: 0.1291 - val_accuracy: 0.9778\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.1249 - accuracy: 0.9714 - val_loss: 0.1273 - val_accuracy: 0.9778\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1247 - accuracy: 0.9714 - val_loss: 0.1253 - val_accuracy: 0.9778\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.1256 - accuracy: 0.9810 - val_loss: 0.1204 - val_accuracy: 0.9778\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.1247 - accuracy: 0.9714 - val_loss: 0.1206 - val_accuracy: 0.9778\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1244 - accuracy: 0.9810 - val_loss: 0.1212 - val_accuracy: 0.9778\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.1242 - accuracy: 0.9810 - val_loss: 0.1235 - val_accuracy: 0.9778\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.1235 - accuracy: 0.9810 - val_loss: 0.1245 - val_accuracy: 0.9778\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.1231 - accuracy: 0.9714 - val_loss: 0.1265 - val_accuracy: 0.9778\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1242 - accuracy: 0.9714 - val_loss: 0.1292 - val_accuracy: 0.9778\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.1236 - accuracy: 0.9714 - val_loss: 0.1262 - val_accuracy: 0.9778\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.1229 - accuracy: 0.9714 - val_loss: 0.1247 - val_accuracy: 0.9778\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 0s 453us/sample - loss: 0.1228 - accuracy: 0.9714 - val_loss: 0.1193 - val_accuracy: 0.9778\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.1226 - accuracy: 0.9810 - val_loss: 0.1178 - val_accuracy: 0.9778\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.1227 - accuracy: 0.9714 - val_loss: 0.1191 - val_accuracy: 0.9778\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.1222 - accuracy: 0.9810 - val_loss: 0.1205 - val_accuracy: 0.9778\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.1218 - accuracy: 0.9810 - val_loss: 0.1196 - val_accuracy: 0.9778\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.1217 - accuracy: 0.9810 - val_loss: 0.1199 - val_accuracy: 0.9778\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 0s 293us/sample - loss: 0.1211 - accuracy: 0.9810 - val_loss: 0.1226 - val_accuracy: 0.9778\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 0s 310us/sample - loss: 0.1212 - accuracy: 0.9714 - val_loss: 0.1237 - val_accuracy: 0.9778\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.1208 - accuracy: 0.9714 - val_loss: 0.1209 - val_accuracy: 0.9778\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1207 - accuracy: 0.9810 - val_loss: 0.1188 - val_accuracy: 0.9778\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.1205 - accuracy: 0.9810 - val_loss: 0.1188 - val_accuracy: 0.9778\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 0.1206 - accuracy: 0.9714 - val_loss: 0.1217 - val_accuracy: 0.9778\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 0s 361us/sample - loss: 0.1208 - accuracy: 0.9714 - val_loss: 0.1269 - val_accuracy: 0.9778\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.1205 - accuracy: 0.9714 - val_loss: 0.1298 - val_accuracy: 0.9778\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 0s 318us/sample - loss: 0.1210 - accuracy: 0.9714 - val_loss: 0.1271 - val_accuracy: 0.9778\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1200 - accuracy: 0.9714 - val_loss: 0.1257 - val_accuracy: 0.9778\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.1196 - accuracy: 0.9714 - val_loss: 0.1234 - val_accuracy: 0.9778\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.1186 - accuracy: 0.9714 - val_loss: 0.1180 - val_accuracy: 0.9778\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1181 - accuracy: 0.9810 - val_loss: 0.1127 - val_accuracy: 0.9778\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1208 - accuracy: 0.9714 - val_loss: 0.1109 - val_accuracy: 0.9778\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.1205 - accuracy: 0.9714 - val_loss: 0.1139 - val_accuracy: 0.9778\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.1184 - accuracy: 0.9810 - val_loss: 0.1167 - val_accuracy: 0.9778\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.1187 - accuracy: 0.9714 - val_loss: 0.1193 - val_accuracy: 0.9778\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.1182 - accuracy: 0.9714 - val_loss: 0.1191 - val_accuracy: 0.9778\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.1177 - accuracy: 0.9714 - val_loss: 0.1177 - val_accuracy: 0.9778\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1174 - accuracy: 0.9810 - val_loss: 0.1161 - val_accuracy: 0.9778\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.1174 - accuracy: 0.9810 - val_loss: 0.1163 - val_accuracy: 0.9778\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.1174 - accuracy: 0.9714 - val_loss: 0.1212 - val_accuracy: 0.9778\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.1175 - accuracy: 0.9714 - val_loss: 0.1228 - val_accuracy: 0.9778\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.1167 - accuracy: 0.9714 - val_loss: 0.1181 - val_accuracy: 0.9778\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.1166 - accuracy: 0.9714 - val_loss: 0.1142 - val_accuracy: 0.9778\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 0s 326us/sample - loss: 0.1164 - accuracy: 0.9810 - val_loss: 0.1140 - val_accuracy: 0.9778\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.1161 - accuracy: 0.9810 - val_loss: 0.1150 - val_accuracy: 0.9778\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.1163 - accuracy: 0.9810 - val_loss: 0.1153 - val_accuracy: 0.9778\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.1154 - accuracy: 0.9714 - val_loss: 0.1194 - val_accuracy: 0.9778\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.1165 - accuracy: 0.9714 - val_loss: 0.1222 - val_accuracy: 0.9778\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.1162 - accuracy: 0.9714 - val_loss: 0.1178 - val_accuracy: 0.9778\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1154 - accuracy: 0.9714 - val_loss: 0.1147 - val_accuracy: 0.9778\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.1158 - accuracy: 0.9714 - val_loss: 0.1146 - val_accuracy: 0.9778\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.1154 - accuracy: 0.9714 - val_loss: 0.1126 - val_accuracy: 0.9778\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 0s 308us/sample - loss: 0.1148 - accuracy: 0.9810 - val_loss: 0.1153 - val_accuracy: 0.9778\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 0.1147 - accuracy: 0.9714 - val_loss: 0.1191 - val_accuracy: 0.9778\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.1147 - accuracy: 0.9714 - val_loss: 0.1190 - val_accuracy: 0.9778\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1145 - accuracy: 0.9714 - val_loss: 0.1176 - val_accuracy: 0.9778\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.1144 - accuracy: 0.9714 - val_loss: 0.1169 - val_accuracy: 0.9778\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.1140 - accuracy: 0.9714 - val_loss: 0.1189 - val_accuracy: 0.9778\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.1142 - accuracy: 0.9714 - val_loss: 0.1201 - val_accuracy: 0.9778\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 0.1148 - accuracy: 0.9714 - val_loss: 0.1243 - val_accuracy: 0.9778\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.1148 - accuracy: 0.9714 - val_loss: 0.1226 - val_accuracy: 0.9778\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.1145 - accuracy: 0.9714 - val_loss: 0.1203 - val_accuracy: 0.9778\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.1138 - accuracy: 0.9714 - val_loss: 0.1193 - val_accuracy: 0.9778\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.1136 - accuracy: 0.9714 - val_loss: 0.1152 - val_accuracy: 0.9778\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.1126 - accuracy: 0.9714 - val_loss: 0.1123 - val_accuracy: 0.9778\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.1124 - accuracy: 0.9714 - val_loss: 0.1111 - val_accuracy: 0.9778\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.1122 - accuracy: 0.9810 - val_loss: 0.1118 - val_accuracy: 0.9778\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.1124 - accuracy: 0.9810 - val_loss: 0.1112 - val_accuracy: 0.9778\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1119 - accuracy: 0.9714 - val_loss: 0.1116 - val_accuracy: 0.9778\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 0.1119 - accuracy: 0.9714 - val_loss: 0.1111 - val_accuracy: 0.9778\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.1116 - accuracy: 0.9714 - val_loss: 0.1110 - val_accuracy: 0.9778\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.1114 - accuracy: 0.9714 - val_loss: 0.1106 - val_accuracy: 0.9778\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1112 - accuracy: 0.9714 - val_loss: 0.1107 - val_accuracy: 0.9778\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1114 - accuracy: 0.9714 - val_loss: 0.1131 - val_accuracy: 0.9778\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.1123 - accuracy: 0.9714 - val_loss: 0.1167 - val_accuracy: 0.9778\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.1113 - accuracy: 0.9714 - val_loss: 0.1130 - val_accuracy: 0.9778\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.1119 - accuracy: 0.9714 - val_loss: 0.1097 - val_accuracy: 0.9778\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.1104 - accuracy: 0.9714 - val_loss: 0.1115 - val_accuracy: 0.9778\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.1112 - accuracy: 0.9714 - val_loss: 0.1144 - val_accuracy: 0.9778\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.1114 - accuracy: 0.9714 - val_loss: 0.1153 - val_accuracy: 0.9778\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.1102 - accuracy: 0.9714 - val_loss: 0.1112 - val_accuracy: 0.9778\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 0s 323us/sample - loss: 0.1105 - accuracy: 0.9714 - val_loss: 0.1079 - val_accuracy: 0.9778\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 0.1099 - accuracy: 0.9810 - val_loss: 0.1073 - val_accuracy: 0.9778\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.1106 - accuracy: 0.9714 - val_loss: 0.1097 - val_accuracy: 0.9778\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.1097 - accuracy: 0.9714 - val_loss: 0.1065 - val_accuracy: 0.9778\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.1092 - accuracy: 0.9810 - val_loss: 0.1064 - val_accuracy: 0.9778\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.1089 - accuracy: 0.9810 - val_loss: 0.1093 - val_accuracy: 0.9778\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.1091 - accuracy: 0.9714 - val_loss: 0.1123 - val_accuracy: 0.9778\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.1098 - accuracy: 0.9714 - val_loss: 0.1130 - val_accuracy: 0.9778\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1089 - accuracy: 0.9714 - val_loss: 0.1100 - val_accuracy: 0.9778\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 0s 313us/sample - loss: 0.1083 - accuracy: 0.9810 - val_loss: 0.1042 - val_accuracy: 0.9778\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.1090 - accuracy: 0.9714 - val_loss: 0.1017 - val_accuracy: 0.9778\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.1092 - accuracy: 0.9714 - val_loss: 0.1021 - val_accuracy: 0.9778\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.1084 - accuracy: 0.9810 - val_loss: 0.1050 - val_accuracy: 0.9778\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.1081 - accuracy: 0.9714 - val_loss: 0.1101 - val_accuracy: 0.9778\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.1081 - accuracy: 0.9714 - val_loss: 0.1123 - val_accuracy: 0.9778\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.1079 - accuracy: 0.9714 - val_loss: 0.1094 - val_accuracy: 0.9778\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.1094 - accuracy: 0.9714 - val_loss: 0.1035 - val_accuracy: 0.9778\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.1072 - accuracy: 0.9810 - val_loss: 0.1031 - val_accuracy: 0.9778\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.1078 - accuracy: 0.9810 - val_loss: 0.1021 - val_accuracy: 0.9778\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.1074 - accuracy: 0.9714 - val_loss: 0.0980 - val_accuracy: 0.9778\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.1089 - accuracy: 0.9714 - val_loss: 0.0978 - val_accuracy: 0.9778\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 0s 234us/sample - loss: 0.1081 - accuracy: 0.9714 - val_loss: 0.1007 - val_accuracy: 0.9778\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 0.1065 - accuracy: 0.9714 - val_loss: 0.1037 - val_accuracy: 0.9778\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.1062 - accuracy: 0.9810 - val_loss: 0.1056 - val_accuracy: 0.9778\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 0.1065 - accuracy: 0.9714 - val_loss: 0.1065 - val_accuracy: 0.9778\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 0s 312us/sample - loss: 0.1068 - accuracy: 0.9714 - val_loss: 0.1029 - val_accuracy: 0.9778\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 0s 321us/sample - loss: 0.1061 - accuracy: 0.9810 - val_loss: 0.1018 - val_accuracy: 0.9778\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.1063 - accuracy: 0.9810 - val_loss: 0.0989 - val_accuracy: 0.9778\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.1063 - accuracy: 0.9714 - val_loss: 0.0992 - val_accuracy: 0.9778\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.1062 - accuracy: 0.9714 - val_loss: 0.1011 - val_accuracy: 0.9778\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.1049 - accuracy: 0.9810 - val_loss: 0.1047 - val_accuracy: 0.9778\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.1049 - accuracy: 0.9714 - val_loss: 0.1099 - val_accuracy: 0.9778\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.1073 - accuracy: 0.9714 - val_loss: 0.1161 - val_accuracy: 0.9778\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.1074 - accuracy: 0.9714 - val_loss: 0.1114 - val_accuracy: 0.9778\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 0s 332us/sample - loss: 0.1042 - accuracy: 0.9714 - val_loss: 0.0990 - val_accuracy: 0.9778\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.1060 - accuracy: 0.9714 - val_loss: 0.0947 - val_accuracy: 0.9778\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.1071 - accuracy: 0.9714 - val_loss: 0.0955 - val_accuracy: 0.9778\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.1061 - accuracy: 0.9714 - val_loss: 0.0961 - val_accuracy: 0.9778\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.1059 - accuracy: 0.9714 - val_loss: 0.0963 - val_accuracy: 0.9778\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.1049 - accuracy: 0.9714 - val_loss: 0.0984 - val_accuracy: 0.9778\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.1045 - accuracy: 0.9810 - val_loss: 0.1011 - val_accuracy: 0.9778\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.1043 - accuracy: 0.9810 - val_loss: 0.1017 - val_accuracy: 0.9778\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.1044 - accuracy: 0.9714 - val_loss: 0.1065 - val_accuracy: 0.9778\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 0s 299us/sample - loss: 0.1042 - accuracy: 0.9714 - val_loss: 0.1079 - val_accuracy: 0.9778\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.1046 - accuracy: 0.9714 - val_loss: 0.1056 - val_accuracy: 0.9778\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.1037 - accuracy: 0.9714 - val_loss: 0.1051 - val_accuracy: 0.9778\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.1037 - accuracy: 0.9714 - val_loss: 0.1056 - val_accuracy: 0.9778\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.1037 - accuracy: 0.9714 - val_loss: 0.1031 - val_accuracy: 0.9778\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.1030 - accuracy: 0.9714 - val_loss: 0.1018 - val_accuracy: 0.9778\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.1027 - accuracy: 0.9714 - val_loss: 0.0998 - val_accuracy: 0.9778\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.1028 - accuracy: 0.9810 - val_loss: 0.0971 - val_accuracy: 0.9778\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1029 - accuracy: 0.9810 - val_loss: 0.0973 - val_accuracy: 0.9778\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1027 - accuracy: 0.9714 - val_loss: 0.0957 - val_accuracy: 0.9778\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1046 - accuracy: 0.9714 - val_loss: 0.0931 - val_accuracy: 0.9778\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.1035 - accuracy: 0.9714 - val_loss: 0.0951 - val_accuracy: 0.9778\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1022 - accuracy: 0.9714 - val_loss: 0.0983 - val_accuracy: 0.9778\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.1016 - accuracy: 0.9810 - val_loss: 0.1026 - val_accuracy: 0.9778\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.1030 - accuracy: 0.9714 - val_loss: 0.1060 - val_accuracy: 0.9778\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 0s 292us/sample - loss: 0.1023 - accuracy: 0.9714 - val_loss: 0.1037 - val_accuracy: 0.9778\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.1019 - accuracy: 0.9714 - val_loss: 0.1019 - val_accuracy: 0.9778\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.1018 - accuracy: 0.9714 - val_loss: 0.0997 - val_accuracy: 0.9778\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.1009 - accuracy: 0.9714 - val_loss: 0.0962 - val_accuracy: 0.9778\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.1019 - accuracy: 0.9714 - val_loss: 0.0936 - val_accuracy: 0.9778\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.1022 - accuracy: 0.9714 - val_loss: 0.0934 - val_accuracy: 0.9778\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.1014 - accuracy: 0.9714 - val_loss: 0.0964 - val_accuracy: 0.9778\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.1008 - accuracy: 0.9810 - val_loss: 0.0987 - val_accuracy: 0.9778\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 0s 325us/sample - loss: 0.1009 - accuracy: 0.9714 - val_loss: 0.0991 - val_accuracy: 0.9778\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.1017 - accuracy: 0.9714 - val_loss: 0.1003 - val_accuracy: 0.9778\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 0s 349us/sample - loss: 0.1024 - accuracy: 0.9714 - val_loss: 0.0953 - val_accuracy: 0.9778\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.1010 - accuracy: 0.9810 - val_loss: 0.0963 - val_accuracy: 0.9778\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.1007 - accuracy: 0.9810 - val_loss: 0.0971 - val_accuracy: 0.9778\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.1004 - accuracy: 0.9714 - val_loss: 0.0967 - val_accuracy: 0.9778\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.1006 - accuracy: 0.9714 - val_loss: 0.0930 - val_accuracy: 0.9778\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.1010 - accuracy: 0.9714 - val_loss: 0.0924 - val_accuracy: 0.9778\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.1006 - accuracy: 0.9714 - val_loss: 0.0956 - val_accuracy: 0.9778\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.0997 - accuracy: 0.9714 - val_loss: 0.0982 - val_accuracy: 0.9778\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.0995 - accuracy: 0.9714 - val_loss: 0.0994 - val_accuracy: 0.9778\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.1000 - accuracy: 0.9714 - val_loss: 0.0993 - val_accuracy: 0.9778\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.1001 - accuracy: 0.9810 - val_loss: 0.0940 - val_accuracy: 0.9778\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.0999 - accuracy: 0.9714 - val_loss: 0.0924 - val_accuracy: 0.9778\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.0994 - accuracy: 0.9714 - val_loss: 0.0945 - val_accuracy: 0.9778\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.0987 - accuracy: 0.9810 - val_loss: 0.0973 - val_accuracy: 0.9778\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.1004 - accuracy: 0.9714 - val_loss: 0.1028 - val_accuracy: 0.9778\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.0995 - accuracy: 0.9714 - val_loss: 0.0999 - val_accuracy: 0.9778\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.0991 - accuracy: 0.9714 - val_loss: 0.0975 - val_accuracy: 0.9778\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.0992 - accuracy: 0.9714 - val_loss: 0.0948 - val_accuracy: 0.9778\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.0987 - accuracy: 0.9810 - val_loss: 0.0935 - val_accuracy: 0.9778\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.0987 - accuracy: 0.9810 - val_loss: 0.0914 - val_accuracy: 0.9778\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.0991 - accuracy: 0.9714 - val_loss: 0.0875 - val_accuracy: 0.9778\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 0s 314us/sample - loss: 0.1007 - accuracy: 0.9714 - val_loss: 0.0873 - val_accuracy: 0.9778\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.1018 - accuracy: 0.9714 - val_loss: 0.0891 - val_accuracy: 0.9778\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.1000 - accuracy: 0.9714 - val_loss: 0.0885 - val_accuracy: 0.9778\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.0996 - accuracy: 0.9714 - val_loss: 0.0920 - val_accuracy: 0.9778\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.0977 - accuracy: 0.9810 - val_loss: 0.0957 - val_accuracy: 0.9778\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.0978 - accuracy: 0.9714 - val_loss: 0.1000 - val_accuracy: 0.9778\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.0981 - accuracy: 0.9714 - val_loss: 0.1062 - val_accuracy: 0.9778\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.0999 - accuracy: 0.9714 - val_loss: 0.1098 - val_accuracy: 0.9778\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 0s 311us/sample - loss: 0.1007 - accuracy: 0.9714 - val_loss: 0.1058 - val_accuracy: 0.9778\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.0993 - accuracy: 0.9714 - val_loss: 0.1043 - val_accuracy: 0.9778\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.0983 - accuracy: 0.9714 - val_loss: 0.0987 - val_accuracy: 0.9778\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.0969 - accuracy: 0.9714 - val_loss: 0.0941 - val_accuracy: 0.9778\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.0971 - accuracy: 0.9810 - val_loss: 0.0911 - val_accuracy: 0.9778\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.0977 - accuracy: 0.9714 - val_loss: 0.0883 - val_accuracy: 0.9778\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 0s 336us/sample - loss: 0.0976 - accuracy: 0.9714 - val_loss: 0.0886 - val_accuracy: 0.9778\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 0s 234us/sample - loss: 0.0977 - accuracy: 0.9714 - val_loss: 0.0911 - val_accuracy: 0.9778\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.0969 - accuracy: 0.9810 - val_loss: 0.0912 - val_accuracy: 0.9778\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 0s 291us/sample - loss: 0.0966 - accuracy: 0.9714 - val_loss: 0.0887 - val_accuracy: 0.9778\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.0977 - accuracy: 0.9714 - val_loss: 0.0875 - val_accuracy: 0.9778\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.0976 - accuracy: 0.9714 - val_loss: 0.0877 - val_accuracy: 0.9778\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.0965 - accuracy: 0.9714 - val_loss: 0.0908 - val_accuracy: 0.9778\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.0958 - accuracy: 0.9810 - val_loss: 0.0930 - val_accuracy: 0.9778\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.0959 - accuracy: 0.9714 - val_loss: 0.0946 - val_accuracy: 0.9778\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.0960 - accuracy: 0.9714 - val_loss: 0.0951 - val_accuracy: 0.9778\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.0962 - accuracy: 0.9714 - val_loss: 0.0960 - val_accuracy: 0.9778\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.0957 - accuracy: 0.9714 - val_loss: 0.0911 - val_accuracy: 0.9778\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 0.0949 - accuracy: 0.9810 - val_loss: 0.0864 - val_accuracy: 0.9778\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.0966 - accuracy: 0.9714 - val_loss: 0.0846 - val_accuracy: 0.9778\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.0984 - accuracy: 0.9714 - val_loss: 0.0833 - val_accuracy: 0.9778\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 0.0993 - accuracy: 0.9714 - val_loss: 0.0838 - val_accuracy: 0.9778\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 0s 303us/sample - loss: 0.0987 - accuracy: 0.9714 - val_loss: 0.0895 - val_accuracy: 0.9778\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.0950 - accuracy: 0.9810 - val_loss: 0.0954 - val_accuracy: 0.9778\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 0s 345us/sample - loss: 0.0949 - accuracy: 0.9714 - val_loss: 0.0996 - val_accuracy: 0.9778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f57f72a4198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km3ROLWBLQqv",
        "colab_type": "text"
      },
      "source": [
        "Faça o gráfico de acurácia e loss x epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTXz1X3z27_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "065c1f05-eef7-4ad3-8821-fe65af4a71e0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title('Loss versus Ephocs')\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.legend(['train', 'test'])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/weMIZu33J6f0gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKeUy0nE39UZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "2e5889a1-9a61-4285-b5d3-a6efa4ee002a"
      },
      "source": [
        "plt.title('Accuracy versus Ephocs')\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.legend(['accuracy', 'test'])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g3FVbJBKsrH",
        "colab_type": "code",
        "outputId": "6c356af4-8738-4b75-94e0-bd481c8f9194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict(X[:1])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6373685 , 0.25980306, 0.10282845]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMSJaBK64SNp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "51ab8a19-157e-4c44-ca60-40bb83092cb5"
      },
      "source": [
        "model.predict_classes(X[:100])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_iUiLI25D95",
        "colab_type": "text"
      },
      "source": [
        "Parte 2: montando o modelo com base nos pesos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6p9IKxd6L5L",
        "colab_type": "text"
      },
      "source": [
        "Usaremos a função ativadora RELU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO0zdXb-6PRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(x):\n",
        "  if x<=0:return 0.0\n",
        "  else: return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU6zJWKAHWbS",
        "colab_type": "text"
      },
      "source": [
        "Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_6C81daHZ3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perceptron(inp,w,b,f):\n",
        "  return f(np.inner(inp,w)+b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dem_FN3hzyB",
        "colab_type": "code",
        "outputId": "5cf90cb9-806d-45a8-afb2-05ce1ae96887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.74580956, -0.19567741],\n",
              "        [ 0.45900488, -0.02527924],\n",
              "        [ 0.5814996 ,  0.8346908 ],\n",
              "        [-0.29675865,  0.84747934]], dtype=float32),\n",
              " array([ 0.        , -0.34895247], dtype=float32),\n",
              " array([[-0.58017397,  0.08825731],\n",
              "        [-0.7011869 ,  1.350685  ]], dtype=float32),\n",
              " array([ 0.       , -0.3502947], dtype=float32),\n",
              " array([[-0.48637426, -0.68442154,  0.5989504 ],\n",
              "        [-0.603194  ,  0.27456725,  0.43675938]], dtype=float32),\n",
              " array([ 0.8821742 , -0.0500871 , -0.65853524], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MP_AOzWh7YI",
        "colab_type": "code",
        "outputId": "93891b4f-a30f-4d22-cec1-045eaf125309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "list_w=model.get_weights()\n",
        "w1=list_w[0]\n",
        "w1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.74580956, -0.19567741],\n",
              "       [ 0.45900488, -0.02527924],\n",
              "       [ 0.5814996 ,  0.8346908 ],\n",
              "       [-0.29675865,  0.84747934]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    }
  ]
}