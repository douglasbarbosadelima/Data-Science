{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stock_rnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douglasbarbosadelima/Data-Science/blob/master/stock_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k44HDMiO3i9u",
        "colab_type": "code",
        "outputId": "aebc58fb-e7b4-4a03-908d-86f44c1b66b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0 "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 31kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.27.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.34.2)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.17.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (45.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2keceUB04Fm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86147e46-4ec9-4c0a-8513-ac0280ee2d7d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo_PsLKjUJwQ",
        "colab_type": "code",
        "outputId": "fdbd920c-b902-4668-9805-202b537e30e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df=pd.read_csv('krot3.CSV',delimiter=';')\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data</th>\n",
              "      <th>Abertura</th>\n",
              "      <th>Fechamento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>07/01/2016</td>\n",
              "      <td>14.13</td>\n",
              "      <td>14.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>07/04/2016</td>\n",
              "      <td>14.40</td>\n",
              "      <td>14.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>07/05/2016</td>\n",
              "      <td>13.93</td>\n",
              "      <td>13.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>07/06/2016</td>\n",
              "      <td>13.90</td>\n",
              "      <td>13.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>07/07/2016</td>\n",
              "      <td>13.70</td>\n",
              "      <td>13.56</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Data  Abertura  Fechamento\n",
              "0  07/01/2016     14.13       14.28\n",
              "1  07/04/2016     14.40       14.08\n",
              "2  07/05/2016     13.93       13.85\n",
              "3  07/06/2016     13.90       13.57\n",
              "4  07/07/2016     13.70       13.56"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69eGo7edU5rI",
        "colab_type": "code",
        "outputId": "b9f2af86-fade-46b2-d40a-49d4fdfd96b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 177 entries, 0 to 176\n",
            "Data columns (total 3 columns):\n",
            "Data          177 non-null object\n",
            "Abertura      177 non-null float64\n",
            "Fechamento    177 non-null float64\n",
            "dtypes: float64(2), object(1)\n",
            "memory usage: 4.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf40z0PvVIts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind=df[df['Abertura']==0.0].index#há dias com movimentação 0.0, vamos eliminá-los"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRHlM4JzWFek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(ind,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4mH-U1pWgET",
        "colab_type": "code",
        "outputId": "c1a745c4-1d84-4352-af4a-fe1b34335b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "df[df['Abertura'] == 0.0]#checking is always good"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data</th>\n",
              "      <th>Abertura</th>\n",
              "      <th>Fechamento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Data, Abertura, Fechamento]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS1JsH7y557g",
        "colab_type": "text"
      },
      "source": [
        "Agora vamos criar sequências de 10 fechamentos, o target será o 11 fechamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH2fzhNi6IBg",
        "colab_type": "code",
        "outputId": "c6a6fbbb-e717-4502-be42-608e0dd9300b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "n=len(df)\n",
        "print(n)\n",
        "X=[]\n",
        "y=[]\n",
        "for i in range(n-10):\n",
        "  seq=[]\n",
        "  for j in range(10):\n",
        "    seq.append([df.iloc[i+j,2]])\n",
        "  X.append(seq)\n",
        "  y.append([df.iloc[i+10,2]])\n",
        "X=np.array(X)\n",
        "y=np.array(y)\n",
        "\n",
        "\n",
        "X[:3],y[:3]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[14.28],\n",
              "         [14.08],\n",
              "         [13.85],\n",
              "         [13.57],\n",
              "         [13.56],\n",
              "         [14.12],\n",
              "         [14.12],\n",
              "         [14.63],\n",
              "         [15.03],\n",
              "         [15.45]],\n",
              " \n",
              "        [[14.08],\n",
              "         [13.85],\n",
              "         [13.57],\n",
              "         [13.56],\n",
              "         [14.12],\n",
              "         [14.12],\n",
              "         [14.63],\n",
              "         [15.03],\n",
              "         [15.45],\n",
              "         [15.64]],\n",
              " \n",
              "        [[13.85],\n",
              "         [13.57],\n",
              "         [13.56],\n",
              "         [14.12],\n",
              "         [14.12],\n",
              "         [14.63],\n",
              "         [15.03],\n",
              "         [15.45],\n",
              "         [15.64],\n",
              "         [15.8 ]]]), array([[15.64],\n",
              "        [15.8 ],\n",
              "        [15.74]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "182B_zEA7xaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.SimpleRNN(4,input_shape=(None,1),activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1J-6uKU-B0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBQpzcGf7mLv",
        "colab_type": "code",
        "outputId": "8ae241a4-56ac-4aa5-ae9f-c0d67f075f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (None, 4)                 24        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 29\n",
            "Trainable params: 29\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WgfIH19-OKh",
        "colab_type": "code",
        "outputId": "9adf4fbd-52fd-466b-9145-91b05732e0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist=model.fit(X, y, epochs=300, validation_split=0.3)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 46 samples\n",
            "Epoch 1/300\n",
            "105/105 [==============================] - 1s 10ms/sample - loss: 172.3171 - mse: 172.3171 - val_loss: 139.4358 - val_mse: 139.4358\n",
            "Epoch 2/300\n",
            "105/105 [==============================] - 0s 479us/sample - loss: 169.0995 - mse: 169.0995 - val_loss: 137.3570 - val_mse: 137.3570\n",
            "Epoch 3/300\n",
            "105/105 [==============================] - 0s 528us/sample - loss: 166.7047 - mse: 166.7047 - val_loss: 135.5662 - val_mse: 135.5662\n",
            "Epoch 4/300\n",
            "105/105 [==============================] - 0s 486us/sample - loss: 164.5823 - mse: 164.5823 - val_loss: 133.8725 - val_mse: 133.8725\n",
            "Epoch 5/300\n",
            "105/105 [==============================] - 0s 432us/sample - loss: 162.5584 - mse: 162.5584 - val_loss: 132.2620 - val_mse: 132.2620\n",
            "Epoch 6/300\n",
            "105/105 [==============================] - 0s 432us/sample - loss: 160.6118 - mse: 160.6118 - val_loss: 130.6491 - val_mse: 130.6490\n",
            "Epoch 7/300\n",
            "105/105 [==============================] - 0s 406us/sample - loss: 158.6645 - mse: 158.6645 - val_loss: 129.0570 - val_mse: 129.0570\n",
            "Epoch 8/300\n",
            "105/105 [==============================] - 0s 440us/sample - loss: 156.7361 - mse: 156.7361 - val_loss: 127.4723 - val_mse: 127.4723\n",
            "Epoch 9/300\n",
            "105/105 [==============================] - 0s 477us/sample - loss: 154.8114 - mse: 154.8114 - val_loss: 125.8707 - val_mse: 125.8707\n",
            "Epoch 10/300\n",
            "105/105 [==============================] - 0s 535us/sample - loss: 152.8673 - mse: 152.8673 - val_loss: 124.2553 - val_mse: 124.2553\n",
            "Epoch 11/300\n",
            "105/105 [==============================] - 0s 514us/sample - loss: 150.9038 - mse: 150.9037 - val_loss: 122.6280 - val_mse: 122.6280\n",
            "Epoch 12/300\n",
            "105/105 [==============================] - 0s 420us/sample - loss: 148.9321 - mse: 148.9321 - val_loss: 121.0045 - val_mse: 121.0045\n",
            "Epoch 13/300\n",
            "105/105 [==============================] - 0s 457us/sample - loss: 146.9531 - mse: 146.9531 - val_loss: 119.3459 - val_mse: 119.3459\n",
            "Epoch 14/300\n",
            "105/105 [==============================] - 0s 529us/sample - loss: 144.9371 - mse: 144.9371 - val_loss: 117.6760 - val_mse: 117.6760\n",
            "Epoch 15/300\n",
            "105/105 [==============================] - 0s 497us/sample - loss: 142.9070 - mse: 142.9070 - val_loss: 115.9880 - val_mse: 115.9880\n",
            "Epoch 16/300\n",
            "105/105 [==============================] - 0s 531us/sample - loss: 140.8560 - mse: 140.8560 - val_loss: 114.2918 - val_mse: 114.2918\n",
            "Epoch 17/300\n",
            "105/105 [==============================] - 0s 484us/sample - loss: 138.7946 - mse: 138.7946 - val_loss: 112.5923 - val_mse: 112.5923\n",
            "Epoch 18/300\n",
            "105/105 [==============================] - 0s 418us/sample - loss: 136.7206 - mse: 136.7206 - val_loss: 110.8460 - val_mse: 110.8460\n",
            "Epoch 19/300\n",
            "105/105 [==============================] - 0s 511us/sample - loss: 134.5941 - mse: 134.5941 - val_loss: 109.0806 - val_mse: 109.0806\n",
            "Epoch 20/300\n",
            "105/105 [==============================] - 0s 426us/sample - loss: 132.4462 - mse: 132.4462 - val_loss: 107.2988 - val_mse: 107.2988\n",
            "Epoch 21/300\n",
            "105/105 [==============================] - 0s 404us/sample - loss: 130.2780 - mse: 130.2780 - val_loss: 105.4953 - val_mse: 105.4954\n",
            "Epoch 22/300\n",
            "105/105 [==============================] - 0s 438us/sample - loss: 128.0858 - mse: 128.0858 - val_loss: 103.6884 - val_mse: 103.6884\n",
            "Epoch 23/300\n",
            "105/105 [==============================] - 0s 602us/sample - loss: 125.8862 - mse: 125.8862 - val_loss: 101.8726 - val_mse: 101.8726\n",
            "Epoch 24/300\n",
            "105/105 [==============================] - 0s 494us/sample - loss: 123.6738 - mse: 123.6738 - val_loss: 100.0272 - val_mse: 100.0272\n",
            "Epoch 25/300\n",
            "105/105 [==============================] - 0s 455us/sample - loss: 121.4276 - mse: 121.4276 - val_loss: 98.1713 - val_mse: 98.1713\n",
            "Epoch 26/300\n",
            "105/105 [==============================] - 0s 463us/sample - loss: 119.1675 - mse: 119.1675 - val_loss: 96.3000 - val_mse: 96.3000\n",
            "Epoch 27/300\n",
            "105/105 [==============================] - 0s 545us/sample - loss: 116.8891 - mse: 116.8891 - val_loss: 94.4043 - val_mse: 94.4043\n",
            "Epoch 28/300\n",
            "105/105 [==============================] - 0s 480us/sample - loss: 114.5825 - mse: 114.5825 - val_loss: 92.4992 - val_mse: 92.4992\n",
            "Epoch 29/300\n",
            "105/105 [==============================] - 0s 459us/sample - loss: 112.2633 - mse: 112.2633 - val_loss: 90.5827 - val_mse: 90.5827\n",
            "Epoch 30/300\n",
            "105/105 [==============================] - 0s 444us/sample - loss: 109.9315 - mse: 109.9315 - val_loss: 88.6528 - val_mse: 88.6528\n",
            "Epoch 31/300\n",
            "105/105 [==============================] - 0s 439us/sample - loss: 107.5845 - mse: 107.5845 - val_loss: 86.7160 - val_mse: 86.7160\n",
            "Epoch 32/300\n",
            "105/105 [==============================] - 0s 515us/sample - loss: 105.2248 - mse: 105.2248 - val_loss: 84.7712 - val_mse: 84.7711\n",
            "Epoch 33/300\n",
            "105/105 [==============================] - 0s 490us/sample - loss: 102.8576 - mse: 102.8576 - val_loss: 82.8204 - val_mse: 82.8204\n",
            "Epoch 34/300\n",
            "105/105 [==============================] - 0s 401us/sample - loss: 100.4779 - mse: 100.4779 - val_loss: 80.8340 - val_mse: 80.8340\n",
            "Epoch 35/300\n",
            "105/105 [==============================] - 0s 451us/sample - loss: 98.0656 - mse: 98.0656 - val_loss: 78.8642 - val_mse: 78.8642\n",
            "Epoch 36/300\n",
            "105/105 [==============================] - 0s 519us/sample - loss: 95.6610 - mse: 95.6610 - val_loss: 76.8498 - val_mse: 76.8498\n",
            "Epoch 37/300\n",
            "105/105 [==============================] - 0s 645us/sample - loss: 93.2122 - mse: 93.2122 - val_loss: 74.8439 - val_mse: 74.8439\n",
            "Epoch 38/300\n",
            "105/105 [==============================] - 0s 403us/sample - loss: 90.7735 - mse: 90.7735 - val_loss: 72.8435 - val_mse: 72.8435\n",
            "Epoch 39/300\n",
            "105/105 [==============================] - 0s 461us/sample - loss: 88.3360 - mse: 88.3360 - val_loss: 70.8199 - val_mse: 70.8199\n",
            "Epoch 40/300\n",
            "105/105 [==============================] - 0s 411us/sample - loss: 85.8736 - mse: 85.8736 - val_loss: 68.7946 - val_mse: 68.7946\n",
            "Epoch 41/300\n",
            "105/105 [==============================] - 0s 409us/sample - loss: 83.4127 - mse: 83.4128 - val_loss: 66.7928 - val_mse: 66.7928\n",
            "Epoch 42/300\n",
            "105/105 [==============================] - 0s 426us/sample - loss: 80.9713 - mse: 80.9713 - val_loss: 64.7677 - val_mse: 64.7677\n",
            "Epoch 43/300\n",
            "105/105 [==============================] - 0s 642us/sample - loss: 78.5066 - mse: 78.5067 - val_loss: 62.7396 - val_mse: 62.7396\n",
            "Epoch 44/300\n",
            "105/105 [==============================] - 0s 422us/sample - loss: 76.0381 - mse: 76.0381 - val_loss: 60.7016 - val_mse: 60.7016\n",
            "Epoch 45/300\n",
            "105/105 [==============================] - 0s 493us/sample - loss: 73.5598 - mse: 73.5598 - val_loss: 58.6772 - val_mse: 58.6772\n",
            "Epoch 46/300\n",
            "105/105 [==============================] - 0s 466us/sample - loss: 71.0953 - mse: 71.0953 - val_loss: 56.6481 - val_mse: 56.6481\n",
            "Epoch 47/300\n",
            "105/105 [==============================] - 0s 523us/sample - loss: 68.6259 - mse: 68.6259 - val_loss: 54.6169 - val_mse: 54.6169\n",
            "Epoch 48/300\n",
            "105/105 [==============================] - 0s 499us/sample - loss: 66.1562 - mse: 66.1562 - val_loss: 52.6049 - val_mse: 52.6049\n",
            "Epoch 49/300\n",
            "105/105 [==============================] - 0s 432us/sample - loss: 63.7076 - mse: 63.7076 - val_loss: 50.5943 - val_mse: 50.5943\n",
            "Epoch 50/300\n",
            "105/105 [==============================] - 0s 491us/sample - loss: 61.2614 - mse: 61.2614 - val_loss: 48.5903 - val_mse: 48.5903\n",
            "Epoch 51/300\n",
            "105/105 [==============================] - 0s 514us/sample - loss: 58.8251 - mse: 58.8251 - val_loss: 46.6050 - val_mse: 46.6050\n",
            "Epoch 52/300\n",
            "105/105 [==============================] - 0s 527us/sample - loss: 56.4077 - mse: 56.4077 - val_loss: 44.6332 - val_mse: 44.6332\n",
            "Epoch 53/300\n",
            "105/105 [==============================] - 0s 414us/sample - loss: 54.0076 - mse: 54.0076 - val_loss: 42.6596 - val_mse: 42.6596\n",
            "Epoch 54/300\n",
            "105/105 [==============================] - 0s 389us/sample - loss: 51.6068 - mse: 51.6068 - val_loss: 40.7088 - val_mse: 40.7088\n",
            "Epoch 55/300\n",
            "105/105 [==============================] - 0s 455us/sample - loss: 49.2361 - mse: 49.2360 - val_loss: 38.7798 - val_mse: 38.7798\n",
            "Epoch 56/300\n",
            "105/105 [==============================] - 0s 442us/sample - loss: 46.8864 - mse: 46.8864 - val_loss: 36.8438 - val_mse: 36.8438\n",
            "Epoch 57/300\n",
            "105/105 [==============================] - 0s 515us/sample - loss: 44.5370 - mse: 44.5370 - val_loss: 34.9473 - val_mse: 34.9473\n",
            "Epoch 58/300\n",
            "105/105 [==============================] - 0s 524us/sample - loss: 42.2309 - mse: 42.2309 - val_loss: 33.0702 - val_mse: 33.0702\n",
            "Epoch 59/300\n",
            "105/105 [==============================] - 0s 461us/sample - loss: 39.9488 - mse: 39.9488 - val_loss: 31.2186 - val_mse: 31.2186\n",
            "Epoch 60/300\n",
            "105/105 [==============================] - 0s 487us/sample - loss: 37.6983 - mse: 37.6983 - val_loss: 29.3920 - val_mse: 29.3920\n",
            "Epoch 61/300\n",
            "105/105 [==============================] - 0s 431us/sample - loss: 35.4778 - mse: 35.4778 - val_loss: 27.5846 - val_mse: 27.5846\n",
            "Epoch 62/300\n",
            "105/105 [==============================] - 0s 451us/sample - loss: 33.2845 - mse: 33.2845 - val_loss: 25.8164 - val_mse: 25.8164\n",
            "Epoch 63/300\n",
            "105/105 [==============================] - 0s 446us/sample - loss: 31.1378 - mse: 31.1378 - val_loss: 24.0848 - val_mse: 24.0848\n",
            "Epoch 64/300\n",
            "105/105 [==============================] - 0s 527us/sample - loss: 29.0352 - mse: 29.0352 - val_loss: 22.3802 - val_mse: 22.3802\n",
            "Epoch 65/300\n",
            "105/105 [==============================] - 0s 559us/sample - loss: 26.9711 - mse: 26.9711 - val_loss: 20.7328 - val_mse: 20.7328\n",
            "Epoch 66/300\n",
            "105/105 [==============================] - 0s 452us/sample - loss: 24.9721 - mse: 24.9721 - val_loss: 19.1335 - val_mse: 19.1335\n",
            "Epoch 67/300\n",
            "105/105 [==============================] - 0s 490us/sample - loss: 23.0310 - mse: 23.0310 - val_loss: 17.5717 - val_mse: 17.5717\n",
            "Epoch 68/300\n",
            "105/105 [==============================] - 0s 478us/sample - loss: 21.1388 - mse: 21.1388 - val_loss: 16.0480 - val_mse: 16.0480\n",
            "Epoch 69/300\n",
            "105/105 [==============================] - 0s 434us/sample - loss: 19.2914 - mse: 19.2914 - val_loss: 14.5696 - val_mse: 14.5696\n",
            "Epoch 70/300\n",
            "105/105 [==============================] - 0s 412us/sample - loss: 17.5052 - mse: 17.5052 - val_loss: 13.1730 - val_mse: 13.1730\n",
            "Epoch 71/300\n",
            "105/105 [==============================] - 0s 463us/sample - loss: 15.8133 - mse: 15.8133 - val_loss: 11.8141 - val_mse: 11.8141\n",
            "Epoch 72/300\n",
            "105/105 [==============================] - 0s 444us/sample - loss: 14.1716 - mse: 14.1716 - val_loss: 10.5168 - val_mse: 10.5168\n",
            "Epoch 73/300\n",
            "105/105 [==============================] - 0s 475us/sample - loss: 12.6075 - mse: 12.6075 - val_loss: 9.2919 - val_mse: 9.2919\n",
            "Epoch 74/300\n",
            "105/105 [==============================] - 0s 442us/sample - loss: 11.1313 - mse: 11.1313 - val_loss: 8.1531 - val_mse: 8.1531\n",
            "Epoch 75/300\n",
            "105/105 [==============================] - 0s 434us/sample - loss: 9.7541 - mse: 9.7541 - val_loss: 7.0617 - val_mse: 7.0617\n",
            "Epoch 76/300\n",
            "105/105 [==============================] - 0s 495us/sample - loss: 8.4418 - mse: 8.4418 - val_loss: 6.0498 - val_mse: 6.0498\n",
            "Epoch 77/300\n",
            "105/105 [==============================] - 0s 467us/sample - loss: 7.2243 - mse: 7.2243 - val_loss: 5.1080 - val_mse: 5.1080\n",
            "Epoch 78/300\n",
            "105/105 [==============================] - 0s 565us/sample - loss: 6.0950 - mse: 6.0950 - val_loss: 4.2416 - val_mse: 4.2416\n",
            "Epoch 79/300\n",
            "105/105 [==============================] - 0s 421us/sample - loss: 5.0602 - mse: 5.0602 - val_loss: 3.4665 - val_mse: 3.4665\n",
            "Epoch 80/300\n",
            "105/105 [==============================] - 0s 492us/sample - loss: 4.1371 - mse: 4.1371 - val_loss: 2.7889 - val_mse: 2.7889\n",
            "Epoch 81/300\n",
            "105/105 [==============================] - 0s 508us/sample - loss: 3.3263 - mse: 3.3263 - val_loss: 2.1803 - val_mse: 2.1803\n",
            "Epoch 82/300\n",
            "105/105 [==============================] - 0s 497us/sample - loss: 2.6061 - mse: 2.6061 - val_loss: 1.6632 - val_mse: 1.6632\n",
            "Epoch 83/300\n",
            "105/105 [==============================] - 0s 434us/sample - loss: 1.9934 - mse: 1.9934 - val_loss: 1.2239 - val_mse: 1.2239\n",
            "Epoch 84/300\n",
            "105/105 [==============================] - 0s 511us/sample - loss: 1.4776 - mse: 1.4776 - val_loss: 0.8675 - val_mse: 0.8675\n",
            "Epoch 85/300\n",
            "105/105 [==============================] - 0s 474us/sample - loss: 1.0620 - mse: 1.0620 - val_loss: 0.5982 - val_mse: 0.5982\n",
            "Epoch 86/300\n",
            "105/105 [==============================] - 0s 477us/sample - loss: 0.7481 - mse: 0.7481 - val_loss: 0.3933 - val_mse: 0.3933\n",
            "Epoch 87/300\n",
            "105/105 [==============================] - 0s 403us/sample - loss: 0.5125 - mse: 0.5125 - val_loss: 0.2487 - val_mse: 0.2487\n",
            "Epoch 88/300\n",
            "105/105 [==============================] - 0s 394us/sample - loss: 0.3490 - mse: 0.3490 - val_loss: 0.1624 - val_mse: 0.1624\n",
            "Epoch 89/300\n",
            "105/105 [==============================] - 0s 624us/sample - loss: 0.2502 - mse: 0.2502 - val_loss: 0.1097 - val_mse: 0.1097\n",
            "Epoch 90/300\n",
            "105/105 [==============================] - 0s 651us/sample - loss: 0.1918 - mse: 0.1918 - val_loss: 0.0876 - val_mse: 0.0876\n",
            "Epoch 91/300\n",
            "105/105 [==============================] - 0s 524us/sample - loss: 0.1669 - mse: 0.1669 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "Epoch 92/300\n",
            "105/105 [==============================] - 0s 465us/sample - loss: 0.1593 - mse: 0.1593 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "Epoch 93/300\n",
            "105/105 [==============================] - 0s 425us/sample - loss: 0.1572 - mse: 0.1572 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 94/300\n",
            "105/105 [==============================] - 0s 469us/sample - loss: 0.1567 - mse: 0.1567 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 95/300\n",
            "105/105 [==============================] - 0s 435us/sample - loss: 0.1556 - mse: 0.1556 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 96/300\n",
            "105/105 [==============================] - 0s 410us/sample - loss: 0.1561 - mse: 0.1561 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "Epoch 97/300\n",
            "105/105 [==============================] - 0s 475us/sample - loss: 0.1560 - mse: 0.1560 - val_loss: 0.0815 - val_mse: 0.0815\n",
            "Epoch 98/300\n",
            "105/105 [==============================] - 0s 603us/sample - loss: 0.1580 - mse: 0.1580 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 99/300\n",
            "105/105 [==============================] - 0s 496us/sample - loss: 0.1551 - mse: 0.1551 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "Epoch 100/300\n",
            "105/105 [==============================] - 0s 606us/sample - loss: 0.1553 - mse: 0.1553 - val_loss: 0.0957 - val_mse: 0.0957\n",
            "Epoch 101/300\n",
            "105/105 [==============================] - 0s 543us/sample - loss: 0.1589 - mse: 0.1589 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 102/300\n",
            "105/105 [==============================] - 0s 420us/sample - loss: 0.1567 - mse: 0.1567 - val_loss: 0.0937 - val_mse: 0.0937\n",
            "Epoch 103/300\n",
            "105/105 [==============================] - 0s 533us/sample - loss: 0.1669 - mse: 0.1669 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "Epoch 104/300\n",
            "105/105 [==============================] - 0s 442us/sample - loss: 0.1581 - mse: 0.1581 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "Epoch 105/300\n",
            "105/105 [==============================] - 0s 400us/sample - loss: 0.1588 - mse: 0.1588 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "Epoch 106/300\n",
            "105/105 [==============================] - 0s 457us/sample - loss: 0.1573 - mse: 0.1573 - val_loss: 0.0897 - val_mse: 0.0897\n",
            "Epoch 107/300\n",
            "105/105 [==============================] - 0s 467us/sample - loss: 0.1584 - mse: 0.1584 - val_loss: 0.0960 - val_mse: 0.0960\n",
            "Epoch 108/300\n",
            "105/105 [==============================] - 0s 408us/sample - loss: 0.1686 - mse: 0.1686 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "Epoch 109/300\n",
            "105/105 [==============================] - 0s 416us/sample - loss: 0.1566 - mse: 0.1566 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "Epoch 110/300\n",
            "105/105 [==============================] - 0s 480us/sample - loss: 0.1558 - mse: 0.1558 - val_loss: 0.0849 - val_mse: 0.0849\n",
            "Epoch 111/300\n",
            "105/105 [==============================] - 0s 495us/sample - loss: 0.1564 - mse: 0.1564 - val_loss: 0.0834 - val_mse: 0.0834\n",
            "Epoch 112/300\n",
            "105/105 [==============================] - 0s 478us/sample - loss: 0.1636 - mse: 0.1636 - val_loss: 0.0820 - val_mse: 0.0820\n",
            "Epoch 113/300\n",
            "105/105 [==============================] - 0s 471us/sample - loss: 0.1599 - mse: 0.1599 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "Epoch 114/300\n",
            "105/105 [==============================] - 0s 434us/sample - loss: 0.1617 - mse: 0.1617 - val_loss: 0.0837 - val_mse: 0.0837\n",
            "Epoch 115/300\n",
            "105/105 [==============================] - 0s 409us/sample - loss: 0.1601 - mse: 0.1601 - val_loss: 0.0936 - val_mse: 0.0936\n",
            "Epoch 116/300\n",
            "105/105 [==============================] - 0s 425us/sample - loss: 0.1635 - mse: 0.1635 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "Epoch 117/300\n",
            "105/105 [==============================] - 0s 472us/sample - loss: 0.1636 - mse: 0.1636 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "Epoch 118/300\n",
            "105/105 [==============================] - 0s 497us/sample - loss: 0.1584 - mse: 0.1584 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "Epoch 119/300\n",
            "105/105 [==============================] - 0s 469us/sample - loss: 0.1575 - mse: 0.1575 - val_loss: 0.0916 - val_mse: 0.0916\n",
            "Epoch 120/300\n",
            "105/105 [==============================] - 0s 485us/sample - loss: 0.1718 - mse: 0.1718 - val_loss: 0.0857 - val_mse: 0.0857\n",
            "Epoch 121/300\n",
            "105/105 [==============================] - 0s 420us/sample - loss: 0.1598 - mse: 0.1598 - val_loss: 0.0946 - val_mse: 0.0946\n",
            "Epoch 122/300\n",
            "105/105 [==============================] - 0s 541us/sample - loss: 0.1607 - mse: 0.1607 - val_loss: 0.0823 - val_mse: 0.0823\n",
            "Epoch 123/300\n",
            "105/105 [==============================] - 0s 492us/sample - loss: 0.1616 - mse: 0.1616 - val_loss: 0.0904 - val_mse: 0.0904\n",
            "Epoch 124/300\n",
            "105/105 [==============================] - 0s 447us/sample - loss: 0.1600 - mse: 0.1600 - val_loss: 0.0843 - val_mse: 0.0843\n",
            "Epoch 125/300\n",
            "105/105 [==============================] - 0s 476us/sample - loss: 0.1552 - mse: 0.1552 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 126/300\n",
            "105/105 [==============================] - 0s 562us/sample - loss: 0.1583 - mse: 0.1583 - val_loss: 0.0830 - val_mse: 0.0830\n",
            "Epoch 127/300\n",
            "105/105 [==============================] - 0s 469us/sample - loss: 0.1553 - mse: 0.1553 - val_loss: 0.0850 - val_mse: 0.0850\n",
            "Epoch 128/300\n",
            "105/105 [==============================] - 0s 502us/sample - loss: 0.1567 - mse: 0.1567 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "Epoch 129/300\n",
            "105/105 [==============================] - 0s 420us/sample - loss: 0.1548 - mse: 0.1548 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 130/300\n",
            "105/105 [==============================] - 0s 402us/sample - loss: 0.1621 - mse: 0.1621 - val_loss: 0.0829 - val_mse: 0.0829\n",
            "Epoch 131/300\n",
            "105/105 [==============================] - 0s 484us/sample - loss: 0.1610 - mse: 0.1610 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "Epoch 132/300\n",
            "105/105 [==============================] - 0s 455us/sample - loss: 0.1562 - mse: 0.1562 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 133/300\n",
            "105/105 [==============================] - 0s 472us/sample - loss: 0.1665 - mse: 0.1665 - val_loss: 0.0971 - val_mse: 0.0971\n",
            "Epoch 134/300\n",
            "105/105 [==============================] - 0s 497us/sample - loss: 0.1683 - mse: 0.1683 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "Epoch 135/300\n",
            "105/105 [==============================] - 0s 439us/sample - loss: 0.1572 - mse: 0.1572 - val_loss: 0.0860 - val_mse: 0.0860\n",
            "Epoch 136/300\n",
            "105/105 [==============================] - 0s 457us/sample - loss: 0.1575 - mse: 0.1575 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "Epoch 137/300\n",
            "105/105 [==============================] - 0s 444us/sample - loss: 0.1579 - mse: 0.1579 - val_loss: 0.0976 - val_mse: 0.0976\n",
            "Epoch 138/300\n",
            "105/105 [==============================] - 0s 484us/sample - loss: 0.1656 - mse: 0.1656 - val_loss: 0.0831 - val_mse: 0.0831\n",
            "Epoch 139/300\n",
            "105/105 [==============================] - 0s 585us/sample - loss: 0.1581 - mse: 0.1581 - val_loss: 0.0809 - val_mse: 0.0809\n",
            "Epoch 140/300\n",
            "105/105 [==============================] - 0s 615us/sample - loss: 0.1572 - mse: 0.1572 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "Epoch 141/300\n",
            "105/105 [==============================] - 0s 441us/sample - loss: 0.1601 - mse: 0.1601 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "Epoch 142/300\n",
            "105/105 [==============================] - 0s 463us/sample - loss: 0.1577 - mse: 0.1577 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "Epoch 143/300\n",
            "105/105 [==============================] - 0s 569us/sample - loss: 0.1696 - mse: 0.1696 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 144/300\n",
            "105/105 [==============================] - 0s 488us/sample - loss: 0.1566 - mse: 0.1566 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "Epoch 145/300\n",
            "105/105 [==============================] - 0s 449us/sample - loss: 0.1555 - mse: 0.1555 - val_loss: 0.0976 - val_mse: 0.0976\n",
            "Epoch 146/300\n",
            "105/105 [==============================] - 0s 468us/sample - loss: 0.1667 - mse: 0.1667 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "Epoch 147/300\n",
            "105/105 [==============================] - 0s 421us/sample - loss: 0.1557 - mse: 0.1557 - val_loss: 0.0934 - val_mse: 0.0934\n",
            "Epoch 148/300\n",
            "105/105 [==============================] - 0s 491us/sample - loss: 0.1609 - mse: 0.1609 - val_loss: 0.0964 - val_mse: 0.0964\n",
            "Epoch 149/300\n",
            "105/105 [==============================] - 0s 443us/sample - loss: 0.1607 - mse: 0.1607 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 150/300\n",
            "105/105 [==============================] - 0s 450us/sample - loss: 0.1586 - mse: 0.1586 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 151/300\n",
            "105/105 [==============================] - 0s 445us/sample - loss: 0.1591 - mse: 0.1591 - val_loss: 0.0895 - val_mse: 0.0895\n",
            "Epoch 152/300\n",
            "105/105 [==============================] - 0s 529us/sample - loss: 0.1625 - mse: 0.1625 - val_loss: 0.1053 - val_mse: 0.1053\n",
            "Epoch 153/300\n",
            "105/105 [==============================] - 0s 446us/sample - loss: 0.1690 - mse: 0.1690 - val_loss: 0.0838 - val_mse: 0.0838\n",
            "Epoch 154/300\n",
            "105/105 [==============================] - 0s 484us/sample - loss: 0.1553 - mse: 0.1553 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "Epoch 155/300\n",
            "105/105 [==============================] - 0s 524us/sample - loss: 0.1586 - mse: 0.1586 - val_loss: 0.0835 - val_mse: 0.0835\n",
            "Epoch 156/300\n",
            "105/105 [==============================] - 0s 479us/sample - loss: 0.1732 - mse: 0.1732 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 157/300\n",
            "105/105 [==============================] - 0s 432us/sample - loss: 0.1570 - mse: 0.1570 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "Epoch 158/300\n",
            "105/105 [==============================] - 0s 439us/sample - loss: 0.1616 - mse: 0.1616 - val_loss: 0.0914 - val_mse: 0.0914\n",
            "Epoch 159/300\n",
            "105/105 [==============================] - 0s 515us/sample - loss: 0.1604 - mse: 0.1604 - val_loss: 0.0864 - val_mse: 0.0864\n",
            "Epoch 160/300\n",
            "105/105 [==============================] - 0s 498us/sample - loss: 0.1573 - mse: 0.1573 - val_loss: 0.0928 - val_mse: 0.0928\n",
            "Epoch 161/300\n",
            "105/105 [==============================] - 0s 521us/sample - loss: 0.1609 - mse: 0.1609 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 162/300\n",
            "105/105 [==============================] - 0s 451us/sample - loss: 0.1627 - mse: 0.1627 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 163/300\n",
            "105/105 [==============================] - 0s 490us/sample - loss: 0.1556 - mse: 0.1556 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "Epoch 164/300\n",
            "105/105 [==============================] - 0s 420us/sample - loss: 0.1589 - mse: 0.1589 - val_loss: 0.0962 - val_mse: 0.0962\n",
            "Epoch 165/300\n",
            "105/105 [==============================] - 0s 423us/sample - loss: 0.1599 - mse: 0.1599 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "Epoch 166/300\n",
            "105/105 [==============================] - 0s 496us/sample - loss: 0.1574 - mse: 0.1574 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "Epoch 167/300\n",
            "105/105 [==============================] - 0s 509us/sample - loss: 0.1616 - mse: 0.1616 - val_loss: 0.0966 - val_mse: 0.0966\n",
            "Epoch 168/300\n",
            "105/105 [==============================] - 0s 409us/sample - loss: 0.1646 - mse: 0.1646 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 169/300\n",
            "105/105 [==============================] - 0s 460us/sample - loss: 0.1563 - mse: 0.1563 - val_loss: 0.0816 - val_mse: 0.0816\n",
            "Epoch 170/300\n",
            "105/105 [==============================] - 0s 596us/sample - loss: 0.1602 - mse: 0.1602 - val_loss: 0.0833 - val_mse: 0.0833\n",
            "Epoch 171/300\n",
            "105/105 [==============================] - 0s 638us/sample - loss: 0.1573 - mse: 0.1573 - val_loss: 0.1024 - val_mse: 0.1024\n",
            "Epoch 172/300\n",
            "105/105 [==============================] - 0s 470us/sample - loss: 0.1579 - mse: 0.1579 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "Epoch 173/300\n",
            "105/105 [==============================] - 0s 416us/sample - loss: 0.1568 - mse: 0.1568 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "Epoch 174/300\n",
            "105/105 [==============================] - 0s 448us/sample - loss: 0.1577 - mse: 0.1577 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 175/300\n",
            "105/105 [==============================] - 0s 453us/sample - loss: 0.1560 - mse: 0.1560 - val_loss: 0.0850 - val_mse: 0.0850\n",
            "Epoch 176/300\n",
            "105/105 [==============================] - 0s 442us/sample - loss: 0.1569 - mse: 0.1569 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 177/300\n",
            "105/105 [==============================] - 0s 467us/sample - loss: 0.1571 - mse: 0.1571 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 178/300\n",
            "105/105 [==============================] - 0s 461us/sample - loss: 0.1545 - mse: 0.1545 - val_loss: 0.0846 - val_mse: 0.0846\n",
            "Epoch 179/300\n",
            "105/105 [==============================] - 0s 506us/sample - loss: 0.1552 - mse: 0.1552 - val_loss: 0.0888 - val_mse: 0.0888\n",
            "Epoch 180/300\n",
            "105/105 [==============================] - 0s 563us/sample - loss: 0.1661 - mse: 0.1661 - val_loss: 0.0891 - val_mse: 0.0891\n",
            "Epoch 181/300\n",
            "105/105 [==============================] - 0s 523us/sample - loss: 0.1563 - mse: 0.1563 - val_loss: 0.0876 - val_mse: 0.0876\n",
            "Epoch 182/300\n",
            "105/105 [==============================] - 0s 436us/sample - loss: 0.1586 - mse: 0.1586 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 183/300\n",
            "105/105 [==============================] - 0s 526us/sample - loss: 0.1552 - mse: 0.1552 - val_loss: 0.0918 - val_mse: 0.0918\n",
            "Epoch 184/300\n",
            "105/105 [==============================] - 0s 516us/sample - loss: 0.1597 - mse: 0.1597 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 185/300\n",
            "105/105 [==============================] - 0s 444us/sample - loss: 0.1637 - mse: 0.1637 - val_loss: 0.1085 - val_mse: 0.1085\n",
            "Epoch 186/300\n",
            "105/105 [==============================] - 0s 501us/sample - loss: 0.1597 - mse: 0.1597 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "Epoch 187/300\n",
            "105/105 [==============================] - 0s 475us/sample - loss: 0.1639 - mse: 0.1639 - val_loss: 0.0817 - val_mse: 0.0817\n",
            "Epoch 188/300\n",
            "105/105 [==============================] - 0s 420us/sample - loss: 0.1560 - mse: 0.1560 - val_loss: 0.0875 - val_mse: 0.0875\n",
            "Epoch 189/300\n",
            "105/105 [==============================] - 0s 392us/sample - loss: 0.1623 - mse: 0.1623 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 190/300\n",
            "105/105 [==============================] - 0s 507us/sample - loss: 0.1569 - mse: 0.1569 - val_loss: 0.0898 - val_mse: 0.0898\n",
            "Epoch 191/300\n",
            "105/105 [==============================] - 0s 433us/sample - loss: 0.1649 - mse: 0.1649 - val_loss: 0.0832 - val_mse: 0.0832\n",
            "Epoch 192/300\n",
            "105/105 [==============================] - 0s 385us/sample - loss: 0.1588 - mse: 0.1588 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "Epoch 193/300\n",
            "105/105 [==============================] - 0s 456us/sample - loss: 0.1582 - mse: 0.1582 - val_loss: 0.0875 - val_mse: 0.0875\n",
            "Epoch 194/300\n",
            "105/105 [==============================] - 0s 531us/sample - loss: 0.1612 - mse: 0.1612 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 195/300\n",
            "105/105 [==============================] - 0s 407us/sample - loss: 0.1547 - mse: 0.1547 - val_loss: 0.0832 - val_mse: 0.0832\n",
            "Epoch 196/300\n",
            "105/105 [==============================] - 0s 427us/sample - loss: 0.1564 - mse: 0.1564 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "Epoch 197/300\n",
            "105/105 [==============================] - 0s 465us/sample - loss: 0.1615 - mse: 0.1615 - val_loss: 0.0821 - val_mse: 0.0821\n",
            "Epoch 198/300\n",
            "105/105 [==============================] - 0s 506us/sample - loss: 0.1632 - mse: 0.1632 - val_loss: 0.0843 - val_mse: 0.0843\n",
            "Epoch 199/300\n",
            "105/105 [==============================] - 0s 423us/sample - loss: 0.1609 - mse: 0.1609 - val_loss: 0.0828 - val_mse: 0.0828\n",
            "Epoch 200/300\n",
            "105/105 [==============================] - 0s 495us/sample - loss: 0.1571 - mse: 0.1571 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 201/300\n",
            "105/105 [==============================] - 0s 533us/sample - loss: 0.1566 - mse: 0.1566 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "Epoch 202/300\n",
            "105/105 [==============================] - 0s 489us/sample - loss: 0.1582 - mse: 0.1582 - val_loss: 0.0845 - val_mse: 0.0845\n",
            "Epoch 203/300\n",
            "105/105 [==============================] - 0s 463us/sample - loss: 0.1588 - mse: 0.1588 - val_loss: 0.0832 - val_mse: 0.0832\n",
            "Epoch 204/300\n",
            "105/105 [==============================] - 0s 482us/sample - loss: 0.1653 - mse: 0.1653 - val_loss: 0.0856 - val_mse: 0.0856\n",
            "Epoch 205/300\n",
            "105/105 [==============================] - 0s 523us/sample - loss: 0.1591 - mse: 0.1591 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "Epoch 206/300\n",
            "105/105 [==============================] - 0s 452us/sample - loss: 0.1554 - mse: 0.1554 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "Epoch 207/300\n",
            "105/105 [==============================] - 0s 450us/sample - loss: 0.1595 - mse: 0.1595 - val_loss: 0.0875 - val_mse: 0.0875\n",
            "Epoch 208/300\n",
            "105/105 [==============================] - 0s 463us/sample - loss: 0.1586 - mse: 0.1586 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "Epoch 209/300\n",
            "105/105 [==============================] - 0s 510us/sample - loss: 0.1557 - mse: 0.1557 - val_loss: 0.0860 - val_mse: 0.0860\n",
            "Epoch 210/300\n",
            "105/105 [==============================] - 0s 421us/sample - loss: 0.1570 - mse: 0.1570 - val_loss: 0.1060 - val_mse: 0.1060\n",
            "Epoch 211/300\n",
            "105/105 [==============================] - 0s 435us/sample - loss: 0.1679 - mse: 0.1679 - val_loss: 0.0799 - val_mse: 0.0799\n",
            "Epoch 212/300\n",
            "105/105 [==============================] - 0s 468us/sample - loss: 0.1546 - mse: 0.1546 - val_loss: 0.0938 - val_mse: 0.0938\n",
            "Epoch 213/300\n",
            "105/105 [==============================] - 0s 470us/sample - loss: 0.1571 - mse: 0.1571 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 214/300\n",
            "105/105 [==============================] - 0s 503us/sample - loss: 0.1547 - mse: 0.1547 - val_loss: 0.0922 - val_mse: 0.0922\n",
            "Epoch 215/300\n",
            "105/105 [==============================] - 0s 491us/sample - loss: 0.1591 - mse: 0.1591 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "Epoch 216/300\n",
            "105/105 [==============================] - 0s 407us/sample - loss: 0.1651 - mse: 0.1651 - val_loss: 0.0903 - val_mse: 0.0903\n",
            "Epoch 217/300\n",
            "105/105 [==============================] - 0s 425us/sample - loss: 0.1625 - mse: 0.1625 - val_loss: 0.0824 - val_mse: 0.0824\n",
            "Epoch 218/300\n",
            "105/105 [==============================] - 0s 482us/sample - loss: 0.1564 - mse: 0.1564 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 219/300\n",
            "105/105 [==============================] - 0s 458us/sample - loss: 0.1564 - mse: 0.1564 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 220/300\n",
            "105/105 [==============================] - 0s 456us/sample - loss: 0.1558 - mse: 0.1558 - val_loss: 0.0921 - val_mse: 0.0921\n",
            "Epoch 221/300\n",
            "105/105 [==============================] - 0s 599us/sample - loss: 0.1664 - mse: 0.1664 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "Epoch 222/300\n",
            "105/105 [==============================] - 0s 475us/sample - loss: 0.1611 - mse: 0.1611 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "Epoch 223/300\n",
            "105/105 [==============================] - 0s 401us/sample - loss: 0.1547 - mse: 0.1547 - val_loss: 0.0968 - val_mse: 0.0968\n",
            "Epoch 224/300\n",
            "105/105 [==============================] - 0s 451us/sample - loss: 0.1643 - mse: 0.1643 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 225/300\n",
            "105/105 [==============================] - 0s 422us/sample - loss: 0.1552 - mse: 0.1552 - val_loss: 0.0819 - val_mse: 0.0819\n",
            "Epoch 226/300\n",
            "105/105 [==============================] - 0s 559us/sample - loss: 0.1555 - mse: 0.1555 - val_loss: 0.0867 - val_mse: 0.0867\n",
            "Epoch 227/300\n",
            "105/105 [==============================] - 0s 667us/sample - loss: 0.1602 - mse: 0.1602 - val_loss: 0.0938 - val_mse: 0.0938\n",
            "Epoch 228/300\n",
            "105/105 [==============================] - 0s 518us/sample - loss: 0.1617 - mse: 0.1617 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "Epoch 229/300\n",
            "105/105 [==============================] - 0s 563us/sample - loss: 0.1570 - mse: 0.1570 - val_loss: 0.0886 - val_mse: 0.0886\n",
            "Epoch 230/300\n",
            "105/105 [==============================] - 0s 438us/sample - loss: 0.1584 - mse: 0.1584 - val_loss: 0.0877 - val_mse: 0.0877\n",
            "Epoch 231/300\n",
            "105/105 [==============================] - 0s 407us/sample - loss: 0.1600 - mse: 0.1600 - val_loss: 0.1044 - val_mse: 0.1044\n",
            "Epoch 232/300\n",
            "105/105 [==============================] - 0s 409us/sample - loss: 0.1628 - mse: 0.1628 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "Epoch 233/300\n",
            "105/105 [==============================] - 0s 472us/sample - loss: 0.1649 - mse: 0.1649 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "Epoch 234/300\n",
            "105/105 [==============================] - 0s 427us/sample - loss: 0.1570 - mse: 0.1570 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 235/300\n",
            "105/105 [==============================] - 0s 395us/sample - loss: 0.1585 - mse: 0.1585 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 236/300\n",
            "105/105 [==============================] - 0s 382us/sample - loss: 0.1585 - mse: 0.1585 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "Epoch 237/300\n",
            "105/105 [==============================] - 0s 491us/sample - loss: 0.1557 - mse: 0.1557 - val_loss: 0.0832 - val_mse: 0.0832\n",
            "Epoch 238/300\n",
            "105/105 [==============================] - 0s 446us/sample - loss: 0.1599 - mse: 0.1599 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "Epoch 239/300\n",
            "105/105 [==============================] - 0s 389us/sample - loss: 0.1570 - mse: 0.1570 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "Epoch 240/300\n",
            "105/105 [==============================] - 0s 443us/sample - loss: 0.1586 - mse: 0.1586 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "Epoch 241/300\n",
            "105/105 [==============================] - 0s 589us/sample - loss: 0.1603 - mse: 0.1603 - val_loss: 0.0827 - val_mse: 0.0827\n",
            "Epoch 242/300\n",
            "105/105 [==============================] - 0s 589us/sample - loss: 0.1687 - mse: 0.1687 - val_loss: 0.0844 - val_mse: 0.0844\n",
            "Epoch 243/300\n",
            "105/105 [==============================] - 0s 493us/sample - loss: 0.1600 - mse: 0.1600 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 244/300\n",
            "105/105 [==============================] - 0s 411us/sample - loss: 0.1562 - mse: 0.1562 - val_loss: 0.0794 - val_mse: 0.0794\n",
            "Epoch 245/300\n",
            "105/105 [==============================] - 0s 429us/sample - loss: 0.1577 - mse: 0.1577 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 246/300\n",
            "105/105 [==============================] - 0s 454us/sample - loss: 0.1610 - mse: 0.1610 - val_loss: 0.0832 - val_mse: 0.0832\n",
            "Epoch 247/300\n",
            "105/105 [==============================] - 0s 450us/sample - loss: 0.1551 - mse: 0.1551 - val_loss: 0.0797 - val_mse: 0.0797\n",
            "Epoch 248/300\n",
            "105/105 [==============================] - 0s 384us/sample - loss: 0.1541 - mse: 0.1541 - val_loss: 0.1112 - val_mse: 0.1112\n",
            "Epoch 249/300\n",
            "105/105 [==============================] - 0s 464us/sample - loss: 0.1674 - mse: 0.1674 - val_loss: 0.0800 - val_mse: 0.0800\n",
            "Epoch 250/300\n",
            "105/105 [==============================] - 0s 469us/sample - loss: 0.1577 - mse: 0.1577 - val_loss: 0.0826 - val_mse: 0.0826\n",
            "Epoch 251/300\n",
            "105/105 [==============================] - 0s 455us/sample - loss: 0.1558 - mse: 0.1558 - val_loss: 0.0833 - val_mse: 0.0833\n",
            "Epoch 252/300\n",
            "105/105 [==============================] - 0s 433us/sample - loss: 0.1538 - mse: 0.1538 - val_loss: 0.0986 - val_mse: 0.0986\n",
            "Epoch 253/300\n",
            "105/105 [==============================] - 0s 441us/sample - loss: 0.1749 - mse: 0.1749 - val_loss: 0.0814 - val_mse: 0.0814\n",
            "Epoch 254/300\n",
            "105/105 [==============================] - 0s 459us/sample - loss: 0.1590 - mse: 0.1590 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "Epoch 255/300\n",
            "105/105 [==============================] - 0s 427us/sample - loss: 0.1599 - mse: 0.1599 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "Epoch 256/300\n",
            "105/105 [==============================] - 0s 435us/sample - loss: 0.1563 - mse: 0.1563 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 257/300\n",
            "105/105 [==============================] - 0s 408us/sample - loss: 0.1555 - mse: 0.1555 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 258/300\n",
            "105/105 [==============================] - 0s 448us/sample - loss: 0.1554 - mse: 0.1554 - val_loss: 0.1028 - val_mse: 0.1028\n",
            "Epoch 259/300\n",
            "105/105 [==============================] - 0s 434us/sample - loss: 0.1624 - mse: 0.1624 - val_loss: 0.0805 - val_mse: 0.0805\n",
            "Epoch 260/300\n",
            "105/105 [==============================] - 0s 409us/sample - loss: 0.1546 - mse: 0.1546 - val_loss: 0.0801 - val_mse: 0.0801\n",
            "Epoch 261/300\n",
            "105/105 [==============================] - 0s 438us/sample - loss: 0.1556 - mse: 0.1556 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "Epoch 262/300\n",
            "105/105 [==============================] - 0s 537us/sample - loss: 0.1583 - mse: 0.1583 - val_loss: 0.0941 - val_mse: 0.0941\n",
            "Epoch 263/300\n",
            "105/105 [==============================] - 0s 493us/sample - loss: 0.1584 - mse: 0.1584 - val_loss: 0.0863 - val_mse: 0.0863\n",
            "Epoch 264/300\n",
            "105/105 [==============================] - 0s 543us/sample - loss: 0.1594 - mse: 0.1594 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 265/300\n",
            "105/105 [==============================] - 0s 417us/sample - loss: 0.1560 - mse: 0.1560 - val_loss: 0.0849 - val_mse: 0.0849\n",
            "Epoch 266/300\n",
            "105/105 [==============================] - 0s 496us/sample - loss: 0.1537 - mse: 0.1537 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "Epoch 267/300\n",
            "105/105 [==============================] - 0s 453us/sample - loss: 0.1577 - mse: 0.1577 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 268/300\n",
            "105/105 [==============================] - 0s 403us/sample - loss: 0.1562 - mse: 0.1562 - val_loss: 0.0993 - val_mse: 0.0993\n",
            "Epoch 269/300\n",
            "105/105 [==============================] - 0s 505us/sample - loss: 0.1609 - mse: 0.1609 - val_loss: 0.0887 - val_mse: 0.0887\n",
            "Epoch 270/300\n",
            "105/105 [==============================] - 0s 552us/sample - loss: 0.1587 - mse: 0.1587 - val_loss: 0.0874 - val_mse: 0.0874\n",
            "Epoch 271/300\n",
            "105/105 [==============================] - 0s 466us/sample - loss: 0.1666 - mse: 0.1666 - val_loss: 0.0870 - val_mse: 0.0870\n",
            "Epoch 272/300\n",
            "105/105 [==============================] - 0s 409us/sample - loss: 0.1562 - mse: 0.1562 - val_loss: 0.0795 - val_mse: 0.0795\n",
            "Epoch 273/300\n",
            "105/105 [==============================] - 0s 492us/sample - loss: 0.1558 - mse: 0.1558 - val_loss: 0.0803 - val_mse: 0.0803\n",
            "Epoch 274/300\n",
            "105/105 [==============================] - 0s 483us/sample - loss: 0.1571 - mse: 0.1571 - val_loss: 0.0796 - val_mse: 0.0796\n",
            "Epoch 275/300\n",
            "105/105 [==============================] - 0s 432us/sample - loss: 0.1594 - mse: 0.1594 - val_loss: 0.0860 - val_mse: 0.0860\n",
            "Epoch 276/300\n",
            "105/105 [==============================] - 0s 426us/sample - loss: 0.1571 - mse: 0.1571 - val_loss: 0.1019 - val_mse: 0.1019\n",
            "Epoch 277/300\n",
            "105/105 [==============================] - 0s 433us/sample - loss: 0.1658 - mse: 0.1658 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "Epoch 278/300\n",
            "105/105 [==============================] - 0s 427us/sample - loss: 0.1567 - mse: 0.1567 - val_loss: 0.0840 - val_mse: 0.0840\n",
            "Epoch 279/300\n",
            "105/105 [==============================] - 0s 462us/sample - loss: 0.1611 - mse: 0.1611 - val_loss: 0.0881 - val_mse: 0.0881\n",
            "Epoch 280/300\n",
            "105/105 [==============================] - 0s 410us/sample - loss: 0.1556 - mse: 0.1556 - val_loss: 0.0793 - val_mse: 0.0793\n",
            "Epoch 281/300\n",
            "105/105 [==============================] - 0s 398us/sample - loss: 0.1556 - mse: 0.1556 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 282/300\n",
            "105/105 [==============================] - 0s 402us/sample - loss: 0.1562 - mse: 0.1562 - val_loss: 0.0864 - val_mse: 0.0864\n",
            "Epoch 283/300\n",
            "105/105 [==============================] - 0s 528us/sample - loss: 0.1622 - mse: 0.1622 - val_loss: 0.1037 - val_mse: 0.1037\n",
            "Epoch 284/300\n",
            "105/105 [==============================] - 0s 477us/sample - loss: 0.1633 - mse: 0.1633 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 285/300\n",
            "105/105 [==============================] - 0s 563us/sample - loss: 0.1573 - mse: 0.1573 - val_loss: 0.0927 - val_mse: 0.0927\n",
            "Epoch 286/300\n",
            "105/105 [==============================] - 0s 472us/sample - loss: 0.1603 - mse: 0.1603 - val_loss: 0.0813 - val_mse: 0.0813\n",
            "Epoch 287/300\n",
            "105/105 [==============================] - 0s 429us/sample - loss: 0.1567 - mse: 0.1567 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "Epoch 288/300\n",
            "105/105 [==============================] - 0s 484us/sample - loss: 0.1589 - mse: 0.1589 - val_loss: 0.0807 - val_mse: 0.0807\n",
            "Epoch 289/300\n",
            "105/105 [==============================] - 0s 472us/sample - loss: 0.1546 - mse: 0.1546 - val_loss: 0.0867 - val_mse: 0.0867\n",
            "Epoch 290/300\n",
            "105/105 [==============================] - 0s 451us/sample - loss: 0.1572 - mse: 0.1572 - val_loss: 0.0915 - val_mse: 0.0915\n",
            "Epoch 291/300\n",
            "105/105 [==============================] - 0s 466us/sample - loss: 0.1599 - mse: 0.1599 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 292/300\n",
            "105/105 [==============================] - 0s 505us/sample - loss: 0.1551 - mse: 0.1551 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 293/300\n",
            "105/105 [==============================] - 0s 487us/sample - loss: 0.1593 - mse: 0.1593 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 294/300\n",
            "105/105 [==============================] - 0s 449us/sample - loss: 0.1554 - mse: 0.1554 - val_loss: 0.0798 - val_mse: 0.0798\n",
            "Epoch 295/300\n",
            "105/105 [==============================] - 0s 477us/sample - loss: 0.1615 - mse: 0.1615 - val_loss: 0.0804 - val_mse: 0.0804\n",
            "Epoch 296/300\n",
            "105/105 [==============================] - 0s 467us/sample - loss: 0.1591 - mse: 0.1591 - val_loss: 0.0806 - val_mse: 0.0806\n",
            "Epoch 297/300\n",
            "105/105 [==============================] - 0s 573us/sample - loss: 0.1611 - mse: 0.1611 - val_loss: 0.0808 - val_mse: 0.0808\n",
            "Epoch 298/300\n",
            "105/105 [==============================] - 0s 458us/sample - loss: 0.1588 - mse: 0.1588 - val_loss: 0.0812 - val_mse: 0.0812\n",
            "Epoch 299/300\n",
            "105/105 [==============================] - 0s 513us/sample - loss: 0.1560 - mse: 0.1560 - val_loss: 0.0791 - val_mse: 0.0791\n",
            "Epoch 300/300\n",
            "105/105 [==============================] - 0s 437us/sample - loss: 0.1554 - mse: 0.1554 - val_loss: 0.0848 - val_mse: 0.0848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "132nTFbp-9bX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "6d755726-30b7-49c6-852e-32cafe52a6d7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#faça o gráfico da predição no dataset de treinamento\n",
        "plt.plot(hist.history['mse'])\n",
        "plt.plot(hist.history['val_mse'])\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2oAduFRcuXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Faça split no fit (validation_split) e compare o gráfico de mse treino-teste"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U8IJ36Xc7CT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#faça split com sklearn split train test e use model.evaluate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rHOgz2sdC7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d9540c9-bfd0-4a65-d0c1-3beaf51084f8"
      },
      "source": [
        "#Tente melhorar o modelo"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4a37c4c978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}