{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testdrive.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douglasbarbosadelima/Data-Science/blob/master/testdrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3MF2vt43xW",
        "colab_type": "text"
      },
      "source": [
        "Test Drive com tensorflow 2.0+keras em classificação binária (Iris dataset..setosa ou não)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvUsUmzja9vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd3mi_XUbADj",
        "colab_type": "code",
        "outputId": "399db210-3843-43a7-9960-e8e705633314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhDfTUgcuj0_",
        "colab_type": "text"
      },
      "source": [
        "Vamos instalar tensorflow 2.0...se necessário"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKZzATyUfsG5",
        "colab_type": "code",
        "outputId": "974365e2-558c-4da2-d48b-7cfda66c77c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 63kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.5)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 57.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/6d/7aae38a9022f982cf8167775c7fc299f203417b698c27080ce09060bba07/google_auth-1.11.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (45.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed google-auth-1.11.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QwGidtdiS_Q",
        "colab_type": "code",
        "outputId": "0765bc7b-5c5e-4274-9e1f-e5814366301a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "iris = load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "X[:10],y[:10]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGakF9bywwn6",
        "colab_type": "text"
      },
      "source": [
        "Vamos fazer classificação binária (classe 0 (Setosa) ou não).\n",
        "Para isso, temos que alterar o target (0, vira 1, o resto vira 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOR9tLB-ibep",
        "colab_type": "code",
        "outputId": "1f1fa219-406a-4895-ac02-dfe1544c1e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "for i in range(len(y)) :\n",
        "  if y[i]==0:y[i]=1\n",
        "  else: y[i]=0\n",
        "y"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUniF8aTxasu",
        "colab_type": "text"
      },
      "source": [
        "O Target para Tensorflow/Keras deve ser um vetor de n linhas e 1 coluna. Vamos alterar o shape.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsr_P1vAlLFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c41038e9-8cf5-42c1-91b9-634d69bee55c"
      },
      "source": [
        "y=y.reshape((-1,1))\n",
        "print(y[:10])\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvQ8xbgij4gB",
        "colab_type": "code",
        "outputId": "6fba30c1-258a-4044-85eb-74a1b6723608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(1,\n",
        "                                activation=tf.nn.sigmoid,\n",
        "                                input_shape=(X.shape[1], )))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "hist = model.fit(X_train,\n",
        "                 y_train,\n",
        "                 validation_data=(X_test,y_test),\n",
        "                 epochs=300,\n",
        "                 verbose=1)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 45 samples\n",
            "Epoch 1/300\n",
            "105/105 [==============================] - 0s 4ms/sample - loss: 3.2839 - accuracy: 0.2952 - val_loss: 2.7180 - val_accuracy: 0.4222\n",
            "Epoch 2/300\n",
            "105/105 [==============================] - 0s 228us/sample - loss: 3.2380 - accuracy: 0.2952 - val_loss: 2.6805 - val_accuracy: 0.4222\n",
            "Epoch 3/300\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 3.1917 - accuracy: 0.2952 - val_loss: 2.6422 - val_accuracy: 0.4222\n",
            "Epoch 4/300\n",
            "105/105 [==============================] - 0s 234us/sample - loss: 3.1466 - accuracy: 0.2952 - val_loss: 2.6033 - val_accuracy: 0.4222\n",
            "Epoch 5/300\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 3.1003 - accuracy: 0.2952 - val_loss: 2.5646 - val_accuracy: 0.4222\n",
            "Epoch 6/300\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 3.0553 - accuracy: 0.2952 - val_loss: 2.5259 - val_accuracy: 0.4222\n",
            "Epoch 7/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 3.0096 - accuracy: 0.2952 - val_loss: 2.4873 - val_accuracy: 0.4222\n",
            "Epoch 8/300\n",
            "105/105 [==============================] - 0s 207us/sample - loss: 2.9638 - accuracy: 0.2952 - val_loss: 2.4489 - val_accuracy: 0.4222\n",
            "Epoch 9/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 2.9175 - accuracy: 0.2952 - val_loss: 2.4105 - val_accuracy: 0.4222\n",
            "Epoch 10/300\n",
            "105/105 [==============================] - 0s 200us/sample - loss: 2.8720 - accuracy: 0.2952 - val_loss: 2.3723 - val_accuracy: 0.4222\n",
            "Epoch 11/300\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 2.8268 - accuracy: 0.2952 - val_loss: 2.3342 - val_accuracy: 0.4222\n",
            "Epoch 12/300\n",
            "105/105 [==============================] - 0s 224us/sample - loss: 2.7822 - accuracy: 0.2952 - val_loss: 2.2961 - val_accuracy: 0.4222\n",
            "Epoch 13/300\n",
            "105/105 [==============================] - 0s 208us/sample - loss: 2.7361 - accuracy: 0.2952 - val_loss: 2.2587 - val_accuracy: 0.4222\n",
            "Epoch 14/300\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 2.6915 - accuracy: 0.2952 - val_loss: 2.2212 - val_accuracy: 0.4222\n",
            "Epoch 15/300\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 2.6465 - accuracy: 0.2952 - val_loss: 2.1837 - val_accuracy: 0.4222\n",
            "Epoch 16/300\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 2.6021 - accuracy: 0.2952 - val_loss: 2.1464 - val_accuracy: 0.4222\n",
            "Epoch 17/300\n",
            "105/105 [==============================] - 0s 206us/sample - loss: 2.5592 - accuracy: 0.2952 - val_loss: 2.1091 - val_accuracy: 0.4222\n",
            "Epoch 18/300\n",
            "105/105 [==============================] - 0s 212us/sample - loss: 2.5148 - accuracy: 0.2952 - val_loss: 2.0721 - val_accuracy: 0.4222\n",
            "Epoch 19/300\n",
            "105/105 [==============================] - 0s 228us/sample - loss: 2.4702 - accuracy: 0.2952 - val_loss: 2.0355 - val_accuracy: 0.4222\n",
            "Epoch 20/300\n",
            "105/105 [==============================] - 0s 213us/sample - loss: 2.4276 - accuracy: 0.2952 - val_loss: 1.9986 - val_accuracy: 0.4222\n",
            "Epoch 21/300\n",
            "105/105 [==============================] - 0s 228us/sample - loss: 2.3822 - accuracy: 0.2952 - val_loss: 1.9620 - val_accuracy: 0.4222\n",
            "Epoch 22/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 2.3391 - accuracy: 0.2952 - val_loss: 1.9249 - val_accuracy: 0.4222\n",
            "Epoch 23/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 2.2957 - accuracy: 0.2952 - val_loss: 1.8884 - val_accuracy: 0.4222\n",
            "Epoch 24/300\n",
            "105/105 [==============================] - 0s 238us/sample - loss: 2.2508 - accuracy: 0.2952 - val_loss: 1.8521 - val_accuracy: 0.4222\n",
            "Epoch 25/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 2.2088 - accuracy: 0.2952 - val_loss: 1.8157 - val_accuracy: 0.4222\n",
            "Epoch 26/300\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 2.1665 - accuracy: 0.2952 - val_loss: 1.7797 - val_accuracy: 0.4222\n",
            "Epoch 27/300\n",
            "105/105 [==============================] - 0s 217us/sample - loss: 2.1241 - accuracy: 0.2952 - val_loss: 1.7445 - val_accuracy: 0.4222\n",
            "Epoch 28/300\n",
            "105/105 [==============================] - 0s 218us/sample - loss: 2.0808 - accuracy: 0.2952 - val_loss: 1.7097 - val_accuracy: 0.4222\n",
            "Epoch 29/300\n",
            "105/105 [==============================] - 0s 215us/sample - loss: 2.0417 - accuracy: 0.2952 - val_loss: 1.6744 - val_accuracy: 0.4222\n",
            "Epoch 30/300\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 1.9986 - accuracy: 0.2952 - val_loss: 1.6401 - val_accuracy: 0.4222\n",
            "Epoch 31/300\n",
            "105/105 [==============================] - 0s 205us/sample - loss: 1.9575 - accuracy: 0.2952 - val_loss: 1.6057 - val_accuracy: 0.4222\n",
            "Epoch 32/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 1.9166 - accuracy: 0.2952 - val_loss: 1.5713 - val_accuracy: 0.4222\n",
            "Epoch 33/300\n",
            "105/105 [==============================] - 0s 224us/sample - loss: 1.8761 - accuracy: 0.2952 - val_loss: 1.5374 - val_accuracy: 0.4222\n",
            "Epoch 34/300\n",
            "105/105 [==============================] - 0s 217us/sample - loss: 1.8349 - accuracy: 0.2952 - val_loss: 1.5036 - val_accuracy: 0.4222\n",
            "Epoch 35/300\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 1.7956 - accuracy: 0.2952 - val_loss: 1.4698 - val_accuracy: 0.4222\n",
            "Epoch 36/300\n",
            "105/105 [==============================] - 0s 217us/sample - loss: 1.7544 - accuracy: 0.2952 - val_loss: 1.4368 - val_accuracy: 0.4222\n",
            "Epoch 37/300\n",
            "105/105 [==============================] - 0s 201us/sample - loss: 1.7153 - accuracy: 0.2952 - val_loss: 1.4035 - val_accuracy: 0.4222\n",
            "Epoch 38/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 1.6771 - accuracy: 0.2952 - val_loss: 1.3706 - val_accuracy: 0.4222\n",
            "Epoch 39/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 1.6375 - accuracy: 0.2952 - val_loss: 1.3385 - val_accuracy: 0.4222\n",
            "Epoch 40/300\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 1.5989 - accuracy: 0.2952 - val_loss: 1.3063 - val_accuracy: 0.4222\n",
            "Epoch 41/300\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 1.5609 - accuracy: 0.2952 - val_loss: 1.2746 - val_accuracy: 0.4222\n",
            "Epoch 42/300\n",
            "105/105 [==============================] - 0s 318us/sample - loss: 1.5229 - accuracy: 0.2952 - val_loss: 1.2436 - val_accuracy: 0.4222\n",
            "Epoch 43/300\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 1.4869 - accuracy: 0.2952 - val_loss: 1.2127 - val_accuracy: 0.4222\n",
            "Epoch 44/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 1.4500 - accuracy: 0.2952 - val_loss: 1.1824 - val_accuracy: 0.4222\n",
            "Epoch 45/300\n",
            "105/105 [==============================] - 0s 229us/sample - loss: 1.4136 - accuracy: 0.2952 - val_loss: 1.1532 - val_accuracy: 0.4222\n",
            "Epoch 46/300\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 1.3779 - accuracy: 0.2952 - val_loss: 1.1247 - val_accuracy: 0.4222\n",
            "Epoch 47/300\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 1.3444 - accuracy: 0.2952 - val_loss: 1.0963 - val_accuracy: 0.4222\n",
            "Epoch 48/300\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 1.3103 - accuracy: 0.2952 - val_loss: 1.0684 - val_accuracy: 0.4222\n",
            "Epoch 49/300\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 1.2778 - accuracy: 0.2952 - val_loss: 1.0406 - val_accuracy: 0.4222\n",
            "Epoch 50/300\n",
            "105/105 [==============================] - 0s 362us/sample - loss: 1.2449 - accuracy: 0.2952 - val_loss: 1.0137 - val_accuracy: 0.4222\n",
            "Epoch 51/300\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 1.2121 - accuracy: 0.2952 - val_loss: 0.9876 - val_accuracy: 0.4222\n",
            "Epoch 52/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 1.1809 - accuracy: 0.2952 - val_loss: 0.9618 - val_accuracy: 0.4222\n",
            "Epoch 53/300\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 1.1490 - accuracy: 0.2952 - val_loss: 0.9366 - val_accuracy: 0.4222\n",
            "Epoch 54/300\n",
            "105/105 [==============================] - 0s 229us/sample - loss: 1.1196 - accuracy: 0.2952 - val_loss: 0.9115 - val_accuracy: 0.4222\n",
            "Epoch 55/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 1.0888 - accuracy: 0.2952 - val_loss: 0.8873 - val_accuracy: 0.4222\n",
            "Epoch 56/300\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 1.0607 - accuracy: 0.2952 - val_loss: 0.8632 - val_accuracy: 0.4222\n",
            "Epoch 57/300\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 1.0316 - accuracy: 0.2952 - val_loss: 0.8401 - val_accuracy: 0.4222\n",
            "Epoch 58/300\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 1.0040 - accuracy: 0.3048 - val_loss: 0.8178 - val_accuracy: 0.4222\n",
            "Epoch 59/300\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.9770 - accuracy: 0.3048 - val_loss: 0.7961 - val_accuracy: 0.4222\n",
            "Epoch 60/300\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.9511 - accuracy: 0.3048 - val_loss: 0.7751 - val_accuracy: 0.4222\n",
            "Epoch 61/300\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.9252 - accuracy: 0.3048 - val_loss: 0.7548 - val_accuracy: 0.4222\n",
            "Epoch 62/300\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.9010 - accuracy: 0.3143 - val_loss: 0.7349 - val_accuracy: 0.4222\n",
            "Epoch 63/300\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.8769 - accuracy: 0.3143 - val_loss: 0.7159 - val_accuracy: 0.4222\n",
            "Epoch 64/300\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.8538 - accuracy: 0.3333 - val_loss: 0.6977 - val_accuracy: 0.4222\n",
            "Epoch 65/300\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.8323 - accuracy: 0.3429 - val_loss: 0.6800 - val_accuracy: 0.4222\n",
            "Epoch 66/300\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.8102 - accuracy: 0.3714 - val_loss: 0.6629 - val_accuracy: 0.4222\n",
            "Epoch 67/300\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 0.7896 - accuracy: 0.3905 - val_loss: 0.6464 - val_accuracy: 0.4667\n",
            "Epoch 68/300\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.7694 - accuracy: 0.3905 - val_loss: 0.6307 - val_accuracy: 0.4889\n",
            "Epoch 69/300\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.7505 - accuracy: 0.4000 - val_loss: 0.6155 - val_accuracy: 0.5778\n",
            "Epoch 70/300\n",
            "105/105 [==============================] - 0s 196us/sample - loss: 0.7313 - accuracy: 0.4190 - val_loss: 0.6008 - val_accuracy: 0.5778\n",
            "Epoch 71/300\n",
            "105/105 [==============================] - 0s 220us/sample - loss: 0.7135 - accuracy: 0.4190 - val_loss: 0.5864 - val_accuracy: 0.5778\n",
            "Epoch 72/300\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.6959 - accuracy: 0.4190 - val_loss: 0.5727 - val_accuracy: 0.6000\n",
            "Epoch 73/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.6793 - accuracy: 0.4286 - val_loss: 0.5595 - val_accuracy: 0.6222\n",
            "Epoch 74/300\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.6632 - accuracy: 0.4381 - val_loss: 0.5472 - val_accuracy: 0.6222\n",
            "Epoch 75/300\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.6471 - accuracy: 0.4857 - val_loss: 0.5355 - val_accuracy: 0.6444\n",
            "Epoch 76/300\n",
            "105/105 [==============================] - 0s 208us/sample - loss: 0.6324 - accuracy: 0.5143 - val_loss: 0.5239 - val_accuracy: 0.6667\n",
            "Epoch 77/300\n",
            "105/105 [==============================] - 0s 228us/sample - loss: 0.6178 - accuracy: 0.5429 - val_loss: 0.5128 - val_accuracy: 0.6667\n",
            "Epoch 78/300\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.6042 - accuracy: 0.5429 - val_loss: 0.5020 - val_accuracy: 0.6889\n",
            "Epoch 79/300\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.5906 - accuracy: 0.5524 - val_loss: 0.4915 - val_accuracy: 0.6889\n",
            "Epoch 80/300\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.5770 - accuracy: 0.5810 - val_loss: 0.4815 - val_accuracy: 0.6889\n",
            "Epoch 81/300\n",
            "105/105 [==============================] - 0s 316us/sample - loss: 0.5642 - accuracy: 0.6095 - val_loss: 0.4719 - val_accuracy: 0.6889\n",
            "Epoch 82/300\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.5528 - accuracy: 0.6476 - val_loss: 0.4626 - val_accuracy: 0.6889\n",
            "Epoch 83/300\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.5407 - accuracy: 0.6476 - val_loss: 0.4537 - val_accuracy: 0.6889\n",
            "Epoch 84/300\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.5290 - accuracy: 0.6571 - val_loss: 0.4455 - val_accuracy: 0.7111\n",
            "Epoch 85/300\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.5189 - accuracy: 0.6762 - val_loss: 0.4375 - val_accuracy: 0.7333\n",
            "Epoch 86/300\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.5080 - accuracy: 0.6857 - val_loss: 0.4302 - val_accuracy: 0.8000\n",
            "Epoch 87/300\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.4989 - accuracy: 0.7143 - val_loss: 0.4230 - val_accuracy: 0.8000\n",
            "Epoch 88/300\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 0.4895 - accuracy: 0.7333 - val_loss: 0.4162 - val_accuracy: 0.8000\n",
            "Epoch 89/300\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.4804 - accuracy: 0.7333 - val_loss: 0.4096 - val_accuracy: 0.8000\n",
            "Epoch 90/300\n",
            "105/105 [==============================] - 0s 333us/sample - loss: 0.4717 - accuracy: 0.7429 - val_loss: 0.4033 - val_accuracy: 0.8222\n",
            "Epoch 91/300\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.4637 - accuracy: 0.7619 - val_loss: 0.3971 - val_accuracy: 0.8444\n",
            "Epoch 92/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.4553 - accuracy: 0.8095 - val_loss: 0.3914 - val_accuracy: 0.8667\n",
            "Epoch 93/300\n",
            "105/105 [==============================] - 0s 238us/sample - loss: 0.4478 - accuracy: 0.8190 - val_loss: 0.3858 - val_accuracy: 0.8889\n",
            "Epoch 94/300\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.4402 - accuracy: 0.8190 - val_loss: 0.3806 - val_accuracy: 0.8889\n",
            "Epoch 95/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 0.4332 - accuracy: 0.8381 - val_loss: 0.3756 - val_accuracy: 0.8889\n",
            "Epoch 96/300\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.4266 - accuracy: 0.8571 - val_loss: 0.3707 - val_accuracy: 0.8889\n",
            "Epoch 97/300\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.4196 - accuracy: 0.8952 - val_loss: 0.3661 - val_accuracy: 0.9333\n",
            "Epoch 98/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 0.4135 - accuracy: 0.9333 - val_loss: 0.3618 - val_accuracy: 0.9333\n",
            "Epoch 99/300\n",
            "105/105 [==============================] - 0s 214us/sample - loss: 0.4075 - accuracy: 0.9333 - val_loss: 0.3577 - val_accuracy: 1.0000\n",
            "Epoch 100/300\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 0.4017 - accuracy: 0.9429 - val_loss: 0.3538 - val_accuracy: 1.0000\n",
            "Epoch 101/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.3961 - accuracy: 0.9524 - val_loss: 0.3502 - val_accuracy: 1.0000\n",
            "Epoch 102/300\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.3913 - accuracy: 0.9524 - val_loss: 0.3467 - val_accuracy: 1.0000\n",
            "Epoch 103/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.3864 - accuracy: 0.9619 - val_loss: 0.3434 - val_accuracy: 1.0000\n",
            "Epoch 104/300\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.3813 - accuracy: 0.9714 - val_loss: 0.3402 - val_accuracy: 1.0000\n",
            "Epoch 105/300\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 0.3768 - accuracy: 0.9714 - val_loss: 0.3371 - val_accuracy: 1.0000\n",
            "Epoch 106/300\n",
            "105/105 [==============================] - 0s 314us/sample - loss: 0.3723 - accuracy: 0.9714 - val_loss: 0.3340 - val_accuracy: 1.0000\n",
            "Epoch 107/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.3678 - accuracy: 0.9714 - val_loss: 0.3311 - val_accuracy: 1.0000\n",
            "Epoch 108/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.3635 - accuracy: 0.9714 - val_loss: 0.3284 - val_accuracy: 1.0000\n",
            "Epoch 109/300\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.3596 - accuracy: 0.9905 - val_loss: 0.3257 - val_accuracy: 1.0000\n",
            "Epoch 110/300\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.3557 - accuracy: 1.0000 - val_loss: 0.3233 - val_accuracy: 1.0000\n",
            "Epoch 111/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.3520 - accuracy: 1.0000 - val_loss: 0.3210 - val_accuracy: 1.0000\n",
            "Epoch 112/300\n",
            "105/105 [==============================] - 0s 234us/sample - loss: 0.3484 - accuracy: 1.0000 - val_loss: 0.3189 - val_accuracy: 1.0000\n",
            "Epoch 113/300\n",
            "105/105 [==============================] - 0s 225us/sample - loss: 0.3454 - accuracy: 1.0000 - val_loss: 0.3168 - val_accuracy: 1.0000\n",
            "Epoch 114/300\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 0.3422 - accuracy: 1.0000 - val_loss: 0.3148 - val_accuracy: 1.0000\n",
            "Epoch 115/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.3391 - accuracy: 1.0000 - val_loss: 0.3129 - val_accuracy: 1.0000\n",
            "Epoch 116/300\n",
            "105/105 [==============================] - 0s 229us/sample - loss: 0.3362 - accuracy: 1.0000 - val_loss: 0.3111 - val_accuracy: 1.0000\n",
            "Epoch 117/300\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.3332 - accuracy: 1.0000 - val_loss: 0.3094 - val_accuracy: 1.0000\n",
            "Epoch 118/300\n",
            "105/105 [==============================] - 0s 207us/sample - loss: 0.3304 - accuracy: 1.0000 - val_loss: 0.3078 - val_accuracy: 1.0000\n",
            "Epoch 119/300\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 0.3276 - accuracy: 1.0000 - val_loss: 0.3062 - val_accuracy: 1.0000\n",
            "Epoch 120/300\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.3253 - accuracy: 1.0000 - val_loss: 0.3046 - val_accuracy: 1.0000\n",
            "Epoch 121/300\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.3224 - accuracy: 1.0000 - val_loss: 0.3032 - val_accuracy: 1.0000\n",
            "Epoch 122/300\n",
            "105/105 [==============================] - 0s 345us/sample - loss: 0.3200 - accuracy: 1.0000 - val_loss: 0.3018 - val_accuracy: 1.0000\n",
            "Epoch 123/300\n",
            "105/105 [==============================] - 0s 299us/sample - loss: 0.3177 - accuracy: 1.0000 - val_loss: 0.3005 - val_accuracy: 1.0000\n",
            "Epoch 124/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.3155 - accuracy: 1.0000 - val_loss: 0.2992 - val_accuracy: 1.0000\n",
            "Epoch 125/300\n",
            "105/105 [==============================] - 0s 234us/sample - loss: 0.3132 - accuracy: 1.0000 - val_loss: 0.2979 - val_accuracy: 1.0000\n",
            "Epoch 126/300\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.3110 - accuracy: 1.0000 - val_loss: 0.2967 - val_accuracy: 1.0000\n",
            "Epoch 127/300\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.3089 - accuracy: 1.0000 - val_loss: 0.2955 - val_accuracy: 1.0000\n",
            "Epoch 128/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.3069 - accuracy: 1.0000 - val_loss: 0.2945 - val_accuracy: 1.0000\n",
            "Epoch 129/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.3050 - accuracy: 1.0000 - val_loss: 0.2934 - val_accuracy: 1.0000\n",
            "Epoch 130/300\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.3031 - accuracy: 1.0000 - val_loss: 0.2925 - val_accuracy: 1.0000\n",
            "Epoch 131/300\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.3015 - accuracy: 1.0000 - val_loss: 0.2916 - val_accuracy: 1.0000\n",
            "Epoch 132/300\n",
            "105/105 [==============================] - 0s 292us/sample - loss: 0.2997 - accuracy: 1.0000 - val_loss: 0.2908 - val_accuracy: 1.0000\n",
            "Epoch 133/300\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.2982 - accuracy: 1.0000 - val_loss: 0.2899 - val_accuracy: 1.0000\n",
            "Epoch 134/300\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.2965 - accuracy: 1.0000 - val_loss: 0.2891 - val_accuracy: 1.0000\n",
            "Epoch 135/300\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.2948 - accuracy: 1.0000 - val_loss: 0.2883 - val_accuracy: 1.0000\n",
            "Epoch 136/300\n",
            "105/105 [==============================] - 0s 349us/sample - loss: 0.2933 - accuracy: 1.0000 - val_loss: 0.2875 - val_accuracy: 1.0000\n",
            "Epoch 137/300\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.2917 - accuracy: 1.0000 - val_loss: 0.2868 - val_accuracy: 1.0000\n",
            "Epoch 138/300\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.2901 - accuracy: 1.0000 - val_loss: 0.2861 - val_accuracy: 1.0000\n",
            "Epoch 139/300\n",
            "105/105 [==============================] - 0s 225us/sample - loss: 0.2887 - accuracy: 1.0000 - val_loss: 0.2854 - val_accuracy: 1.0000\n",
            "Epoch 140/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.2872 - accuracy: 1.0000 - val_loss: 0.2848 - val_accuracy: 1.0000\n",
            "Epoch 141/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.2859 - accuracy: 1.0000 - val_loss: 0.2842 - val_accuracy: 1.0000\n",
            "Epoch 142/300\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 0.2847 - accuracy: 1.0000 - val_loss: 0.2837 - val_accuracy: 1.0000\n",
            "Epoch 143/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.2835 - accuracy: 1.0000 - val_loss: 0.2831 - val_accuracy: 1.0000\n",
            "Epoch 144/300\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.2824 - accuracy: 1.0000 - val_loss: 0.2827 - val_accuracy: 1.0000\n",
            "Epoch 145/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.2814 - accuracy: 1.0000 - val_loss: 0.2822 - val_accuracy: 1.0000\n",
            "Epoch 146/300\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.2803 - accuracy: 1.0000 - val_loss: 0.2817 - val_accuracy: 1.0000\n",
            "Epoch 147/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.2794 - accuracy: 1.0000 - val_loss: 0.2813 - val_accuracy: 1.0000\n",
            "Epoch 148/300\n",
            "105/105 [==============================] - 0s 229us/sample - loss: 0.2783 - accuracy: 1.0000 - val_loss: 0.2808 - val_accuracy: 1.0000\n",
            "Epoch 149/300\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.2774 - accuracy: 1.0000 - val_loss: 0.2804 - val_accuracy: 1.0000\n",
            "Epoch 150/300\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 0.2764 - accuracy: 1.0000 - val_loss: 0.2800 - val_accuracy: 1.0000\n",
            "Epoch 151/300\n",
            "105/105 [==============================] - 0s 228us/sample - loss: 0.2754 - accuracy: 1.0000 - val_loss: 0.2796 - val_accuracy: 1.0000\n",
            "Epoch 152/300\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.2745 - accuracy: 1.0000 - val_loss: 0.2793 - val_accuracy: 1.0000\n",
            "Epoch 153/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 0.2737 - accuracy: 1.0000 - val_loss: 0.2789 - val_accuracy: 1.0000\n",
            "Epoch 154/300\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.2728 - accuracy: 1.0000 - val_loss: 0.2786 - val_accuracy: 1.0000\n",
            "Epoch 155/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.2719 - accuracy: 1.0000 - val_loss: 0.2782 - val_accuracy: 1.0000\n",
            "Epoch 156/300\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.2711 - accuracy: 1.0000 - val_loss: 0.2779 - val_accuracy: 1.0000\n",
            "Epoch 157/300\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.2702 - accuracy: 1.0000 - val_loss: 0.2776 - val_accuracy: 1.0000\n",
            "Epoch 158/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.2695 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 1.0000\n",
            "Epoch 159/300\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.2688 - accuracy: 1.0000 - val_loss: 0.2770 - val_accuracy: 1.0000\n",
            "Epoch 160/300\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.2680 - accuracy: 1.0000 - val_loss: 0.2768 - val_accuracy: 1.0000\n",
            "Epoch 161/300\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.2673 - accuracy: 1.0000 - val_loss: 0.2765 - val_accuracy: 1.0000\n",
            "Epoch 162/300\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.2665 - accuracy: 1.0000 - val_loss: 0.2762 - val_accuracy: 1.0000\n",
            "Epoch 163/300\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.2659 - accuracy: 1.0000 - val_loss: 0.2760 - val_accuracy: 1.0000\n",
            "Epoch 164/300\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.2652 - accuracy: 1.0000 - val_loss: 0.2758 - val_accuracy: 1.0000\n",
            "Epoch 165/300\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.2645 - accuracy: 1.0000 - val_loss: 0.2755 - val_accuracy: 1.0000\n",
            "Epoch 166/300\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.2639 - accuracy: 1.0000 - val_loss: 0.2753 - val_accuracy: 1.0000\n",
            "Epoch 167/300\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.2633 - accuracy: 1.0000 - val_loss: 0.2750 - val_accuracy: 1.0000\n",
            "Epoch 168/300\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.2628 - accuracy: 1.0000 - val_loss: 0.2747 - val_accuracy: 1.0000\n",
            "Epoch 169/300\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.2623 - accuracy: 1.0000 - val_loss: 0.2745 - val_accuracy: 1.0000\n",
            "Epoch 170/300\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.2618 - accuracy: 1.0000 - val_loss: 0.2742 - val_accuracy: 1.0000\n",
            "Epoch 171/300\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.2612 - accuracy: 1.0000 - val_loss: 0.2740 - val_accuracy: 1.0000\n",
            "Epoch 172/300\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.2607 - accuracy: 1.0000 - val_loss: 0.2738 - val_accuracy: 1.0000\n",
            "Epoch 173/300\n",
            "105/105 [==============================] - 0s 226us/sample - loss: 0.2601 - accuracy: 1.0000 - val_loss: 0.2735 - val_accuracy: 1.0000\n",
            "Epoch 174/300\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.2597 - accuracy: 1.0000 - val_loss: 0.2733 - val_accuracy: 1.0000\n",
            "Epoch 175/300\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.2591 - accuracy: 1.0000 - val_loss: 0.2731 - val_accuracy: 1.0000\n",
            "Epoch 176/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.2586 - accuracy: 1.0000 - val_loss: 0.2728 - val_accuracy: 1.0000\n",
            "Epoch 177/300\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.2581 - accuracy: 1.0000 - val_loss: 0.2726 - val_accuracy: 1.0000\n",
            "Epoch 178/300\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.2576 - accuracy: 1.0000 - val_loss: 0.2724 - val_accuracy: 1.0000\n",
            "Epoch 179/300\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.2571 - accuracy: 1.0000 - val_loss: 0.2723 - val_accuracy: 1.0000\n",
            "Epoch 180/300\n",
            "105/105 [==============================] - 0s 234us/sample - loss: 0.2566 - accuracy: 1.0000 - val_loss: 0.2721 - val_accuracy: 1.0000\n",
            "Epoch 181/300\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.2561 - accuracy: 1.0000 - val_loss: 0.2718 - val_accuracy: 1.0000\n",
            "Epoch 182/300\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.2557 - accuracy: 1.0000 - val_loss: 0.2716 - val_accuracy: 1.0000\n",
            "Epoch 183/300\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.2552 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 1.0000\n",
            "Epoch 184/300\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 0.2548 - accuracy: 1.0000 - val_loss: 0.2712 - val_accuracy: 1.0000\n",
            "Epoch 185/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.2543 - accuracy: 1.0000 - val_loss: 0.2710 - val_accuracy: 1.0000\n",
            "Epoch 186/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 0.2539 - accuracy: 1.0000 - val_loss: 0.2709 - val_accuracy: 1.0000\n",
            "Epoch 187/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 0.2534 - accuracy: 1.0000 - val_loss: 0.2707 - val_accuracy: 1.0000\n",
            "Epoch 188/300\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.2530 - accuracy: 1.0000 - val_loss: 0.2705 - val_accuracy: 1.0000\n",
            "Epoch 189/300\n",
            "105/105 [==============================] - 0s 211us/sample - loss: 0.2525 - accuracy: 1.0000 - val_loss: 0.2703 - val_accuracy: 1.0000\n",
            "Epoch 190/300\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.2521 - accuracy: 1.0000 - val_loss: 0.2702 - val_accuracy: 1.0000\n",
            "Epoch 191/300\n",
            "105/105 [==============================] - 0s 234us/sample - loss: 0.2517 - accuracy: 1.0000 - val_loss: 0.2700 - val_accuracy: 1.0000\n",
            "Epoch 192/300\n",
            "105/105 [==============================] - 0s 206us/sample - loss: 0.2513 - accuracy: 1.0000 - val_loss: 0.2697 - val_accuracy: 1.0000\n",
            "Epoch 193/300\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.2509 - accuracy: 1.0000 - val_loss: 0.2695 - val_accuracy: 1.0000\n",
            "Epoch 194/300\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 0.2505 - accuracy: 1.0000 - val_loss: 0.2693 - val_accuracy: 1.0000\n",
            "Epoch 195/300\n",
            "105/105 [==============================] - 0s 205us/sample - loss: 0.2501 - accuracy: 1.0000 - val_loss: 0.2691 - val_accuracy: 1.0000\n",
            "Epoch 196/300\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.2497 - accuracy: 1.0000 - val_loss: 0.2689 - val_accuracy: 1.0000\n",
            "Epoch 197/300\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.2494 - accuracy: 1.0000 - val_loss: 0.2688 - val_accuracy: 1.0000\n",
            "Epoch 198/300\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.2489 - accuracy: 1.0000 - val_loss: 0.2686 - val_accuracy: 1.0000\n",
            "Epoch 199/300\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 0.2486 - accuracy: 1.0000 - val_loss: 0.2684 - val_accuracy: 1.0000\n",
            "Epoch 200/300\n",
            "105/105 [==============================] - 0s 209us/sample - loss: 0.2482 - accuracy: 1.0000 - val_loss: 0.2682 - val_accuracy: 1.0000\n",
            "Epoch 201/300\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.2479 - accuracy: 1.0000 - val_loss: 0.2680 - val_accuracy: 1.0000\n",
            "Epoch 202/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.2475 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 1.0000\n",
            "Epoch 203/300\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.2471 - accuracy: 1.0000 - val_loss: 0.2675 - val_accuracy: 1.0000\n",
            "Epoch 204/300\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 0.2468 - accuracy: 1.0000 - val_loss: 0.2672 - val_accuracy: 1.0000\n",
            "Epoch 205/300\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 0.2464 - accuracy: 1.0000 - val_loss: 0.2670 - val_accuracy: 1.0000\n",
            "Epoch 206/300\n",
            "105/105 [==============================] - 0s 225us/sample - loss: 0.2461 - accuracy: 1.0000 - val_loss: 0.2667 - val_accuracy: 1.0000\n",
            "Epoch 207/300\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.2457 - accuracy: 1.0000 - val_loss: 0.2666 - val_accuracy: 1.0000\n",
            "Epoch 208/300\n",
            "105/105 [==============================] - 0s 226us/sample - loss: 0.2454 - accuracy: 1.0000 - val_loss: 0.2664 - val_accuracy: 1.0000\n",
            "Epoch 209/300\n",
            "105/105 [==============================] - 0s 218us/sample - loss: 0.2450 - accuracy: 1.0000 - val_loss: 0.2662 - val_accuracy: 1.0000\n",
            "Epoch 210/300\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.2446 - accuracy: 1.0000 - val_loss: 0.2660 - val_accuracy: 1.0000\n",
            "Epoch 211/300\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.2443 - accuracy: 1.0000 - val_loss: 0.2658 - val_accuracy: 1.0000\n",
            "Epoch 212/300\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 0.2439 - accuracy: 1.0000 - val_loss: 0.2655 - val_accuracy: 1.0000\n",
            "Epoch 213/300\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.2436 - accuracy: 1.0000 - val_loss: 0.2653 - val_accuracy: 1.0000\n",
            "Epoch 214/300\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 0.2433 - accuracy: 1.0000 - val_loss: 0.2651 - val_accuracy: 1.0000\n",
            "Epoch 215/300\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.2430 - accuracy: 1.0000 - val_loss: 0.2649 - val_accuracy: 1.0000\n",
            "Epoch 216/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.2426 - accuracy: 1.0000 - val_loss: 0.2647 - val_accuracy: 1.0000\n",
            "Epoch 217/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.2423 - accuracy: 1.0000 - val_loss: 0.2644 - val_accuracy: 1.0000\n",
            "Epoch 218/300\n",
            "105/105 [==============================] - 0s 216us/sample - loss: 0.2419 - accuracy: 1.0000 - val_loss: 0.2641 - val_accuracy: 1.0000\n",
            "Epoch 219/300\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.2416 - accuracy: 1.0000 - val_loss: 0.2638 - val_accuracy: 1.0000\n",
            "Epoch 220/300\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 0.2413 - accuracy: 1.0000 - val_loss: 0.2635 - val_accuracy: 1.0000\n",
            "Epoch 221/300\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.2410 - accuracy: 1.0000 - val_loss: 0.2632 - val_accuracy: 1.0000\n",
            "Epoch 222/300\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 0.2406 - accuracy: 1.0000 - val_loss: 0.2629 - val_accuracy: 1.0000\n",
            "Epoch 223/300\n",
            "105/105 [==============================] - 0s 220us/sample - loss: 0.2403 - accuracy: 1.0000 - val_loss: 0.2626 - val_accuracy: 1.0000\n",
            "Epoch 224/300\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 0.2400 - accuracy: 1.0000 - val_loss: 0.2623 - val_accuracy: 1.0000\n",
            "Epoch 225/300\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 0.2397 - accuracy: 1.0000 - val_loss: 0.2620 - val_accuracy: 1.0000\n",
            "Epoch 226/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 0.2393 - accuracy: 1.0000 - val_loss: 0.2617 - val_accuracy: 1.0000\n",
            "Epoch 227/300\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.2390 - accuracy: 1.0000 - val_loss: 0.2615 - val_accuracy: 1.0000\n",
            "Epoch 228/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.2387 - accuracy: 1.0000 - val_loss: 0.2612 - val_accuracy: 1.0000\n",
            "Epoch 229/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.2384 - accuracy: 1.0000 - val_loss: 0.2610 - val_accuracy: 1.0000\n",
            "Epoch 230/300\n",
            "105/105 [==============================] - 0s 231us/sample - loss: 0.2380 - accuracy: 1.0000 - val_loss: 0.2606 - val_accuracy: 1.0000\n",
            "Epoch 231/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.2377 - accuracy: 1.0000 - val_loss: 0.2603 - val_accuracy: 1.0000\n",
            "Epoch 232/300\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.2374 - accuracy: 1.0000 - val_loss: 0.2600 - val_accuracy: 1.0000\n",
            "Epoch 233/300\n",
            "105/105 [==============================] - 0s 238us/sample - loss: 0.2371 - accuracy: 1.0000 - val_loss: 0.2596 - val_accuracy: 1.0000\n",
            "Epoch 234/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.2368 - accuracy: 1.0000 - val_loss: 0.2592 - val_accuracy: 1.0000\n",
            "Epoch 235/300\n",
            "105/105 [==============================] - 0s 215us/sample - loss: 0.2365 - accuracy: 1.0000 - val_loss: 0.2588 - val_accuracy: 1.0000\n",
            "Epoch 236/300\n",
            "105/105 [==============================] - 0s 208us/sample - loss: 0.2361 - accuracy: 1.0000 - val_loss: 0.2584 - val_accuracy: 1.0000\n",
            "Epoch 237/300\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.2359 - accuracy: 1.0000 - val_loss: 0.2579 - val_accuracy: 1.0000\n",
            "Epoch 238/300\n",
            "105/105 [==============================] - 0s 215us/sample - loss: 0.2355 - accuracy: 1.0000 - val_loss: 0.2575 - val_accuracy: 1.0000\n",
            "Epoch 239/300\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.2352 - accuracy: 1.0000 - val_loss: 0.2571 - val_accuracy: 1.0000\n",
            "Epoch 240/300\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.2349 - accuracy: 1.0000 - val_loss: 0.2568 - val_accuracy: 1.0000\n",
            "Epoch 241/300\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 0.2346 - accuracy: 1.0000 - val_loss: 0.2564 - val_accuracy: 1.0000\n",
            "Epoch 242/300\n",
            "105/105 [==============================] - 0s 215us/sample - loss: 0.2342 - accuracy: 1.0000 - val_loss: 0.2561 - val_accuracy: 1.0000\n",
            "Epoch 243/300\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 0.2339 - accuracy: 1.0000 - val_loss: 0.2557 - val_accuracy: 1.0000\n",
            "Epoch 244/300\n",
            "105/105 [==============================] - 0s 216us/sample - loss: 0.2336 - accuracy: 1.0000 - val_loss: 0.2554 - val_accuracy: 1.0000\n",
            "Epoch 245/300\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 0.2333 - accuracy: 1.0000 - val_loss: 0.2550 - val_accuracy: 1.0000\n",
            "Epoch 246/300\n",
            "105/105 [==============================] - 0s 204us/sample - loss: 0.2330 - accuracy: 1.0000 - val_loss: 0.2547 - val_accuracy: 1.0000\n",
            "Epoch 247/300\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.2327 - accuracy: 1.0000 - val_loss: 0.2543 - val_accuracy: 1.0000\n",
            "Epoch 248/300\n",
            "105/105 [==============================] - 0s 225us/sample - loss: 0.2323 - accuracy: 1.0000 - val_loss: 0.2539 - val_accuracy: 1.0000\n",
            "Epoch 249/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.2320 - accuracy: 1.0000 - val_loss: 0.2536 - val_accuracy: 1.0000\n",
            "Epoch 250/300\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.2317 - accuracy: 1.0000 - val_loss: 0.2532 - val_accuracy: 1.0000\n",
            "Epoch 251/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.2314 - accuracy: 1.0000 - val_loss: 0.2529 - val_accuracy: 1.0000\n",
            "Epoch 252/300\n",
            "105/105 [==============================] - 0s 210us/sample - loss: 0.2311 - accuracy: 1.0000 - val_loss: 0.2526 - val_accuracy: 1.0000\n",
            "Epoch 253/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.2307 - accuracy: 1.0000 - val_loss: 0.2524 - val_accuracy: 1.0000\n",
            "Epoch 254/300\n",
            "105/105 [==============================] - 0s 226us/sample - loss: 0.2304 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 1.0000\n",
            "Epoch 255/300\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.2301 - accuracy: 1.0000 - val_loss: 0.2518 - val_accuracy: 1.0000\n",
            "Epoch 256/300\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 0.2298 - accuracy: 1.0000 - val_loss: 0.2515 - val_accuracy: 1.0000\n",
            "Epoch 257/300\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.2295 - accuracy: 1.0000 - val_loss: 0.2513 - val_accuracy: 1.0000\n",
            "Epoch 258/300\n",
            "105/105 [==============================] - 0s 238us/sample - loss: 0.2292 - accuracy: 1.0000 - val_loss: 0.2510 - val_accuracy: 1.0000\n",
            "Epoch 259/300\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.2288 - accuracy: 1.0000 - val_loss: 0.2507 - val_accuracy: 1.0000\n",
            "Epoch 260/300\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.2285 - accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 1.0000\n",
            "Epoch 261/300\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.2282 - accuracy: 1.0000 - val_loss: 0.2499 - val_accuracy: 1.0000\n",
            "Epoch 262/300\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.2279 - accuracy: 1.0000 - val_loss: 0.2495 - val_accuracy: 1.0000\n",
            "Epoch 263/300\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 0.2276 - accuracy: 1.0000 - val_loss: 0.2491 - val_accuracy: 1.0000\n",
            "Epoch 264/300\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.2273 - accuracy: 1.0000 - val_loss: 0.2488 - val_accuracy: 1.0000\n",
            "Epoch 265/300\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 0.2269 - accuracy: 1.0000 - val_loss: 0.2485 - val_accuracy: 1.0000\n",
            "Epoch 266/300\n",
            "105/105 [==============================] - 0s 218us/sample - loss: 0.2266 - accuracy: 1.0000 - val_loss: 0.2482 - val_accuracy: 1.0000\n",
            "Epoch 267/300\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.2263 - accuracy: 1.0000 - val_loss: 0.2477 - val_accuracy: 1.0000\n",
            "Epoch 268/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.2260 - accuracy: 1.0000 - val_loss: 0.2473 - val_accuracy: 1.0000\n",
            "Epoch 269/300\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.2257 - accuracy: 1.0000 - val_loss: 0.2469 - val_accuracy: 1.0000\n",
            "Epoch 270/300\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.2253 - accuracy: 1.0000 - val_loss: 0.2465 - val_accuracy: 1.0000\n",
            "Epoch 271/300\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.2250 - accuracy: 1.0000 - val_loss: 0.2461 - val_accuracy: 1.0000\n",
            "Epoch 272/300\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.2247 - accuracy: 1.0000 - val_loss: 0.2457 - val_accuracy: 1.0000\n",
            "Epoch 273/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 0.2244 - accuracy: 1.0000 - val_loss: 0.2453 - val_accuracy: 1.0000\n",
            "Epoch 274/300\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 0.2241 - accuracy: 1.0000 - val_loss: 0.2449 - val_accuracy: 1.0000\n",
            "Epoch 275/300\n",
            "105/105 [==============================] - 0s 223us/sample - loss: 0.2238 - accuracy: 1.0000 - val_loss: 0.2445 - val_accuracy: 1.0000\n",
            "Epoch 276/300\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.2235 - accuracy: 1.0000 - val_loss: 0.2442 - val_accuracy: 1.0000\n",
            "Epoch 277/300\n",
            "105/105 [==============================] - 0s 225us/sample - loss: 0.2232 - accuracy: 1.0000 - val_loss: 0.2438 - val_accuracy: 1.0000\n",
            "Epoch 278/300\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.2228 - accuracy: 1.0000 - val_loss: 0.2435 - val_accuracy: 1.0000\n",
            "Epoch 279/300\n",
            "105/105 [==============================] - 0s 229us/sample - loss: 0.2225 - accuracy: 1.0000 - val_loss: 0.2432 - val_accuracy: 1.0000\n",
            "Epoch 280/300\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.2222 - accuracy: 1.0000 - val_loss: 0.2430 - val_accuracy: 1.0000\n",
            "Epoch 281/300\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.2219 - accuracy: 1.0000 - val_loss: 0.2427 - val_accuracy: 1.0000\n",
            "Epoch 282/300\n",
            "105/105 [==============================] - 0s 221us/sample - loss: 0.2216 - accuracy: 1.0000 - val_loss: 0.2424 - val_accuracy: 1.0000\n",
            "Epoch 283/300\n",
            "105/105 [==============================] - 0s 220us/sample - loss: 0.2212 - accuracy: 1.0000 - val_loss: 0.2421 - val_accuracy: 1.0000\n",
            "Epoch 284/300\n",
            "105/105 [==============================] - 0s 214us/sample - loss: 0.2209 - accuracy: 1.0000 - val_loss: 0.2418 - val_accuracy: 1.0000\n",
            "Epoch 285/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.2206 - accuracy: 1.0000 - val_loss: 0.2416 - val_accuracy: 1.0000\n",
            "Epoch 286/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 0.2203 - accuracy: 1.0000 - val_loss: 0.2414 - val_accuracy: 1.0000\n",
            "Epoch 287/300\n",
            "105/105 [==============================] - 0s 229us/sample - loss: 0.2200 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 1.0000\n",
            "Epoch 288/300\n",
            "105/105 [==============================] - 0s 208us/sample - loss: 0.2197 - accuracy: 1.0000 - val_loss: 0.2408 - val_accuracy: 1.0000\n",
            "Epoch 289/300\n",
            "105/105 [==============================] - 0s 229us/sample - loss: 0.2193 - accuracy: 1.0000 - val_loss: 0.2405 - val_accuracy: 1.0000\n",
            "Epoch 290/300\n",
            "105/105 [==============================] - 0s 236us/sample - loss: 0.2190 - accuracy: 1.0000 - val_loss: 0.2402 - val_accuracy: 1.0000\n",
            "Epoch 291/300\n",
            "105/105 [==============================] - 0s 238us/sample - loss: 0.2187 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 1.0000\n",
            "Epoch 292/300\n",
            "105/105 [==============================] - 0s 212us/sample - loss: 0.2184 - accuracy: 1.0000 - val_loss: 0.2398 - val_accuracy: 1.0000\n",
            "Epoch 293/300\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.2181 - accuracy: 1.0000 - val_loss: 0.2394 - val_accuracy: 1.0000\n",
            "Epoch 294/300\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 0.2178 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 1.0000\n",
            "Epoch 295/300\n",
            "105/105 [==============================] - 0s 226us/sample - loss: 0.2175 - accuracy: 1.0000 - val_loss: 0.2388 - val_accuracy: 1.0000\n",
            "Epoch 296/300\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.2172 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 1.0000\n",
            "Epoch 297/300\n",
            "105/105 [==============================] - 0s 233us/sample - loss: 0.2169 - accuracy: 1.0000 - val_loss: 0.2384 - val_accuracy: 1.0000\n",
            "Epoch 298/300\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 0.2166 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 1.0000\n",
            "Epoch 299/300\n",
            "105/105 [==============================] - 0s 207us/sample - loss: 0.2163 - accuracy: 1.0000 - val_loss: 0.2378 - val_accuracy: 1.0000\n",
            "Epoch 300/300\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.2160 - accuracy: 1.0000 - val_loss: 0.2374 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "367pGaFc3RW0",
        "colab_type": "text"
      },
      "source": [
        "Acompanhamento da convergência do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZiQs4Muic1e",
        "colab_type": "code",
        "outputId": "8b22dcae-915c-4c62-9eee-676814637264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBhjTIKzIDDGmAT3/wESEyfRygcmwQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqqAxytM2k-L",
        "colab_type": "code",
        "outputId": "e389fcf2-6845-4981-f409-4446df43b3eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQEQk4ZQIREQSTolARCTh/j/Ye19y3h48GQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEjej8Yi3prN",
        "colab_type": "text"
      },
      "source": [
        "Dados sobre o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQM8r4ML3rnc",
        "colab_type": "code",
        "outputId": "9ca0f744-856f-4f28-fc0e-b701918913d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 5\n",
            "Trainable params: 5\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx_nO1Mp3t0_",
        "colab_type": "code",
        "outputId": "64c0650c-0dac-4d0f-895f-87cb7dea228b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model.weights #os parâmetros do modelo"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_1/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
              " array([[-0.23687287],\n",
              "        [-0.17132577],\n",
              "        [ 0.27488634],\n",
              "        [ 1.5097218 ]], dtype=float32)>,\n",
              " <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.3467143], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzOL3O6_f-Ab",
        "colab_type": "code",
        "outputId": "0d63e123-ac52-4b81-def9-842af1c2a5fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict_classes([X[0:1]])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUjcOy_QhE6u",
        "colab_type": "code",
        "outputId": "bdeee0fa-b61c-49d1-ca97-ea98c46eba5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r45/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 90us/sample - loss: 0.2523 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2374152276251051, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pz6BXb3gL-7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}