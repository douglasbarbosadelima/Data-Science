{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "deep_iris2 .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douglasbarbosadelima/Data-Science/blob/master/deep_iris2_multiclass_classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFFlrJEyR_iN",
        "colab_type": "text"
      },
      "source": [
        "Classificação Multiclasse com Iris e TF 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ehhQFEK449e",
        "colab_type": "text"
      },
      "source": [
        "Parte 1: montando e calibrando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLY0Ut32SMMb",
        "colab_type": "code",
        "outputId": "09f89190-09c2-4415-dc17-726197f65729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0 #com GPU"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.17.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.27.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (45.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jte8_L_eTN26",
        "colab_type": "code",
        "outputId": "afb4e0b5-0d1b-4927-a69c-5540d61104df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1vkzIf8Y3eP",
        "colab_type": "code",
        "outputId": "faf27ef8-2d7a-48d9-c4ee-4189ee2d9ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X = iris.data\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq3WFyBxY3eW",
        "colab_type": "code",
        "outputId": "bc181311-f473-453a-d753-aab00b7a39f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = iris.target\n",
        "y[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9z8YRhTY3ee",
        "colab_type": "code",
        "outputId": "bdd96c84-0852-4236-8e64-06192d8ea4ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehot = OneHotEncoder()\n",
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEHyEASzY3eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y é (150,)...mude para(150,1)\n",
        "y=y.reshape((150,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMpv9Vg4Y3el",
        "colab_type": "code",
        "outputId": "cc9aa0f7-ce65-4b9a-8257-72e2a6b46bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "y = onehot.fit_transform(y).toarray()\n",
        "y[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKeMwtJbY3eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PbP4TDkY3e3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#crie o modelo com duas hidden layers de 4 perceptrons, relu e 3 outputs\n",
        "#com softmax \n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(2, activation='relu',input_shape=(4,)))\n",
        "model.add(tf.keras.layers.Dense(2, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXrntr4DZvvQ",
        "colab_type": "code",
        "outputId": "232c8ce2-ec21-44cf-a972-4c6455033eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "hist=model.fit(x=X_train,y=y_train, validation_data=(X_test, y_test),epochs=300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 45 samples\n",
            "Epoch 1/300\n",
            "105/105 [==============================] - 2s 17ms/sample - loss: 2.1541 - accuracy: 0.2857 - val_loss: 1.5984 - val_accuracy: 0.4444\n",
            "Epoch 2/300\n",
            "105/105 [==============================] - 0s 351us/sample - loss: 2.0702 - accuracy: 0.2857 - val_loss: 1.5410 - val_accuracy: 0.4444\n",
            "Epoch 3/300\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 1.9918 - accuracy: 0.2857 - val_loss: 1.4856 - val_accuracy: 0.4444\n",
            "Epoch 4/300\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 1.9171 - accuracy: 0.2857 - val_loss: 1.4324 - val_accuracy: 0.4444\n",
            "Epoch 5/300\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 1.8422 - accuracy: 0.2857 - val_loss: 1.3826 - val_accuracy: 0.4444\n",
            "Epoch 6/300\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 1.7740 - accuracy: 0.2857 - val_loss: 1.3355 - val_accuracy: 0.4444\n",
            "Epoch 7/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 1.7067 - accuracy: 0.2857 - val_loss: 1.2923 - val_accuracy: 0.4444\n",
            "Epoch 8/300\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 1.6459 - accuracy: 0.2857 - val_loss: 1.2520 - val_accuracy: 0.4444\n",
            "Epoch 9/300\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 1.5914 - accuracy: 0.2857 - val_loss: 1.2146 - val_accuracy: 0.4444\n",
            "Epoch 10/300\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 1.5356 - accuracy: 0.2857 - val_loss: 1.1816 - val_accuracy: 0.4444\n",
            "Epoch 11/300\n",
            "105/105 [==============================] - 0s 311us/sample - loss: 1.4846 - accuracy: 0.2857 - val_loss: 1.1518 - val_accuracy: 0.4444\n",
            "Epoch 12/300\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 1.4413 - accuracy: 0.2857 - val_loss: 1.1241 - val_accuracy: 0.4444\n",
            "Epoch 13/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 1.3989 - accuracy: 0.2857 - val_loss: 1.0991 - val_accuracy: 0.4444\n",
            "Epoch 14/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 1.3573 - accuracy: 0.2857 - val_loss: 1.0772 - val_accuracy: 0.4444\n",
            "Epoch 15/300\n",
            "105/105 [==============================] - 0s 312us/sample - loss: 1.3212 - accuracy: 0.2857 - val_loss: 1.0570 - val_accuracy: 0.4444\n",
            "Epoch 16/300\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 1.2867 - accuracy: 0.2857 - val_loss: 1.0389 - val_accuracy: 0.4444\n",
            "Epoch 17/300\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 1.2554 - accuracy: 0.2857 - val_loss: 1.0231 - val_accuracy: 0.4444\n",
            "Epoch 18/300\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 1.2251 - accuracy: 0.2857 - val_loss: 1.0097 - val_accuracy: 0.4444\n",
            "Epoch 19/300\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 1.1967 - accuracy: 0.2857 - val_loss: 0.9986 - val_accuracy: 0.4444\n",
            "Epoch 20/300\n",
            "105/105 [==============================] - 0s 308us/sample - loss: 1.1737 - accuracy: 0.2857 - val_loss: 0.9893 - val_accuracy: 0.4444\n",
            "Epoch 21/300\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 1.1515 - accuracy: 0.2857 - val_loss: 0.9815 - val_accuracy: 0.4444\n",
            "Epoch 22/300\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 1.1309 - accuracy: 0.2857 - val_loss: 0.9754 - val_accuracy: 0.4444\n",
            "Epoch 23/300\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 1.1136 - accuracy: 0.2857 - val_loss: 0.9706 - val_accuracy: 0.4444\n",
            "Epoch 24/300\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 1.0990 - accuracy: 0.2857 - val_loss: 0.9671 - val_accuracy: 0.4444\n",
            "Epoch 25/300\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 1.0861 - accuracy: 0.2857 - val_loss: 0.9645 - val_accuracy: 0.4444\n",
            "Epoch 26/300\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 1.0729 - accuracy: 0.2762 - val_loss: 0.9626 - val_accuracy: 0.4000\n",
            "Epoch 27/300\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 1.0620 - accuracy: 0.2762 - val_loss: 0.9608 - val_accuracy: 0.4000\n",
            "Epoch 28/300\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 1.0530 - accuracy: 0.2667 - val_loss: 0.9598 - val_accuracy: 0.4000\n",
            "Epoch 29/300\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 1.0435 - accuracy: 0.2667 - val_loss: 0.9586 - val_accuracy: 0.4000\n",
            "Epoch 30/300\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 1.0346 - accuracy: 0.2667 - val_loss: 0.9575 - val_accuracy: 0.4000\n",
            "Epoch 31/300\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 1.0271 - accuracy: 0.2667 - val_loss: 0.9573 - val_accuracy: 0.3778\n",
            "Epoch 32/300\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 1.0210 - accuracy: 0.2476 - val_loss: 0.9579 - val_accuracy: 0.4000\n",
            "Epoch 33/300\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 1.0133 - accuracy: 0.2476 - val_loss: 0.9579 - val_accuracy: 0.4000\n",
            "Epoch 34/300\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 1.0074 - accuracy: 0.2476 - val_loss: 0.9580 - val_accuracy: 0.3111\n",
            "Epoch 35/300\n",
            "105/105 [==============================] - 0s 377us/sample - loss: 1.0026 - accuracy: 0.2476 - val_loss: 0.9581 - val_accuracy: 0.2667\n",
            "Epoch 36/300\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.9974 - accuracy: 0.2476 - val_loss: 0.9574 - val_accuracy: 0.2667\n",
            "Epoch 37/300\n",
            "105/105 [==============================] - 0s 310us/sample - loss: 0.9922 - accuracy: 0.2190 - val_loss: 0.9567 - val_accuracy: 0.2444\n",
            "Epoch 38/300\n",
            "105/105 [==============================] - 0s 317us/sample - loss: 0.9881 - accuracy: 0.2190 - val_loss: 0.9556 - val_accuracy: 0.2444\n",
            "Epoch 39/300\n",
            "105/105 [==============================] - 0s 349us/sample - loss: 0.9842 - accuracy: 0.2095 - val_loss: 0.9557 - val_accuracy: 0.2444\n",
            "Epoch 40/300\n",
            "105/105 [==============================] - 0s 313us/sample - loss: 0.9796 - accuracy: 0.2095 - val_loss: 0.9554 - val_accuracy: 0.2444\n",
            "Epoch 41/300\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.9762 - accuracy: 0.2000 - val_loss: 0.9560 - val_accuracy: 0.2222\n",
            "Epoch 42/300\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.9724 - accuracy: 0.2095 - val_loss: 0.9557 - val_accuracy: 0.2222\n",
            "Epoch 43/300\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.9689 - accuracy: 0.2095 - val_loss: 0.9550 - val_accuracy: 0.2222\n",
            "Epoch 44/300\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.9657 - accuracy: 0.2095 - val_loss: 0.9537 - val_accuracy: 0.2222\n",
            "Epoch 45/300\n",
            "105/105 [==============================] - 0s 324us/sample - loss: 0.9628 - accuracy: 0.2190 - val_loss: 0.9523 - val_accuracy: 0.2444\n",
            "Epoch 46/300\n",
            "105/105 [==============================] - 0s 311us/sample - loss: 0.9598 - accuracy: 0.2190 - val_loss: 0.9514 - val_accuracy: 0.2444\n",
            "Epoch 47/300\n",
            "105/105 [==============================] - 0s 285us/sample - loss: 0.9569 - accuracy: 0.2190 - val_loss: 0.9497 - val_accuracy: 0.2444\n",
            "Epoch 48/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.9540 - accuracy: 0.2381 - val_loss: 0.9492 - val_accuracy: 0.2444\n",
            "Epoch 49/300\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.9515 - accuracy: 0.2476 - val_loss: 0.9477 - val_accuracy: 0.2667\n",
            "Epoch 50/300\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.9487 - accuracy: 0.2667 - val_loss: 0.9451 - val_accuracy: 0.2667\n",
            "Epoch 51/300\n",
            "105/105 [==============================] - 0s 317us/sample - loss: 0.9462 - accuracy: 0.2762 - val_loss: 0.9426 - val_accuracy: 0.2667\n",
            "Epoch 52/300\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 0.9434 - accuracy: 0.3048 - val_loss: 0.9403 - val_accuracy: 0.2889\n",
            "Epoch 53/300\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.9408 - accuracy: 0.3143 - val_loss: 0.9386 - val_accuracy: 0.2889\n",
            "Epoch 54/300\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.9381 - accuracy: 0.3238 - val_loss: 0.9367 - val_accuracy: 0.2889\n",
            "Epoch 55/300\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.9353 - accuracy: 0.3333 - val_loss: 0.9347 - val_accuracy: 0.2889\n",
            "Epoch 56/300\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.9325 - accuracy: 0.3429 - val_loss: 0.9323 - val_accuracy: 0.3111\n",
            "Epoch 57/300\n",
            "105/105 [==============================] - 0s 326us/sample - loss: 0.9300 - accuracy: 0.3619 - val_loss: 0.9286 - val_accuracy: 0.3111\n",
            "Epoch 58/300\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.9273 - accuracy: 0.3524 - val_loss: 0.9257 - val_accuracy: 0.3778\n",
            "Epoch 59/300\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.9248 - accuracy: 0.3905 - val_loss: 0.9224 - val_accuracy: 0.4222\n",
            "Epoch 60/300\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.9222 - accuracy: 0.4095 - val_loss: 0.9207 - val_accuracy: 0.4222\n",
            "Epoch 61/300\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.9196 - accuracy: 0.4190 - val_loss: 0.9187 - val_accuracy: 0.4222\n",
            "Epoch 62/300\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.9167 - accuracy: 0.4286 - val_loss: 0.9179 - val_accuracy: 0.4444\n",
            "Epoch 63/300\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.9139 - accuracy: 0.4286 - val_loss: 0.9171 - val_accuracy: 0.4444\n",
            "Epoch 64/300\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.9109 - accuracy: 0.4571 - val_loss: 0.9158 - val_accuracy: 0.4667\n",
            "Epoch 65/300\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.9081 - accuracy: 0.4762 - val_loss: 0.9144 - val_accuracy: 0.5111\n",
            "Epoch 66/300\n",
            "105/105 [==============================] - 0s 270us/sample - loss: 0.9053 - accuracy: 0.4857 - val_loss: 0.9142 - val_accuracy: 0.5333\n",
            "Epoch 67/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.9024 - accuracy: 0.4857 - val_loss: 0.9137 - val_accuracy: 0.5333\n",
            "Epoch 68/300\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.8998 - accuracy: 0.5238 - val_loss: 0.9124 - val_accuracy: 0.5333\n",
            "Epoch 69/300\n",
            "105/105 [==============================] - 0s 297us/sample - loss: 0.8969 - accuracy: 0.5333 - val_loss: 0.9090 - val_accuracy: 0.5333\n",
            "Epoch 70/300\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.8942 - accuracy: 0.5333 - val_loss: 0.9068 - val_accuracy: 0.5333\n",
            "Epoch 71/300\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 0.8913 - accuracy: 0.5333 - val_loss: 0.9031 - val_accuracy: 0.5333\n",
            "Epoch 72/300\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.8888 - accuracy: 0.5429 - val_loss: 0.8989 - val_accuracy: 0.5333\n",
            "Epoch 73/300\n",
            "105/105 [==============================] - 0s 310us/sample - loss: 0.8859 - accuracy: 0.5429 - val_loss: 0.8963 - val_accuracy: 0.5333\n",
            "Epoch 74/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.8830 - accuracy: 0.5429 - val_loss: 0.8940 - val_accuracy: 0.5333\n",
            "Epoch 75/300\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.8801 - accuracy: 0.5524 - val_loss: 0.8905 - val_accuracy: 0.5333\n",
            "Epoch 76/300\n",
            "105/105 [==============================] - 0s 362us/sample - loss: 0.8774 - accuracy: 0.5619 - val_loss: 0.8877 - val_accuracy: 0.5333\n",
            "Epoch 77/300\n",
            "105/105 [==============================] - 0s 318us/sample - loss: 0.8746 - accuracy: 0.5619 - val_loss: 0.8856 - val_accuracy: 0.5333\n",
            "Epoch 78/300\n",
            "105/105 [==============================] - 0s 310us/sample - loss: 0.8720 - accuracy: 0.5714 - val_loss: 0.8837 - val_accuracy: 0.5333\n",
            "Epoch 79/300\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.8691 - accuracy: 0.5714 - val_loss: 0.8804 - val_accuracy: 0.5333\n",
            "Epoch 80/300\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.8664 - accuracy: 0.5714 - val_loss: 0.8782 - val_accuracy: 0.5333\n",
            "Epoch 81/300\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.8637 - accuracy: 0.5714 - val_loss: 0.8768 - val_accuracy: 0.5333\n",
            "Epoch 82/300\n",
            "105/105 [==============================] - 0s 329us/sample - loss: 0.8612 - accuracy: 0.5714 - val_loss: 0.8759 - val_accuracy: 0.5333\n",
            "Epoch 83/300\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.8584 - accuracy: 0.5714 - val_loss: 0.8739 - val_accuracy: 0.5333\n",
            "Epoch 84/300\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.8558 - accuracy: 0.5810 - val_loss: 0.8722 - val_accuracy: 0.5333\n",
            "Epoch 85/300\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.8533 - accuracy: 0.5810 - val_loss: 0.8706 - val_accuracy: 0.5111\n",
            "Epoch 86/300\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.8513 - accuracy: 0.5905 - val_loss: 0.8698 - val_accuracy: 0.5111\n",
            "Epoch 87/300\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.8485 - accuracy: 0.5905 - val_loss: 0.8669 - val_accuracy: 0.5111\n",
            "Epoch 88/300\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.8461 - accuracy: 0.5905 - val_loss: 0.8642 - val_accuracy: 0.5111\n",
            "Epoch 89/300\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.8438 - accuracy: 0.5905 - val_loss: 0.8621 - val_accuracy: 0.5111\n",
            "Epoch 90/300\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.8416 - accuracy: 0.5905 - val_loss: 0.8595 - val_accuracy: 0.5111\n",
            "Epoch 91/300\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.8391 - accuracy: 0.5905 - val_loss: 0.8542 - val_accuracy: 0.5333\n",
            "Epoch 92/300\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.8367 - accuracy: 0.5905 - val_loss: 0.8501 - val_accuracy: 0.5333\n",
            "Epoch 93/300\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.8344 - accuracy: 0.5905 - val_loss: 0.8458 - val_accuracy: 0.5333\n",
            "Epoch 94/300\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.8322 - accuracy: 0.5905 - val_loss: 0.8413 - val_accuracy: 0.5333\n",
            "Epoch 95/300\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.8301 - accuracy: 0.5810 - val_loss: 0.8365 - val_accuracy: 0.5556\n",
            "Epoch 96/300\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.8278 - accuracy: 0.5714 - val_loss: 0.8322 - val_accuracy: 0.5778\n",
            "Epoch 97/300\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.8260 - accuracy: 0.5714 - val_loss: 0.8274 - val_accuracy: 0.5556\n",
            "Epoch 98/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.8239 - accuracy: 0.5810 - val_loss: 0.8235 - val_accuracy: 0.5556\n",
            "Epoch 99/300\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.8216 - accuracy: 0.5810 - val_loss: 0.8210 - val_accuracy: 0.5556\n",
            "Epoch 100/300\n",
            "105/105 [==============================] - 0s 315us/sample - loss: 0.8195 - accuracy: 0.5905 - val_loss: 0.8182 - val_accuracy: 0.5333\n",
            "Epoch 101/300\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.8174 - accuracy: 0.5905 - val_loss: 0.8165 - val_accuracy: 0.5556\n",
            "Epoch 102/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.8152 - accuracy: 0.6095 - val_loss: 0.8133 - val_accuracy: 0.5333\n",
            "Epoch 103/300\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.8130 - accuracy: 0.6000 - val_loss: 0.8107 - val_accuracy: 0.5333\n",
            "Epoch 104/300\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.8110 - accuracy: 0.6000 - val_loss: 0.8084 - val_accuracy: 0.5556\n",
            "Epoch 105/300\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.8088 - accuracy: 0.6000 - val_loss: 0.8065 - val_accuracy: 0.5556\n",
            "Epoch 106/300\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.8067 - accuracy: 0.6000 - val_loss: 0.8058 - val_accuracy: 0.5778\n",
            "Epoch 107/300\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.8050 - accuracy: 0.5905 - val_loss: 0.8050 - val_accuracy: 0.5778\n",
            "Epoch 108/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.8024 - accuracy: 0.6000 - val_loss: 0.8019 - val_accuracy: 0.5778\n",
            "Epoch 109/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.8008 - accuracy: 0.6095 - val_loss: 0.7982 - val_accuracy: 0.5778\n",
            "Epoch 110/300\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.7986 - accuracy: 0.6095 - val_loss: 0.7958 - val_accuracy: 0.5778\n",
            "Epoch 111/300\n",
            "105/105 [==============================] - 0s 313us/sample - loss: 0.7967 - accuracy: 0.6190 - val_loss: 0.7949 - val_accuracy: 0.5778\n",
            "Epoch 112/300\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.7947 - accuracy: 0.6190 - val_loss: 0.7934 - val_accuracy: 0.5778\n",
            "Epoch 113/300\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.7929 - accuracy: 0.6190 - val_loss: 0.7913 - val_accuracy: 0.5778\n",
            "Epoch 114/300\n",
            "105/105 [==============================] - 0s 227us/sample - loss: 0.7909 - accuracy: 0.6190 - val_loss: 0.7887 - val_accuracy: 0.5778\n",
            "Epoch 115/300\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.7892 - accuracy: 0.6190 - val_loss: 0.7863 - val_accuracy: 0.6000\n",
            "Epoch 116/300\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.7875 - accuracy: 0.6095 - val_loss: 0.7840 - val_accuracy: 0.6000\n",
            "Epoch 117/300\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.7856 - accuracy: 0.6190 - val_loss: 0.7828 - val_accuracy: 0.6000\n",
            "Epoch 118/300\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.7839 - accuracy: 0.6095 - val_loss: 0.7802 - val_accuracy: 0.6000\n",
            "Epoch 119/300\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.7820 - accuracy: 0.6190 - val_loss: 0.7784 - val_accuracy: 0.6000\n",
            "Epoch 120/300\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.7802 - accuracy: 0.6286 - val_loss: 0.7743 - val_accuracy: 0.5556\n",
            "Epoch 121/300\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.7783 - accuracy: 0.6381 - val_loss: 0.7720 - val_accuracy: 0.5556\n",
            "Epoch 122/300\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.7767 - accuracy: 0.6286 - val_loss: 0.7697 - val_accuracy: 0.5556\n",
            "Epoch 123/300\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.7749 - accuracy: 0.6381 - val_loss: 0.7689 - val_accuracy: 0.5556\n",
            "Epoch 124/300\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.7730 - accuracy: 0.6381 - val_loss: 0.7678 - val_accuracy: 0.5778\n",
            "Epoch 125/300\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 0.7715 - accuracy: 0.6286 - val_loss: 0.7678 - val_accuracy: 0.6000\n",
            "Epoch 126/300\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.7695 - accuracy: 0.6190 - val_loss: 0.7660 - val_accuracy: 0.6000\n",
            "Epoch 127/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.7678 - accuracy: 0.6381 - val_loss: 0.7639 - val_accuracy: 0.6000\n",
            "Epoch 128/300\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.7661 - accuracy: 0.6476 - val_loss: 0.7611 - val_accuracy: 0.5778\n",
            "Epoch 129/300\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.7646 - accuracy: 0.6381 - val_loss: 0.7576 - val_accuracy: 0.5778\n",
            "Epoch 130/300\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.7631 - accuracy: 0.6286 - val_loss: 0.7548 - val_accuracy: 0.5778\n",
            "Epoch 131/300\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.7614 - accuracy: 0.6381 - val_loss: 0.7526 - val_accuracy: 0.5778\n",
            "Epoch 132/300\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.7600 - accuracy: 0.6286 - val_loss: 0.7503 - val_accuracy: 0.5778\n",
            "Epoch 133/300\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.7585 - accuracy: 0.6381 - val_loss: 0.7500 - val_accuracy: 0.5778\n",
            "Epoch 134/300\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.7567 - accuracy: 0.6286 - val_loss: 0.7479 - val_accuracy: 0.5778\n",
            "Epoch 135/300\n",
            "105/105 [==============================] - 0s 385us/sample - loss: 0.7552 - accuracy: 0.6286 - val_loss: 0.7474 - val_accuracy: 0.5778\n",
            "Epoch 136/300\n",
            "105/105 [==============================] - 0s 296us/sample - loss: 0.7535 - accuracy: 0.6476 - val_loss: 0.7457 - val_accuracy: 0.5778\n",
            "Epoch 137/300\n",
            "105/105 [==============================] - 0s 301us/sample - loss: 0.7521 - accuracy: 0.6476 - val_loss: 0.7431 - val_accuracy: 0.5778\n",
            "Epoch 138/300\n",
            "105/105 [==============================] - 0s 307us/sample - loss: 0.7505 - accuracy: 0.6286 - val_loss: 0.7421 - val_accuracy: 0.5778\n",
            "Epoch 139/300\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.7489 - accuracy: 0.6286 - val_loss: 0.7406 - val_accuracy: 0.5778\n",
            "Epoch 140/300\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.7474 - accuracy: 0.6476 - val_loss: 0.7391 - val_accuracy: 0.5778\n",
            "Epoch 141/300\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.7457 - accuracy: 0.6381 - val_loss: 0.7367 - val_accuracy: 0.6000\n",
            "Epoch 142/300\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.7443 - accuracy: 0.6381 - val_loss: 0.7344 - val_accuracy: 0.6000\n",
            "Epoch 143/300\n",
            "105/105 [==============================] - 0s 284us/sample - loss: 0.7430 - accuracy: 0.6476 - val_loss: 0.7321 - val_accuracy: 0.6222\n",
            "Epoch 144/300\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.7415 - accuracy: 0.6476 - val_loss: 0.7314 - val_accuracy: 0.6222\n",
            "Epoch 145/300\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.7398 - accuracy: 0.6476 - val_loss: 0.7306 - val_accuracy: 0.6222\n",
            "Epoch 146/300\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.7383 - accuracy: 0.6476 - val_loss: 0.7303 - val_accuracy: 0.6000\n",
            "Epoch 147/300\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.7368 - accuracy: 0.6571 - val_loss: 0.7295 - val_accuracy: 0.6000\n",
            "Epoch 148/300\n",
            "105/105 [==============================] - 0s 297us/sample - loss: 0.7353 - accuracy: 0.6476 - val_loss: 0.7293 - val_accuracy: 0.5778\n",
            "Epoch 149/300\n",
            "105/105 [==============================] - 0s 260us/sample - loss: 0.7340 - accuracy: 0.6476 - val_loss: 0.7292 - val_accuracy: 0.6222\n",
            "Epoch 150/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.7324 - accuracy: 0.6571 - val_loss: 0.7289 - val_accuracy: 0.6222\n",
            "Epoch 151/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.7309 - accuracy: 0.6571 - val_loss: 0.7285 - val_accuracy: 0.6222\n",
            "Epoch 152/300\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.7295 - accuracy: 0.6571 - val_loss: 0.7286 - val_accuracy: 0.6222\n",
            "Epoch 153/300\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.7281 - accuracy: 0.6571 - val_loss: 0.7272 - val_accuracy: 0.6222\n",
            "Epoch 154/300\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.7266 - accuracy: 0.6571 - val_loss: 0.7256 - val_accuracy: 0.6222\n",
            "Epoch 155/300\n",
            "105/105 [==============================] - 0s 321us/sample - loss: 0.7253 - accuracy: 0.6571 - val_loss: 0.7241 - val_accuracy: 0.6222\n",
            "Epoch 156/300\n",
            "105/105 [==============================] - 0s 336us/sample - loss: 0.7239 - accuracy: 0.6571 - val_loss: 0.7235 - val_accuracy: 0.6222\n",
            "Epoch 157/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.7225 - accuracy: 0.6571 - val_loss: 0.7217 - val_accuracy: 0.6222\n",
            "Epoch 158/300\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.7212 - accuracy: 0.6667 - val_loss: 0.7193 - val_accuracy: 0.6222\n",
            "Epoch 159/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.7198 - accuracy: 0.6667 - val_loss: 0.7179 - val_accuracy: 0.6222\n",
            "Epoch 160/300\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.7183 - accuracy: 0.6667 - val_loss: 0.7146 - val_accuracy: 0.6444\n",
            "Epoch 161/300\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.7170 - accuracy: 0.6667 - val_loss: 0.7118 - val_accuracy: 0.6444\n",
            "Epoch 162/300\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.7156 - accuracy: 0.6762 - val_loss: 0.7092 - val_accuracy: 0.6222\n",
            "Epoch 163/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.7148 - accuracy: 0.6762 - val_loss: 0.7062 - val_accuracy: 0.6222\n",
            "Epoch 164/300\n",
            "105/105 [==============================] - 0s 282us/sample - loss: 0.7132 - accuracy: 0.6667 - val_loss: 0.7055 - val_accuracy: 0.6222\n",
            "Epoch 165/300\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.7119 - accuracy: 0.6667 - val_loss: 0.7038 - val_accuracy: 0.6222\n",
            "Epoch 166/300\n",
            "105/105 [==============================] - 0s 305us/sample - loss: 0.7108 - accuracy: 0.6667 - val_loss: 0.7035 - val_accuracy: 0.6222\n",
            "Epoch 167/300\n",
            "105/105 [==============================] - 0s 251us/sample - loss: 0.7092 - accuracy: 0.6762 - val_loss: 0.7026 - val_accuracy: 0.6444\n",
            "Epoch 168/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.7079 - accuracy: 0.6857 - val_loss: 0.7020 - val_accuracy: 0.6444\n",
            "Epoch 169/300\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.7065 - accuracy: 0.6857 - val_loss: 0.6997 - val_accuracy: 0.6222\n",
            "Epoch 170/300\n",
            "105/105 [==============================] - 0s 287us/sample - loss: 0.7053 - accuracy: 0.6762 - val_loss: 0.6967 - val_accuracy: 0.6222\n",
            "Epoch 171/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.7041 - accuracy: 0.6667 - val_loss: 0.6944 - val_accuracy: 0.6889\n",
            "Epoch 172/300\n",
            "105/105 [==============================] - 0s 357us/sample - loss: 0.7027 - accuracy: 0.6667 - val_loss: 0.6940 - val_accuracy: 0.6222\n",
            "Epoch 173/300\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.7014 - accuracy: 0.6667 - val_loss: 0.6929 - val_accuracy: 0.6222\n",
            "Epoch 174/300\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.7000 - accuracy: 0.6667 - val_loss: 0.6930 - val_accuracy: 0.6444\n",
            "Epoch 175/300\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.6987 - accuracy: 0.6857 - val_loss: 0.6935 - val_accuracy: 0.6444\n",
            "Epoch 176/300\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.6976 - accuracy: 0.6952 - val_loss: 0.6941 - val_accuracy: 0.6667\n",
            "Epoch 177/300\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.6961 - accuracy: 0.7048 - val_loss: 0.6932 - val_accuracy: 0.6667\n",
            "Epoch 178/300\n",
            "105/105 [==============================] - 0s 309us/sample - loss: 0.6949 - accuracy: 0.7048 - val_loss: 0.6918 - val_accuracy: 0.6667\n",
            "Epoch 179/300\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.6936 - accuracy: 0.7048 - val_loss: 0.6911 - val_accuracy: 0.6667\n",
            "Epoch 180/300\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.6927 - accuracy: 0.7048 - val_loss: 0.6892 - val_accuracy: 0.6667\n",
            "Epoch 181/300\n",
            "105/105 [==============================] - 0s 314us/sample - loss: 0.6910 - accuracy: 0.6952 - val_loss: 0.6906 - val_accuracy: 0.6667\n",
            "Epoch 182/300\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.6900 - accuracy: 0.6762 - val_loss: 0.6926 - val_accuracy: 0.6444\n",
            "Epoch 183/300\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 0.6889 - accuracy: 0.6857 - val_loss: 0.6936 - val_accuracy: 0.6444\n",
            "Epoch 184/300\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.6875 - accuracy: 0.6667 - val_loss: 0.6933 - val_accuracy: 0.6444\n",
            "Epoch 185/300\n",
            "105/105 [==============================] - 0s 290us/sample - loss: 0.6863 - accuracy: 0.6667 - val_loss: 0.6938 - val_accuracy: 0.6444\n",
            "Epoch 186/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.6855 - accuracy: 0.6571 - val_loss: 0.6952 - val_accuracy: 0.6444\n",
            "Epoch 187/300\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.6841 - accuracy: 0.6571 - val_loss: 0.6938 - val_accuracy: 0.6444\n",
            "Epoch 188/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.6829 - accuracy: 0.6571 - val_loss: 0.6923 - val_accuracy: 0.6444\n",
            "Epoch 189/300\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.6816 - accuracy: 0.6571 - val_loss: 0.6907 - val_accuracy: 0.6444\n",
            "Epoch 190/300\n",
            "105/105 [==============================] - 0s 311us/sample - loss: 0.6804 - accuracy: 0.6667 - val_loss: 0.6887 - val_accuracy: 0.6444\n",
            "Epoch 191/300\n",
            "105/105 [==============================] - 0s 324us/sample - loss: 0.6791 - accuracy: 0.6667 - val_loss: 0.6870 - val_accuracy: 0.6444\n",
            "Epoch 192/300\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.6779 - accuracy: 0.6762 - val_loss: 0.6849 - val_accuracy: 0.6444\n",
            "Epoch 193/300\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.6765 - accuracy: 0.6857 - val_loss: 0.6814 - val_accuracy: 0.6667\n",
            "Epoch 194/300\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.6757 - accuracy: 0.6857 - val_loss: 0.6782 - val_accuracy: 0.6889\n",
            "Epoch 195/300\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.6741 - accuracy: 0.6857 - val_loss: 0.6782 - val_accuracy: 0.6889\n",
            "Epoch 196/300\n",
            "105/105 [==============================] - 0s 258us/sample - loss: 0.6729 - accuracy: 0.6952 - val_loss: 0.6771 - val_accuracy: 0.6889\n",
            "Epoch 197/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.6718 - accuracy: 0.6952 - val_loss: 0.6752 - val_accuracy: 0.6889\n",
            "Epoch 198/300\n",
            "105/105 [==============================] - 0s 321us/sample - loss: 0.6705 - accuracy: 0.6952 - val_loss: 0.6743 - val_accuracy: 0.6889\n",
            "Epoch 199/300\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.6693 - accuracy: 0.6952 - val_loss: 0.6728 - val_accuracy: 0.6889\n",
            "Epoch 200/300\n",
            "105/105 [==============================] - 0s 246us/sample - loss: 0.6681 - accuracy: 0.6952 - val_loss: 0.6720 - val_accuracy: 0.6889\n",
            "Epoch 201/300\n",
            "105/105 [==============================] - 0s 364us/sample - loss: 0.6669 - accuracy: 0.6952 - val_loss: 0.6724 - val_accuracy: 0.6889\n",
            "Epoch 202/300\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.6658 - accuracy: 0.7048 - val_loss: 0.6723 - val_accuracy: 0.6667\n",
            "Epoch 203/300\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.6649 - accuracy: 0.6952 - val_loss: 0.6729 - val_accuracy: 0.6667\n",
            "Epoch 204/300\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.6637 - accuracy: 0.6952 - val_loss: 0.6715 - val_accuracy: 0.6667\n",
            "Epoch 205/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.6625 - accuracy: 0.6952 - val_loss: 0.6697 - val_accuracy: 0.6889\n",
            "Epoch 206/300\n",
            "105/105 [==============================] - 0s 302us/sample - loss: 0.6613 - accuracy: 0.6952 - val_loss: 0.6683 - val_accuracy: 0.6889\n",
            "Epoch 207/300\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.6601 - accuracy: 0.7048 - val_loss: 0.6665 - val_accuracy: 0.7111\n",
            "Epoch 208/300\n",
            "105/105 [==============================] - 0s 241us/sample - loss: 0.6590 - accuracy: 0.7048 - val_loss: 0.6643 - val_accuracy: 0.7111\n",
            "Epoch 209/300\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.6578 - accuracy: 0.7048 - val_loss: 0.6640 - val_accuracy: 0.7111\n",
            "Epoch 210/300\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.6567 - accuracy: 0.7048 - val_loss: 0.6637 - val_accuracy: 0.7111\n",
            "Epoch 211/300\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.6558 - accuracy: 0.7143 - val_loss: 0.6636 - val_accuracy: 0.6889\n",
            "Epoch 212/300\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.6545 - accuracy: 0.7048 - val_loss: 0.6624 - val_accuracy: 0.7111\n",
            "Epoch 213/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.6535 - accuracy: 0.7143 - val_loss: 0.6612 - val_accuracy: 0.7111\n",
            "Epoch 214/300\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.6524 - accuracy: 0.7238 - val_loss: 0.6605 - val_accuracy: 0.7111\n",
            "Epoch 215/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.6513 - accuracy: 0.7143 - val_loss: 0.6592 - val_accuracy: 0.7111\n",
            "Epoch 216/300\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.6502 - accuracy: 0.7143 - val_loss: 0.6571 - val_accuracy: 0.7111\n",
            "Epoch 217/300\n",
            "105/105 [==============================] - 0s 304us/sample - loss: 0.6491 - accuracy: 0.7238 - val_loss: 0.6569 - val_accuracy: 0.7111\n",
            "Epoch 218/300\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.6479 - accuracy: 0.7238 - val_loss: 0.6560 - val_accuracy: 0.7333\n",
            "Epoch 219/300\n",
            "105/105 [==============================] - 0s 305us/sample - loss: 0.6469 - accuracy: 0.7238 - val_loss: 0.6540 - val_accuracy: 0.7111\n",
            "Epoch 220/300\n",
            "105/105 [==============================] - 0s 332us/sample - loss: 0.6459 - accuracy: 0.7238 - val_loss: 0.6535 - val_accuracy: 0.7556\n",
            "Epoch 221/300\n",
            "105/105 [==============================] - 0s 330us/sample - loss: 0.6447 - accuracy: 0.7238 - val_loss: 0.6506 - val_accuracy: 0.7111\n",
            "Epoch 222/300\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.6435 - accuracy: 0.7429 - val_loss: 0.6484 - val_accuracy: 0.7111\n",
            "Epoch 223/300\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.6422 - accuracy: 0.7429 - val_loss: 0.6455 - val_accuracy: 0.7111\n",
            "Epoch 224/300\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.6410 - accuracy: 0.7333 - val_loss: 0.6408 - val_accuracy: 0.7778\n",
            "Epoch 225/300\n",
            "105/105 [==============================] - 0s 250us/sample - loss: 0.6400 - accuracy: 0.7333 - val_loss: 0.6375 - val_accuracy: 0.7556\n",
            "Epoch 226/300\n",
            "105/105 [==============================] - 0s 307us/sample - loss: 0.6389 - accuracy: 0.7333 - val_loss: 0.6357 - val_accuracy: 0.7556\n",
            "Epoch 227/300\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.6379 - accuracy: 0.7333 - val_loss: 0.6340 - val_accuracy: 0.7556\n",
            "Epoch 228/300\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.6369 - accuracy: 0.7333 - val_loss: 0.6320 - val_accuracy: 0.7556\n",
            "Epoch 229/300\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.6358 - accuracy: 0.7333 - val_loss: 0.6310 - val_accuracy: 0.7556\n",
            "Epoch 230/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.6346 - accuracy: 0.7333 - val_loss: 0.6310 - val_accuracy: 0.7556\n",
            "Epoch 231/300\n",
            "105/105 [==============================] - 0s 279us/sample - loss: 0.6337 - accuracy: 0.7333 - val_loss: 0.6319 - val_accuracy: 0.7778\n",
            "Epoch 232/300\n",
            "105/105 [==============================] - 0s 294us/sample - loss: 0.6323 - accuracy: 0.7333 - val_loss: 0.6339 - val_accuracy: 0.7778\n",
            "Epoch 233/300\n",
            "105/105 [==============================] - 0s 295us/sample - loss: 0.6315 - accuracy: 0.7429 - val_loss: 0.6361 - val_accuracy: 0.7556\n",
            "Epoch 234/300\n",
            "105/105 [==============================] - 0s 326us/sample - loss: 0.6309 - accuracy: 0.7714 - val_loss: 0.6388 - val_accuracy: 0.7778\n",
            "Epoch 235/300\n",
            "105/105 [==============================] - 0s 289us/sample - loss: 0.6304 - accuracy: 0.7429 - val_loss: 0.6404 - val_accuracy: 0.7778\n",
            "Epoch 236/300\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.6292 - accuracy: 0.7238 - val_loss: 0.6391 - val_accuracy: 0.7778\n",
            "Epoch 237/300\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.6278 - accuracy: 0.7429 - val_loss: 0.6354 - val_accuracy: 0.7778\n",
            "Epoch 238/300\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.6267 - accuracy: 0.7714 - val_loss: 0.6315 - val_accuracy: 0.7778\n",
            "Epoch 239/300\n",
            "105/105 [==============================] - 0s 275us/sample - loss: 0.6253 - accuracy: 0.7714 - val_loss: 0.6283 - val_accuracy: 0.8000\n",
            "Epoch 240/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.6249 - accuracy: 0.7524 - val_loss: 0.6228 - val_accuracy: 0.7778\n",
            "Epoch 241/300\n",
            "105/105 [==============================] - 0s 272us/sample - loss: 0.6232 - accuracy: 0.7524 - val_loss: 0.6196 - val_accuracy: 0.7556\n",
            "Epoch 242/300\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.6223 - accuracy: 0.7429 - val_loss: 0.6163 - val_accuracy: 0.7556\n",
            "Epoch 243/300\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.6211 - accuracy: 0.7429 - val_loss: 0.6160 - val_accuracy: 0.7556\n",
            "Epoch 244/300\n",
            "105/105 [==============================] - 0s 262us/sample - loss: 0.6203 - accuracy: 0.7429 - val_loss: 0.6151 - val_accuracy: 0.7556\n",
            "Epoch 245/300\n",
            "105/105 [==============================] - 0s 274us/sample - loss: 0.6195 - accuracy: 0.7524 - val_loss: 0.6109 - val_accuracy: 0.7556\n",
            "Epoch 246/300\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.6181 - accuracy: 0.7429 - val_loss: 0.6090 - val_accuracy: 0.7556\n",
            "Epoch 247/300\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.6171 - accuracy: 0.7429 - val_loss: 0.6081 - val_accuracy: 0.7556\n",
            "Epoch 248/300\n",
            "105/105 [==============================] - 0s 265us/sample - loss: 0.6163 - accuracy: 0.7429 - val_loss: 0.6070 - val_accuracy: 0.8000\n",
            "Epoch 249/300\n",
            "105/105 [==============================] - 0s 253us/sample - loss: 0.6153 - accuracy: 0.7429 - val_loss: 0.6041 - val_accuracy: 0.7778\n",
            "Epoch 250/300\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.6143 - accuracy: 0.7524 - val_loss: 0.6029 - val_accuracy: 0.7778\n",
            "Epoch 251/300\n",
            "105/105 [==============================] - 0s 266us/sample - loss: 0.6132 - accuracy: 0.7524 - val_loss: 0.6014 - val_accuracy: 0.7778\n",
            "Epoch 252/300\n",
            "105/105 [==============================] - 0s 276us/sample - loss: 0.6123 - accuracy: 0.7524 - val_loss: 0.6000 - val_accuracy: 0.7778\n",
            "Epoch 253/300\n",
            "105/105 [==============================] - 0s 256us/sample - loss: 0.6114 - accuracy: 0.7524 - val_loss: 0.6007 - val_accuracy: 0.7778\n",
            "Epoch 254/300\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.6101 - accuracy: 0.7524 - val_loss: 0.6012 - val_accuracy: 0.8000\n",
            "Epoch 255/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.6090 - accuracy: 0.7619 - val_loss: 0.6020 - val_accuracy: 0.7778\n",
            "Epoch 256/300\n",
            "105/105 [==============================] - 0s 271us/sample - loss: 0.6083 - accuracy: 0.7619 - val_loss: 0.6035 - val_accuracy: 0.8222\n",
            "Epoch 257/300\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.6071 - accuracy: 0.7619 - val_loss: 0.6039 - val_accuracy: 0.8444\n",
            "Epoch 258/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.6062 - accuracy: 0.7810 - val_loss: 0.6033 - val_accuracy: 0.8444\n",
            "Epoch 259/300\n",
            "105/105 [==============================] - 0s 254us/sample - loss: 0.6051 - accuracy: 0.7810 - val_loss: 0.6024 - val_accuracy: 0.8444\n",
            "Epoch 260/300\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.6041 - accuracy: 0.7810 - val_loss: 0.6008 - val_accuracy: 0.8444\n",
            "Epoch 261/300\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.6034 - accuracy: 0.7810 - val_loss: 0.5977 - val_accuracy: 0.8444\n",
            "Epoch 262/300\n",
            "105/105 [==============================] - 0s 247us/sample - loss: 0.6022 - accuracy: 0.7810 - val_loss: 0.5973 - val_accuracy: 0.8667\n",
            "Epoch 263/300\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.6011 - accuracy: 0.7905 - val_loss: 0.5970 - val_accuracy: 0.8667\n",
            "Epoch 264/300\n",
            "105/105 [==============================] - 0s 300us/sample - loss: 0.6001 - accuracy: 0.7810 - val_loss: 0.5971 - val_accuracy: 0.8667\n",
            "Epoch 265/300\n",
            "105/105 [==============================] - 0s 237us/sample - loss: 0.5992 - accuracy: 0.7905 - val_loss: 0.5973 - val_accuracy: 0.8667\n",
            "Epoch 266/300\n",
            "105/105 [==============================] - 0s 261us/sample - loss: 0.5985 - accuracy: 0.7905 - val_loss: 0.5972 - val_accuracy: 0.8444\n",
            "Epoch 267/300\n",
            "105/105 [==============================] - 0s 249us/sample - loss: 0.5972 - accuracy: 0.7905 - val_loss: 0.5950 - val_accuracy: 0.8667\n",
            "Epoch 268/300\n",
            "105/105 [==============================] - 0s 268us/sample - loss: 0.5962 - accuracy: 0.7905 - val_loss: 0.5922 - val_accuracy: 0.8667\n",
            "Epoch 269/300\n",
            "105/105 [==============================] - 0s 340us/sample - loss: 0.5951 - accuracy: 0.7905 - val_loss: 0.5883 - val_accuracy: 0.8667\n",
            "Epoch 270/300\n",
            "105/105 [==============================] - 0s 286us/sample - loss: 0.5944 - accuracy: 0.8190 - val_loss: 0.5840 - val_accuracy: 0.8222\n",
            "Epoch 271/300\n",
            "105/105 [==============================] - 0s 280us/sample - loss: 0.5935 - accuracy: 0.8000 - val_loss: 0.5814 - val_accuracy: 0.7778\n",
            "Epoch 272/300\n",
            "105/105 [==============================] - 0s 306us/sample - loss: 0.5930 - accuracy: 0.7905 - val_loss: 0.5785 - val_accuracy: 0.8222\n",
            "Epoch 273/300\n",
            "105/105 [==============================] - 0s 259us/sample - loss: 0.5918 - accuracy: 0.7810 - val_loss: 0.5772 - val_accuracy: 0.8222\n",
            "Epoch 274/300\n",
            "105/105 [==============================] - 0s 264us/sample - loss: 0.5909 - accuracy: 0.7810 - val_loss: 0.5765 - val_accuracy: 0.8222\n",
            "Epoch 275/300\n",
            "105/105 [==============================] - 0s 288us/sample - loss: 0.5900 - accuracy: 0.7905 - val_loss: 0.5770 - val_accuracy: 0.8000\n",
            "Epoch 276/300\n",
            "105/105 [==============================] - 0s 255us/sample - loss: 0.5888 - accuracy: 0.8000 - val_loss: 0.5762 - val_accuracy: 0.8444\n",
            "Epoch 277/300\n",
            "105/105 [==============================] - 0s 281us/sample - loss: 0.5878 - accuracy: 0.8095 - val_loss: 0.5758 - val_accuracy: 0.8667\n",
            "Epoch 278/300\n",
            "105/105 [==============================] - 0s 252us/sample - loss: 0.5868 - accuracy: 0.8095 - val_loss: 0.5754 - val_accuracy: 0.8667\n",
            "Epoch 279/300\n",
            "105/105 [==============================] - 0s 269us/sample - loss: 0.5858 - accuracy: 0.8190 - val_loss: 0.5754 - val_accuracy: 0.8667\n",
            "Epoch 280/300\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.5851 - accuracy: 0.8286 - val_loss: 0.5768 - val_accuracy: 0.8889\n",
            "Epoch 281/300\n",
            "105/105 [==============================] - 0s 297us/sample - loss: 0.5840 - accuracy: 0.8286 - val_loss: 0.5766 - val_accuracy: 0.8889\n",
            "Epoch 282/300\n",
            "105/105 [==============================] - 0s 300us/sample - loss: 0.5831 - accuracy: 0.8286 - val_loss: 0.5757 - val_accuracy: 0.8889\n",
            "Epoch 283/300\n",
            "105/105 [==============================] - 0s 297us/sample - loss: 0.5821 - accuracy: 0.8286 - val_loss: 0.5744 - val_accuracy: 0.8889\n",
            "Epoch 284/300\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.5813 - accuracy: 0.8286 - val_loss: 0.5734 - val_accuracy: 0.8889\n",
            "Epoch 285/300\n",
            "105/105 [==============================] - 0s 243us/sample - loss: 0.5804 - accuracy: 0.8286 - val_loss: 0.5731 - val_accuracy: 0.8889\n",
            "Epoch 286/300\n",
            "105/105 [==============================] - 0s 277us/sample - loss: 0.5795 - accuracy: 0.8286 - val_loss: 0.5730 - val_accuracy: 0.9111\n",
            "Epoch 287/300\n",
            "105/105 [==============================] - 0s 328us/sample - loss: 0.5787 - accuracy: 0.8190 - val_loss: 0.5736 - val_accuracy: 0.9111\n",
            "Epoch 288/300\n",
            "105/105 [==============================] - 0s 257us/sample - loss: 0.5777 - accuracy: 0.8095 - val_loss: 0.5737 - val_accuracy: 0.9111\n",
            "Epoch 289/300\n",
            "105/105 [==============================] - 0s 238us/sample - loss: 0.5767 - accuracy: 0.8190 - val_loss: 0.5740 - val_accuracy: 0.9111\n",
            "Epoch 290/300\n",
            "105/105 [==============================] - 0s 245us/sample - loss: 0.5761 - accuracy: 0.8381 - val_loss: 0.5748 - val_accuracy: 0.8889\n",
            "Epoch 291/300\n",
            "105/105 [==============================] - 0s 228us/sample - loss: 0.5752 - accuracy: 0.8381 - val_loss: 0.5737 - val_accuracy: 0.8889\n",
            "Epoch 292/300\n",
            "105/105 [==============================] - 0s 248us/sample - loss: 0.5742 - accuracy: 0.8381 - val_loss: 0.5724 - val_accuracy: 0.8889\n",
            "Epoch 293/300\n",
            "105/105 [==============================] - 0s 300us/sample - loss: 0.5733 - accuracy: 0.8381 - val_loss: 0.5702 - val_accuracy: 0.9111\n",
            "Epoch 294/300\n",
            "105/105 [==============================] - 0s 278us/sample - loss: 0.5724 - accuracy: 0.8286 - val_loss: 0.5667 - val_accuracy: 0.9111\n",
            "Epoch 295/300\n",
            "105/105 [==============================] - 0s 354us/sample - loss: 0.5715 - accuracy: 0.8190 - val_loss: 0.5655 - val_accuracy: 0.9111\n",
            "Epoch 296/300\n",
            "105/105 [==============================] - 0s 267us/sample - loss: 0.5706 - accuracy: 0.8286 - val_loss: 0.5649 - val_accuracy: 0.9111\n",
            "Epoch 297/300\n",
            "105/105 [==============================] - 0s 263us/sample - loss: 0.5699 - accuracy: 0.8190 - val_loss: 0.5644 - val_accuracy: 0.9111\n",
            "Epoch 298/300\n",
            "105/105 [==============================] - 0s 283us/sample - loss: 0.5690 - accuracy: 0.8286 - val_loss: 0.5622 - val_accuracy: 0.9111\n",
            "Epoch 299/300\n",
            "105/105 [==============================] - 0s 354us/sample - loss: 0.5679 - accuracy: 0.8286 - val_loss: 0.5625 - val_accuracy: 0.9111\n",
            "Epoch 300/300\n",
            "105/105 [==============================] - 0s 397us/sample - loss: 0.5670 - accuracy: 0.8286 - val_loss: 0.5632 - val_accuracy: 0.9111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7HRpmiPc212",
        "colab_type": "code",
        "outputId": "d977253b-096f-4bdb-c745-4f1fd8913f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('Accuracy')\n",
        "plt.plot(hist.history['accuracy'],color='green')\n",
        "plt.plot(hist.history['val_accuracy'],color='black')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ4K576Ec3zU",
        "colab_type": "code",
        "outputId": "cc69d6b2-a290-44dc-c7b3-2381c2dce475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('Loss')\n",
        "plt.plot(hist.history['loss'],color='green')\n",
        "plt.plot(hist.history['val_loss'],color='black')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_EICg_NhxOx",
        "colab_type": "code",
        "outputId": "bbccbb35-ee0d-4746-d948-4a46f3fbc1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 6         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 9         \n",
            "=================================================================\n",
            "Total params: 25\n",
            "Trainable params: 25\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g3FVbJBKsrH",
        "colab_type": "code",
        "outputId": "1fea7b83-d0ef-4ded-ef10-a09298b80102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict(X[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.652969 , 0.2140051, 0.1330259]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_iUiLI25D95",
        "colab_type": "text"
      },
      "source": [
        "Parte 2: montando o modelo com base nos pesos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6p9IKxd6L5L",
        "colab_type": "text"
      },
      "source": [
        "Usaremos a função ativadora RELU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO0zdXb-6PRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(x):\n",
        "  if x<=0:return 0.0\n",
        "  else: return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU6zJWKAHWbS",
        "colab_type": "text"
      },
      "source": [
        "Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_6C81daHZ3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perceptron(inp,w,b,f):\n",
        "  return f(np.inner(inp,w)+b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dem_FN3hzyB",
        "colab_type": "code",
        "outputId": "ceb80093-b825-4b0e-b8f6-9666eb6c1cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.46019593, -0.77743536],\n",
              "        [-0.9362714 ,  0.5621408 ],\n",
              "        [ 0.34664682,  1.1545835 ],\n",
              "        [-0.15891035,  0.790548  ]], dtype=float32),\n",
              " array([-0.31881723, -0.07698157], dtype=float32),\n",
              " array([[ 0.5906803 ,  1.0090822 ],\n",
              "        [-0.00545696,  1.6309538 ]], dtype=float32),\n",
              " array([-0.38254857, -0.3704896 ], dtype=float32),\n",
              " array([[-0.1791789 , -0.40423286,  0.49170467],\n",
              "        [-1.229505  , -0.33195883, -0.298226  ]], dtype=float32),\n",
              " array([ 0.9382546 , -0.17727524, -0.6527312 ], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MP_AOzWh7YI",
        "colab_type": "code",
        "outputId": "2ce75eac-e0c9-4500-a514-61cef7200f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "list_w=model.get_weights()\n",
        "w1=list_w[0]\n",
        "w1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.46019593, -0.77743536],\n",
              "       [-0.9362714 ,  0.5621408 ],\n",
              "       [ 0.34664682,  1.1545835 ],\n",
              "       [-0.15891035,  0.790548  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRxBTcUuICTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b1=list_w[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjnzeu0c5xcJ",
        "colab_type": "code",
        "outputId": "5a41f346-f5f0-4773-a64f-cf387d6d7f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inpt=np.array([5.1,3.5,1.4,0.2])\n",
        "out11=perceptron(inpt,w1[:,0],b1[0],relu)\n",
        "out21=perceptron(inpt,w1[:,1],b1[1],relu)\n",
        "out11,out21"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.7952445209026338, -0.2998826026916501)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7H4pWP563Gf",
        "colab_type": "code",
        "outputId": "c3650e4a-c4a1-465d-eeec-6d551bfbca73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w2=list_w[2]\n",
        "b2=list_w[3]\n",
        "out1=np.array([out11,out21])\n",
        "out12=perceptron(out1,w2[:,0],b2[0],relu)\n",
        "out22=perceptron(out1,w2[:,1],b2[1],relu)\n",
        "out12,out22"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y7bSWbBFZo8",
        "colab_type": "code",
        "outputId": "71aef37d-e81f-4ca9-e74c-814157a885e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w3=list_w[4]\n",
        "b3=list_w[5]\n",
        "out2=np.array([out12,out22])\n",
        "out13=perceptron(out2,w3[:,0],b3[0],lambda x:x)\n",
        "out23=perceptron(out2,w3[:,1],b3[1],lambda x:x)\n",
        "out33=perceptron(out2,w3[:,2],b3[2],lambda x:x)\n",
        "out13,out23,out33\n",
        "out3=np.array([out13,out23,out33])\n",
        "out3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.93825459, -0.17727524, -0.65273118])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqIodrZQ8QNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def softmax(v):\n",
        "  le=[math.exp(i) for i in v]\n",
        "  soma=sum(le)\n",
        "  le=[v/soma for v in le]\n",
        "  return le"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb1hq0VbGLMu",
        "colab_type": "code",
        "outputId": "446771fb-233a-44b5-eca4-fd7b57c5cc0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "softmax(out3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6529690008031664, 0.214005094530291, 0.13302590466654263]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZXeZU2bsG7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layer(inp,w1,b1,f):\n",
        "  v=np.dot(w1.T,inp)+b1\n",
        "  for i in range(len(v)):\n",
        "    v[i]=f(v[i])\n",
        "  return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efVg_WUXsnq4",
        "colab_type": "code",
        "outputId": "f5133b1d-bd4b-4584-8098-06a7d480f74b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "softmax(layer(layer(layer(np.array([5.1,3.5,1.4,0.2]),w1,b1,relu),w2,b2,relu),w3,b3,lambda x:x))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6529690008031664, 0.214005094530291, 0.13302590466654263]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hNz12GItsWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}