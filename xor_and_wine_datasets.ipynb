{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xor-and-wine-datasets.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douglasbarbosadelima/Data-Science/blob/master/xor_and_wine_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwu85fc4vrxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_wine\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPP58cR8v9Gc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ac9a040-0c05-4ded-b2c9-6b1dfb413eba"
      },
      "source": [
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "model = MLPClassifier(learning_rate_init = 0.01, hidden_layer_sizes=(4), verbose=True, max_iter=2000, activation='relu')\n",
        "model.fit(X, y)\n",
        "\n",
        "for i in range(len(X)):\n",
        "  print(model.predict([X[i]]))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.72427803\n",
            "Iteration 2, loss = 0.71776799\n",
            "Iteration 3, loss = 0.71159816\n",
            "Iteration 4, loss = 0.70625784\n",
            "Iteration 5, loss = 0.70200469\n",
            "Iteration 6, loss = 0.69786078\n",
            "Iteration 7, loss = 0.69466895\n",
            "Iteration 8, loss = 0.69144231\n",
            "Iteration 9, loss = 0.68813309\n",
            "Iteration 10, loss = 0.68477355\n",
            "Iteration 11, loss = 0.68138541\n",
            "Iteration 12, loss = 0.67798271\n",
            "Iteration 13, loss = 0.67457446\n",
            "Iteration 14, loss = 0.67116627\n",
            "Iteration 15, loss = 0.66776125\n",
            "Iteration 16, loss = 0.66436060\n",
            "Iteration 17, loss = 0.66102958\n",
            "Iteration 18, loss = 0.65932421\n",
            "Iteration 19, loss = 0.65760017\n",
            "Iteration 20, loss = 0.65585812\n",
            "Iteration 21, loss = 0.65409848\n",
            "Iteration 22, loss = 0.65232152\n",
            "Iteration 23, loss = 0.65052743\n",
            "Iteration 24, loss = 0.64871638\n",
            "Iteration 25, loss = 0.64865184\n",
            "Iteration 26, loss = 0.64796255\n",
            "Iteration 27, loss = 0.64829616\n",
            "Iteration 28, loss = 0.64721942\n",
            "Iteration 29, loss = 0.64588892\n",
            "Iteration 30, loss = 0.64457088\n",
            "Iteration 31, loss = 0.64358656\n",
            "Iteration 32, loss = 0.64239620\n",
            "Iteration 33, loss = 0.64104532\n",
            "Iteration 34, loss = 0.63956279\n",
            "Iteration 35, loss = 0.63839473\n",
            "Iteration 36, loss = 0.63707139\n",
            "Iteration 37, loss = 0.63550272\n",
            "Iteration 38, loss = 0.63393139\n",
            "Iteration 39, loss = 0.63253612\n",
            "Iteration 40, loss = 0.63100405\n",
            "Iteration 41, loss = 0.62972299\n",
            "Iteration 42, loss = 0.62879830\n",
            "Iteration 43, loss = 0.62766333\n",
            "Iteration 44, loss = 0.62624034\n",
            "Iteration 45, loss = 0.62471628\n",
            "Iteration 46, loss = 0.62369131\n",
            "Iteration 47, loss = 0.62250273\n",
            "Iteration 48, loss = 0.62117177\n",
            "Iteration 49, loss = 0.61971672\n",
            "Iteration 50, loss = 0.61856068\n",
            "Iteration 51, loss = 0.61732529\n",
            "Iteration 52, loss = 0.61589657\n",
            "Iteration 53, loss = 0.61429495\n",
            "Iteration 54, loss = 0.61317506\n",
            "Iteration 55, loss = 0.61214981\n",
            "Iteration 56, loss = 0.61086584\n",
            "Iteration 57, loss = 0.60980104\n",
            "Iteration 58, loss = 0.60861126\n",
            "Iteration 59, loss = 0.60727058\n",
            "Iteration 60, loss = 0.60579920\n",
            "Iteration 61, loss = 0.60421544\n",
            "Iteration 62, loss = 0.60306771\n",
            "Iteration 63, loss = 0.60203104\n",
            "Iteration 64, loss = 0.60010217\n",
            "Iteration 65, loss = 0.59900101\n",
            "Iteration 66, loss = 0.59788782\n",
            "Iteration 67, loss = 0.59663728\n",
            "Iteration 68, loss = 0.59526703\n",
            "Iteration 69, loss = 0.59379366\n",
            "Iteration 70, loss = 0.59248725\n",
            "Iteration 71, loss = 0.59129497\n",
            "Iteration 72, loss = 0.58999360\n",
            "Iteration 73, loss = 0.58858521\n",
            "Iteration 74, loss = 0.58767304\n",
            "Iteration 75, loss = 0.58625084\n",
            "Iteration 76, loss = 0.58509850\n",
            "Iteration 77, loss = 0.58403091\n",
            "Iteration 78, loss = 0.58284417\n",
            "Iteration 79, loss = 0.58155302\n",
            "Iteration 80, loss = 0.58017099\n",
            "Iteration 81, loss = 0.57871052\n",
            "Iteration 82, loss = 0.57791673\n",
            "Iteration 83, loss = 0.57703107\n",
            "Iteration 84, loss = 0.57520840\n",
            "Iteration 85, loss = 0.57410893\n",
            "Iteration 86, loss = 0.57317762\n",
            "Iteration 87, loss = 0.57213294\n",
            "Iteration 88, loss = 0.57098831\n",
            "Iteration 89, loss = 0.56975629\n",
            "Iteration 90, loss = 0.56844857\n",
            "Iteration 91, loss = 0.56707602\n",
            "Iteration 92, loss = 0.56646767\n",
            "Iteration 93, loss = 0.56530715\n",
            "Iteration 94, loss = 0.56363592\n",
            "Iteration 95, loss = 0.56242483\n",
            "Iteration 96, loss = 0.56160775\n",
            "Iteration 97, loss = 0.56070476\n",
            "Iteration 98, loss = 0.55970377\n",
            "Iteration 99, loss = 0.55861724\n",
            "Iteration 100, loss = 0.55749853\n",
            "Iteration 101, loss = 0.55646414\n",
            "Iteration 102, loss = 0.55539120\n",
            "Iteration 103, loss = 0.55446865\n",
            "Iteration 104, loss = 0.55327847\n",
            "Iteration 105, loss = 0.55223156\n",
            "Iteration 106, loss = 0.55111947\n",
            "Iteration 107, loss = 0.55023406\n",
            "Iteration 108, loss = 0.54896543\n",
            "Iteration 109, loss = 0.54791404\n",
            "Iteration 110, loss = 0.54688403\n",
            "Iteration 111, loss = 0.54587956\n",
            "Iteration 112, loss = 0.54530226\n",
            "Iteration 113, loss = 0.54422599\n",
            "Iteration 114, loss = 0.54339082\n",
            "Iteration 115, loss = 0.54263153\n",
            "Iteration 116, loss = 0.54179641\n",
            "Iteration 117, loss = 0.54089564\n",
            "Iteration 118, loss = 0.53993876\n",
            "Iteration 119, loss = 0.53893460\n",
            "Iteration 120, loss = 0.53789129\n",
            "Iteration 121, loss = 0.53730910\n",
            "Iteration 122, loss = 0.53645491\n",
            "Iteration 123, loss = 0.53521528\n",
            "Iteration 124, loss = 0.53443725\n",
            "Iteration 125, loss = 0.53391623\n",
            "Iteration 126, loss = 0.53294055\n",
            "Iteration 127, loss = 0.53221312\n",
            "Iteration 128, loss = 0.53143225\n",
            "Iteration 129, loss = 0.53060596\n",
            "Iteration 130, loss = 0.52982557\n",
            "Iteration 131, loss = 0.52905187\n",
            "Iteration 132, loss = 0.52831280\n",
            "Iteration 133, loss = 0.52783129\n",
            "Iteration 134, loss = 0.52691890\n",
            "Iteration 135, loss = 0.52625372\n",
            "Iteration 136, loss = 0.52554405\n",
            "Iteration 137, loss = 0.52493322\n",
            "Iteration 138, loss = 0.52421640\n",
            "Iteration 139, loss = 0.52358758\n",
            "Iteration 140, loss = 0.52291752\n",
            "Iteration 141, loss = 0.52221279\n",
            "Iteration 142, loss = 0.52193306\n",
            "Iteration 143, loss = 0.52092614\n",
            "Iteration 144, loss = 0.52049995\n",
            "Iteration 145, loss = 0.52002864\n",
            "Iteration 146, loss = 0.51950846\n",
            "Iteration 147, loss = 0.51894614\n",
            "Iteration 148, loss = 0.51834797\n",
            "Iteration 149, loss = 0.51784606\n",
            "Iteration 150, loss = 0.51725163\n",
            "Iteration 151, loss = 0.51674199\n",
            "Iteration 152, loss = 0.51619689\n",
            "Iteration 153, loss = 0.51562189\n",
            "Iteration 154, loss = 0.51508096\n",
            "Iteration 155, loss = 0.51458117\n",
            "Iteration 156, loss = 0.51410333\n",
            "Iteration 157, loss = 0.51359406\n",
            "Iteration 158, loss = 0.51305840\n",
            "Iteration 159, loss = 0.51250097\n",
            "Iteration 160, loss = 0.51192598\n",
            "Iteration 161, loss = 0.51133723\n",
            "Iteration 162, loss = 0.51128251\n",
            "Iteration 163, loss = 0.51063654\n",
            "Iteration 164, loss = 0.51016616\n",
            "Iteration 165, loss = 0.50997702\n",
            "Iteration 166, loss = 0.50973909\n",
            "Iteration 167, loss = 0.50945772\n",
            "Iteration 168, loss = 0.50913803\n",
            "Iteration 169, loss = 0.50878485\n",
            "Iteration 170, loss = 0.50840274\n",
            "Iteration 171, loss = 0.50799596\n",
            "Iteration 172, loss = 0.50756848\n",
            "Iteration 173, loss = 0.50712393\n",
            "Iteration 174, loss = 0.50666565\n",
            "Iteration 175, loss = 0.50619662\n",
            "Iteration 176, loss = 0.50571956\n",
            "Iteration 177, loss = 0.50524671\n",
            "Iteration 178, loss = 0.50489970\n",
            "Iteration 179, loss = 0.50454297\n",
            "Iteration 180, loss = 0.50416987\n",
            "Iteration 181, loss = 0.50378330\n",
            "Iteration 182, loss = 0.50338591\n",
            "Iteration 183, loss = 0.50298008\n",
            "Iteration 184, loss = 0.50268279\n",
            "Iteration 185, loss = 0.50229228\n",
            "Iteration 186, loss = 0.50199739\n",
            "Iteration 187, loss = 0.50168619\n",
            "Iteration 188, loss = 0.50178828\n",
            "Iteration 189, loss = 0.50116221\n",
            "Iteration 190, loss = 0.50093855\n",
            "Iteration 191, loss = 0.50069352\n",
            "Iteration 192, loss = 0.50043000\n",
            "Iteration 193, loss = 0.50015070\n",
            "Iteration 194, loss = 0.49985810\n",
            "Iteration 195, loss = 0.49955450\n",
            "Iteration 196, loss = 0.49924741\n",
            "Iteration 197, loss = 0.49905268\n",
            "Iteration 198, loss = 0.49884306\n",
            "Iteration 199, loss = 0.49861583\n",
            "Iteration 200, loss = 0.49837352\n",
            "Iteration 201, loss = 0.49811846\n",
            "Iteration 202, loss = 0.49785277\n",
            "Iteration 203, loss = 0.49757841\n",
            "Iteration 204, loss = 0.49729713\n",
            "Iteration 205, loss = 0.49701052\n",
            "Iteration 206, loss = 0.49730198\n",
            "Iteration 207, loss = 0.49654894\n",
            "Iteration 208, loss = 0.49636210\n",
            "Iteration 209, loss = 0.49616166\n",
            "Iteration 210, loss = 0.49594961\n",
            "Iteration 211, loss = 0.49572779\n",
            "Iteration 212, loss = 0.49583909\n",
            "Iteration 213, loss = 0.49537846\n",
            "Iteration 214, loss = 0.49524066\n",
            "Iteration 215, loss = 0.49508670\n",
            "Iteration 216, loss = 0.49491868\n",
            "Iteration 217, loss = 0.49473851\n",
            "Iteration 218, loss = 0.49454799\n",
            "Iteration 219, loss = 0.49434873\n",
            "Iteration 220, loss = 0.49414224\n",
            "Iteration 221, loss = 0.49392987\n",
            "Iteration 222, loss = 0.49371284\n",
            "Iteration 223, loss = 0.49355316\n",
            "Iteration 224, loss = 0.49337867\n",
            "Iteration 225, loss = 0.49325084\n",
            "Iteration 226, loss = 0.49311059\n",
            "Iteration 227, loss = 0.49295958\n",
            "Iteration 228, loss = 0.49279935\n",
            "Iteration 229, loss = 0.49263131\n",
            "Iteration 230, loss = 0.49264428\n",
            "Iteration 231, loss = 0.49238211\n",
            "Iteration 232, loss = 0.49229147\n",
            "Iteration 233, loss = 0.49218665\n",
            "Iteration 234, loss = 0.49206938\n",
            "Iteration 235, loss = 0.49194125\n",
            "Iteration 236, loss = 0.49180374\n",
            "Iteration 237, loss = 0.49165820\n",
            "Iteration 238, loss = 0.49150587\n",
            "Iteration 239, loss = 0.49134788\n",
            "Iteration 240, loss = 0.49118526\n",
            "Iteration 241, loss = 0.49101893\n",
            "Iteration 242, loss = 0.49084972\n",
            "Iteration 243, loss = 0.49067836\n",
            "Iteration 244, loss = 0.49050552\n",
            "Iteration 245, loss = 0.49033178\n",
            "Iteration 246, loss = 0.49054347\n",
            "Iteration 247, loss = 0.49017102\n",
            "Iteration 248, loss = 0.49016515\n",
            "Iteration 249, loss = 0.49014181\n",
            "Iteration 250, loss = 0.49010268\n",
            "Iteration 251, loss = 0.49004936\n",
            "Iteration 252, loss = 0.48998336\n",
            "Iteration 253, loss = 0.48990608\n",
            "Iteration 254, loss = 0.48981885\n",
            "Iteration 255, loss = 0.48972292\n",
            "Iteration 256, loss = 0.48961944\n",
            "Iteration 257, loss = 0.48950945\n",
            "Iteration 258, loss = 0.48939394\n",
            "Iteration 259, loss = 0.48927378\n",
            "Iteration 260, loss = 0.48914976\n",
            "Iteration 261, loss = 0.48902261\n",
            "Iteration 262, loss = 0.48889295\n",
            "Iteration 263, loss = 0.48876137\n",
            "Iteration 264, loss = 0.48862836\n",
            "Iteration 265, loss = 0.48849438\n",
            "Iteration 266, loss = 0.48835981\n",
            "Iteration 267, loss = 0.48822503\n",
            "Iteration 268, loss = 0.48809033\n",
            "Iteration 269, loss = 0.48795601\n",
            "Iteration 270, loss = 0.48782230\n",
            "Iteration 271, loss = 0.48768943\n",
            "Iteration 272, loss = 0.48774225\n",
            "Iteration 273, loss = 0.48750806\n",
            "Iteration 274, loss = 0.48745078\n",
            "Iteration 275, loss = 0.48738665\n",
            "Iteration 276, loss = 0.48731650\n",
            "Iteration 277, loss = 0.48724110\n",
            "Iteration 278, loss = 0.48720953\n",
            "Iteration 279, loss = 0.48715602\n",
            "Iteration 280, loss = 0.48713927\n",
            "Iteration 281, loss = 0.48711200\n",
            "Iteration 282, loss = 0.48707528\n",
            "Iteration 283, loss = 0.48703011\n",
            "Iteration 284, loss = 0.48697744\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[0]\n",
            "[1]\n",
            "[1]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im7fp4WC1DHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60283074-e67b-4fe8-9f60-1e3dfbef17f7"
      },
      "source": [
        "dataset_wine = load_wine(return_X_y=True)\n",
        "dataset_wine, load_wine().target_names\n",
        "\n",
        "X_wine, y_wine = dataset_wine\n",
        "\n",
        "model_wine = MLPClassifier(hidden_layer_sizes=(X_wine.shape[0] + 1), activation='logistic', learning_rate_init=0.01, verbose=True, max_iter=2000)\n",
        "model_wine.fit(X_wine, y_wine)\n",
        "\n",
        "ye = model_wine.predict(X_wine)\n",
        "ye, accuracy_score(y_wine, ye)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.08616656\n",
            "Iteration 2, loss = 1.33519543\n",
            "Iteration 3, loss = 1.06930398\n",
            "Iteration 4, loss = 1.12788166\n",
            "Iteration 5, loss = 1.11986898\n",
            "Iteration 6, loss = 1.03455377\n",
            "Iteration 7, loss = 0.95103037\n",
            "Iteration 8, loss = 0.96027668\n",
            "Iteration 9, loss = 0.97868372\n",
            "Iteration 10, loss = 0.91130735\n",
            "Iteration 11, loss = 0.87732164\n",
            "Iteration 12, loss = 0.82662053\n",
            "Iteration 13, loss = 0.80329662\n",
            "Iteration 14, loss = 0.82255259\n",
            "Iteration 15, loss = 0.78579428\n",
            "Iteration 16, loss = 0.76317271\n",
            "Iteration 17, loss = 0.73979191\n",
            "Iteration 18, loss = 0.72308022\n",
            "Iteration 19, loss = 0.69672647\n",
            "Iteration 20, loss = 0.68259436\n",
            "Iteration 21, loss = 0.66022094\n",
            "Iteration 22, loss = 0.64323720\n",
            "Iteration 23, loss = 0.62521499\n",
            "Iteration 24, loss = 0.62925950\n",
            "Iteration 25, loss = 0.62670079\n",
            "Iteration 26, loss = 0.59095226\n",
            "Iteration 27, loss = 0.57251750\n",
            "Iteration 28, loss = 0.56303667\n",
            "Iteration 29, loss = 0.56351689\n",
            "Iteration 30, loss = 0.57730721\n",
            "Iteration 31, loss = 0.53150231\n",
            "Iteration 32, loss = 0.56309567\n",
            "Iteration 33, loss = 0.54860305\n",
            "Iteration 34, loss = 0.51692873\n",
            "Iteration 35, loss = 0.53176019\n",
            "Iteration 36, loss = 0.49097819\n",
            "Iteration 37, loss = 0.50117882\n",
            "Iteration 38, loss = 0.47316194\n",
            "Iteration 39, loss = 0.46940463\n",
            "Iteration 40, loss = 0.46108615\n",
            "Iteration 41, loss = 0.44407292\n",
            "Iteration 42, loss = 0.43437221\n",
            "Iteration 43, loss = 0.42758456\n",
            "Iteration 44, loss = 0.40686266\n",
            "Iteration 45, loss = 0.39131881\n",
            "Iteration 46, loss = 0.37620190\n",
            "Iteration 47, loss = 0.36416956\n",
            "Iteration 48, loss = 0.35681070\n",
            "Iteration 49, loss = 0.38349709\n",
            "Iteration 50, loss = 0.39865647\n",
            "Iteration 51, loss = 0.35908891\n",
            "Iteration 52, loss = 0.33140286\n",
            "Iteration 53, loss = 0.35343833\n",
            "Iteration 54, loss = 0.30214993\n",
            "Iteration 55, loss = 0.29768352\n",
            "Iteration 56, loss = 0.32876876\n",
            "Iteration 57, loss = 0.30602413\n",
            "Iteration 58, loss = 0.27344812\n",
            "Iteration 59, loss = 0.29604506\n",
            "Iteration 60, loss = 0.25967717\n",
            "Iteration 61, loss = 0.24667765\n",
            "Iteration 62, loss = 0.24701279\n",
            "Iteration 63, loss = 0.23961987\n",
            "Iteration 64, loss = 0.23618502\n",
            "Iteration 65, loss = 0.22295893\n",
            "Iteration 66, loss = 0.20469206\n",
            "Iteration 67, loss = 0.20670954\n",
            "Iteration 68, loss = 0.20992848\n",
            "Iteration 69, loss = 0.19682256\n",
            "Iteration 70, loss = 0.18475342\n",
            "Iteration 71, loss = 0.18745156\n",
            "Iteration 72, loss = 0.18376878\n",
            "Iteration 73, loss = 0.17267705\n",
            "Iteration 74, loss = 0.16902171\n",
            "Iteration 75, loss = 0.17053531\n",
            "Iteration 76, loss = 0.16233898\n",
            "Iteration 77, loss = 0.15433738\n",
            "Iteration 78, loss = 0.15270263\n",
            "Iteration 79, loss = 0.15524871\n",
            "Iteration 80, loss = 0.15494400\n",
            "Iteration 81, loss = 0.14712997\n",
            "Iteration 82, loss = 0.13934353\n",
            "Iteration 83, loss = 0.14207127\n",
            "Iteration 84, loss = 0.14183082\n",
            "Iteration 85, loss = 0.13657914\n",
            "Iteration 86, loss = 0.12982397\n",
            "Iteration 87, loss = 0.12786790\n",
            "Iteration 88, loss = 0.12947320\n",
            "Iteration 89, loss = 0.12953434\n",
            "Iteration 90, loss = 0.12898838\n",
            "Iteration 91, loss = 0.12304725\n",
            "Iteration 92, loss = 0.11931180\n",
            "Iteration 93, loss = 0.11687121\n",
            "Iteration 94, loss = 0.11976100\n",
            "Iteration 95, loss = 0.12435242\n",
            "Iteration 96, loss = 0.12793021\n",
            "Iteration 97, loss = 0.13135106\n",
            "Iteration 98, loss = 0.12139077\n",
            "Iteration 99, loss = 0.11476319\n",
            "Iteration 100, loss = 0.11037288\n",
            "Iteration 101, loss = 0.11934302\n",
            "Iteration 102, loss = 0.11905302\n",
            "Iteration 103, loss = 0.11569198\n",
            "Iteration 104, loss = 0.10604160\n",
            "Iteration 105, loss = 0.10334789\n",
            "Iteration 106, loss = 0.10276851\n",
            "Iteration 107, loss = 0.10347849\n",
            "Iteration 108, loss = 0.10526221\n",
            "Iteration 109, loss = 0.10378117\n",
            "Iteration 110, loss = 0.10370437\n",
            "Iteration 111, loss = 0.10144338\n",
            "Iteration 112, loss = 0.10050844\n",
            "Iteration 113, loss = 0.09599211\n",
            "Iteration 114, loss = 0.09461330\n",
            "Iteration 115, loss = 0.09424098\n",
            "Iteration 116, loss = 0.09411387\n",
            "Iteration 117, loss = 0.09400437\n",
            "Iteration 118, loss = 0.09301856\n",
            "Iteration 119, loss = 0.09388844\n",
            "Iteration 120, loss = 0.09432220\n",
            "Iteration 121, loss = 0.09474653\n",
            "Iteration 122, loss = 0.09426442\n",
            "Iteration 123, loss = 0.09226010\n",
            "Iteration 124, loss = 0.08856892\n",
            "Iteration 125, loss = 0.08594987\n",
            "Iteration 126, loss = 0.08358718\n",
            "Iteration 127, loss = 0.08380659\n",
            "Iteration 128, loss = 0.08515543\n",
            "Iteration 129, loss = 0.08949928\n",
            "Iteration 130, loss = 0.09617128\n",
            "Iteration 131, loss = 0.10575881\n",
            "Iteration 132, loss = 0.09805142\n",
            "Iteration 133, loss = 0.08345523\n",
            "Iteration 134, loss = 0.08197180\n",
            "Iteration 135, loss = 0.09371865\n",
            "Iteration 136, loss = 0.10654935\n",
            "Iteration 137, loss = 0.10262845\n",
            "Iteration 138, loss = 0.09040231\n",
            "Iteration 139, loss = 0.07772172\n",
            "Iteration 140, loss = 0.08751792\n",
            "Iteration 141, loss = 0.10076788\n",
            "Iteration 142, loss = 0.09992283\n",
            "Iteration 143, loss = 0.08548182\n",
            "Iteration 144, loss = 0.07825038\n",
            "Iteration 145, loss = 0.08045084\n",
            "Iteration 146, loss = 0.08532464\n",
            "Iteration 147, loss = 0.08368587\n",
            "Iteration 148, loss = 0.07879513\n",
            "Iteration 149, loss = 0.07509319\n",
            "Iteration 150, loss = 0.07169539\n",
            "Iteration 151, loss = 0.07245752\n",
            "Iteration 152, loss = 0.07599290\n",
            "Iteration 153, loss = 0.07801767\n",
            "Iteration 154, loss = 0.07584990\n",
            "Iteration 155, loss = 0.07114187\n",
            "Iteration 156, loss = 0.06969908\n",
            "Iteration 157, loss = 0.07043432\n",
            "Iteration 158, loss = 0.07189034\n",
            "Iteration 159, loss = 0.07167624\n",
            "Iteration 160, loss = 0.06893641\n",
            "Iteration 161, loss = 0.06635205\n",
            "Iteration 162, loss = 0.06679738\n",
            "Iteration 163, loss = 0.06926345\n",
            "Iteration 164, loss = 0.07066066\n",
            "Iteration 165, loss = 0.07045414\n",
            "Iteration 166, loss = 0.07026153\n",
            "Iteration 167, loss = 0.06710730\n",
            "Iteration 168, loss = 0.06417806\n",
            "Iteration 169, loss = 0.06515735\n",
            "Iteration 170, loss = 0.06713898\n",
            "Iteration 171, loss = 0.06865600\n",
            "Iteration 172, loss = 0.06751996\n",
            "Iteration 173, loss = 0.06541966\n",
            "Iteration 174, loss = 0.06261233\n",
            "Iteration 175, loss = 0.06092103\n",
            "Iteration 176, loss = 0.06114011\n",
            "Iteration 177, loss = 0.06251088\n",
            "Iteration 178, loss = 0.06415028\n",
            "Iteration 179, loss = 0.06501390\n",
            "Iteration 180, loss = 0.06488290\n",
            "Iteration 181, loss = 0.06299727\n",
            "Iteration 182, loss = 0.06007085\n",
            "Iteration 183, loss = 0.05824740\n",
            "Iteration 184, loss = 0.05837325\n",
            "Iteration 185, loss = 0.06011896\n",
            "Iteration 186, loss = 0.06273904\n",
            "Iteration 187, loss = 0.06660843\n",
            "Iteration 188, loss = 0.07090230\n",
            "Iteration 189, loss = 0.07036115\n",
            "Iteration 190, loss = 0.06272312\n",
            "Iteration 191, loss = 0.05699939\n",
            "Iteration 192, loss = 0.06098710\n",
            "Iteration 193, loss = 0.06719654\n",
            "Iteration 194, loss = 0.07301734\n",
            "Iteration 195, loss = 0.07861256\n",
            "Iteration 196, loss = 0.07486125\n",
            "Iteration 197, loss = 0.06058403\n",
            "Iteration 198, loss = 0.05745976\n",
            "Iteration 199, loss = 0.06747011\n",
            "Iteration 200, loss = 0.06762624\n",
            "Iteration 201, loss = 0.05968301\n",
            "Iteration 202, loss = 0.05345686\n",
            "Iteration 203, loss = 0.05539960\n",
            "Iteration 204, loss = 0.06209989\n",
            "Iteration 205, loss = 0.06923049\n",
            "Iteration 206, loss = 0.07015678\n",
            "Iteration 207, loss = 0.05915028\n",
            "Iteration 208, loss = 0.05401937\n",
            "Iteration 209, loss = 0.06021652\n",
            "Iteration 210, loss = 0.05768537\n",
            "Iteration 211, loss = 0.05228858\n",
            "Iteration 212, loss = 0.05135533\n",
            "Iteration 213, loss = 0.05548265\n",
            "Iteration 214, loss = 0.06334452\n",
            "Iteration 215, loss = 0.06832872\n",
            "Iteration 216, loss = 0.06462813\n",
            "Iteration 217, loss = 0.05274379\n",
            "Iteration 218, loss = 0.05756577\n",
            "Iteration 219, loss = 0.06322710\n",
            "Iteration 220, loss = 0.05398015\n",
            "Iteration 221, loss = 0.04897310\n",
            "Iteration 222, loss = 0.05512940\n",
            "Iteration 223, loss = 0.06656814\n",
            "Iteration 224, loss = 0.07692609\n",
            "Iteration 225, loss = 0.07156500\n",
            "Iteration 226, loss = 0.05554719\n",
            "Iteration 227, loss = 0.05969960\n",
            "Iteration 228, loss = 0.06774965\n",
            "Iteration 229, loss = 0.05739190\n",
            "Iteration 230, loss = 0.04976406\n",
            "Iteration 231, loss = 0.05996097\n",
            "Iteration 232, loss = 0.06577496\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2]), 0.9775280898876404)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JilSN-5x3CXd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e5850190-1137-4bbc-d11f-93d5a59a1a62"
      },
      "source": [
        "plt.plot(y_wine, 'bo')\n",
        "plt.plot(ye, 'ro')\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j8rwrx13cRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}