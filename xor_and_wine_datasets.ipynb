{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xor-and-wine-datasets.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douglasbarbosadelima/Data-Science/blob/master/xor_and_wine_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwu85fc4vrxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_wine\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPP58cR8v9Gc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff40e557-b73e-433b-eb89-cce530047a3b"
      },
      "source": [
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "model = MLPClassifier(learning_rate_init = 0.01, hidden_layer_sizes=(4), verbose=True, max_iter=2000, activation='relu')\n",
        "model.fit(X, y)\n",
        "\n",
        "for i in range(len(X)):\n",
        "  print(model.predict([X[i]]))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.19173978\n",
            "Iteration 2, loss = 1.16867951\n",
            "Iteration 3, loss = 1.14628304\n",
            "Iteration 4, loss = 1.12456067\n",
            "Iteration 5, loss = 1.10352105\n",
            "Iteration 6, loss = 1.08317114\n",
            "Iteration 7, loss = 1.06351603\n",
            "Iteration 8, loss = 1.04455895\n",
            "Iteration 9, loss = 1.02706908\n",
            "Iteration 10, loss = 1.01047634\n",
            "Iteration 11, loss = 0.99426953\n",
            "Iteration 12, loss = 0.97859100\n",
            "Iteration 13, loss = 0.96350662\n",
            "Iteration 14, loss = 0.94904230\n",
            "Iteration 15, loss = 0.93519698\n",
            "Iteration 16, loss = 0.92195154\n",
            "Iteration 17, loss = 0.90927837\n",
            "Iteration 18, loss = 0.89605088\n",
            "Iteration 19, loss = 0.88314775\n",
            "Iteration 20, loss = 0.87064814\n",
            "Iteration 21, loss = 0.85855148\n",
            "Iteration 22, loss = 0.84685787\n",
            "Iteration 23, loss = 0.83556723\n",
            "Iteration 24, loss = 0.82467887\n",
            "Iteration 25, loss = 0.81419135\n",
            "Iteration 26, loss = 0.80410255\n",
            "Iteration 27, loss = 0.79440968\n",
            "Iteration 28, loss = 0.78510944\n",
            "Iteration 29, loss = 0.77619810\n",
            "Iteration 30, loss = 0.76767156\n",
            "Iteration 31, loss = 0.75952546\n",
            "Iteration 32, loss = 0.75175518\n",
            "Iteration 33, loss = 0.74435587\n",
            "Iteration 34, loss = 0.73732242\n",
            "Iteration 35, loss = 0.73064944\n",
            "Iteration 36, loss = 0.72479007\n",
            "Iteration 37, loss = 0.71966155\n",
            "Iteration 38, loss = 0.71497470\n",
            "Iteration 39, loss = 0.71071698\n",
            "Iteration 40, loss = 0.70687365\n",
            "Iteration 41, loss = 0.70342756\n",
            "Iteration 42, loss = 0.70035890\n",
            "Iteration 43, loss = 0.69764502\n",
            "Iteration 44, loss = 0.69526036\n",
            "Iteration 45, loss = 0.69317655\n",
            "Iteration 46, loss = 0.69136265\n",
            "Iteration 47, loss = 0.68978560\n",
            "Iteration 48, loss = 0.68841095\n",
            "Iteration 49, loss = 0.68720376\n",
            "Iteration 50, loss = 0.68612963\n",
            "Iteration 51, loss = 0.68515578\n",
            "Iteration 52, loss = 0.68425215\n",
            "Iteration 53, loss = 0.68339225\n",
            "Iteration 54, loss = 0.68255377\n",
            "Iteration 55, loss = 0.68171890\n",
            "Iteration 56, loss = 0.68175865\n",
            "Iteration 57, loss = 0.68180612\n",
            "Iteration 58, loss = 0.68165451\n",
            "Iteration 59, loss = 0.68134057\n",
            "Iteration 60, loss = 0.68090206\n",
            "Iteration 61, loss = 0.68035256\n",
            "Iteration 62, loss = 0.67970546\n",
            "Iteration 63, loss = 0.67897349\n",
            "Iteration 64, loss = 0.67816851\n",
            "Iteration 65, loss = 0.67730124\n",
            "Iteration 66, loss = 0.67638113\n",
            "Iteration 67, loss = 0.67541635\n",
            "Iteration 68, loss = 0.67441374\n",
            "Iteration 69, loss = 0.67337887\n",
            "Iteration 70, loss = 0.67231613\n",
            "Iteration 71, loss = 0.67122883\n",
            "Iteration 72, loss = 0.67011932\n",
            "Iteration 73, loss = 0.66898915\n",
            "Iteration 74, loss = 0.66821413\n",
            "Iteration 75, loss = 0.66768787\n",
            "Iteration 76, loss = 0.66709647\n",
            "Iteration 77, loss = 0.66644055\n",
            "Iteration 78, loss = 0.66572104\n",
            "Iteration 79, loss = 0.66493929\n",
            "Iteration 80, loss = 0.66409703\n",
            "Iteration 81, loss = 0.66319638\n",
            "Iteration 82, loss = 0.66223984\n",
            "Iteration 83, loss = 0.66137813\n",
            "Iteration 84, loss = 0.66055309\n",
            "Iteration 85, loss = 0.65966383\n",
            "Iteration 86, loss = 0.65914650\n",
            "Iteration 87, loss = 0.65851995\n",
            "Iteration 88, loss = 0.65779194\n",
            "Iteration 89, loss = 0.65696951\n",
            "Iteration 90, loss = 0.65605904\n",
            "Iteration 91, loss = 0.65510600\n",
            "Iteration 92, loss = 0.65432508\n",
            "Iteration 93, loss = 0.65345326\n",
            "Iteration 94, loss = 0.65249651\n",
            "Iteration 95, loss = 0.65153894\n",
            "Iteration 96, loss = 0.65061031\n",
            "Iteration 97, loss = 0.64971850\n",
            "Iteration 98, loss = 0.64892388\n",
            "Iteration 99, loss = 0.64807205\n",
            "Iteration 100, loss = 0.64711546\n",
            "Iteration 101, loss = 0.64611137\n",
            "Iteration 102, loss = 0.64513263\n",
            "Iteration 103, loss = 0.64413000\n",
            "Iteration 104, loss = 0.64317315\n",
            "Iteration 105, loss = 0.64225469\n",
            "Iteration 106, loss = 0.64131790\n",
            "Iteration 107, loss = 0.64036298\n",
            "Iteration 108, loss = 0.63933513\n",
            "Iteration 109, loss = 0.63824983\n",
            "Iteration 110, loss = 0.63722504\n",
            "Iteration 111, loss = 0.63615143\n",
            "Iteration 112, loss = 0.63505556\n",
            "Iteration 113, loss = 0.63403680\n",
            "Iteration 114, loss = 0.63291434\n",
            "Iteration 115, loss = 0.63169338\n",
            "Iteration 116, loss = 0.63053405\n",
            "Iteration 117, loss = 0.62934616\n",
            "Iteration 118, loss = 0.62803793\n",
            "Iteration 119, loss = 0.62677620\n",
            "Iteration 120, loss = 0.62555063\n",
            "Iteration 121, loss = 0.62429757\n",
            "Iteration 122, loss = 0.62300076\n",
            "Iteration 123, loss = 0.62171640\n",
            "Iteration 124, loss = 0.62037458\n",
            "Iteration 125, loss = 0.61895132\n",
            "Iteration 126, loss = 0.61758114\n",
            "Iteration 127, loss = 0.61617812\n",
            "Iteration 128, loss = 0.61475130\n",
            "Iteration 129, loss = 0.61327438\n",
            "Iteration 130, loss = 0.61172383\n",
            "Iteration 131, loss = 0.61044087\n",
            "Iteration 132, loss = 0.60895300\n",
            "Iteration 133, loss = 0.60727286\n",
            "Iteration 134, loss = 0.60557715\n",
            "Iteration 135, loss = 0.60400073\n",
            "Iteration 136, loss = 0.60230163\n",
            "Iteration 137, loss = 0.60069550\n",
            "Iteration 138, loss = 0.59907107\n",
            "Iteration 139, loss = 0.59725708\n",
            "Iteration 140, loss = 0.59555745\n",
            "Iteration 141, loss = 0.59393756\n",
            "Iteration 142, loss = 0.59213330\n",
            "Iteration 143, loss = 0.59014907\n",
            "Iteration 144, loss = 0.58830636\n",
            "Iteration 145, loss = 0.58647527\n",
            "Iteration 146, loss = 0.58454209\n",
            "Iteration 147, loss = 0.58240152\n",
            "Iteration 148, loss = 0.58051330\n",
            "Iteration 149, loss = 0.57857982\n",
            "Iteration 150, loss = 0.57648450\n",
            "Iteration 151, loss = 0.57428017\n",
            "Iteration 152, loss = 0.57201927\n",
            "Iteration 153, loss = 0.56994491\n",
            "Iteration 154, loss = 0.56763763\n",
            "Iteration 155, loss = 0.56550675\n",
            "Iteration 156, loss = 0.56330529\n",
            "Iteration 157, loss = 0.56095391\n",
            "Iteration 158, loss = 0.55850337\n",
            "Iteration 159, loss = 0.55618924\n",
            "Iteration 160, loss = 0.55368107\n",
            "Iteration 161, loss = 0.55120761\n",
            "Iteration 162, loss = 0.54884064\n",
            "Iteration 163, loss = 0.54630175\n",
            "Iteration 164, loss = 0.54367394\n",
            "Iteration 165, loss = 0.54103137\n",
            "Iteration 166, loss = 0.53848918\n",
            "Iteration 167, loss = 0.53584054\n",
            "Iteration 168, loss = 0.53311654\n",
            "Iteration 169, loss = 0.53039013\n",
            "Iteration 170, loss = 0.52758740\n",
            "Iteration 171, loss = 0.52488169\n",
            "Iteration 172, loss = 0.52207128\n",
            "Iteration 173, loss = 0.51907784\n",
            "Iteration 174, loss = 0.51667714\n",
            "Iteration 175, loss = 0.51399300\n",
            "Iteration 176, loss = 0.51131817\n",
            "Iteration 177, loss = 0.50848904\n",
            "Iteration 178, loss = 0.50555829\n",
            "Iteration 179, loss = 0.50257534\n",
            "Iteration 180, loss = 0.49942712\n",
            "Iteration 181, loss = 0.49620430\n",
            "Iteration 182, loss = 0.49304136\n",
            "Iteration 183, loss = 0.48968755\n",
            "Iteration 184, loss = 0.48674893\n",
            "Iteration 185, loss = 0.48366004\n",
            "Iteration 186, loss = 0.48037777\n",
            "Iteration 187, loss = 0.47719497\n",
            "Iteration 188, loss = 0.47398499\n",
            "Iteration 189, loss = 0.47067260\n",
            "Iteration 190, loss = 0.46766149\n",
            "Iteration 191, loss = 0.46404263\n",
            "Iteration 192, loss = 0.46093265\n",
            "Iteration 193, loss = 0.45776756\n",
            "Iteration 194, loss = 0.45438944\n",
            "Iteration 195, loss = 0.45075331\n",
            "Iteration 196, loss = 0.44738549\n",
            "Iteration 197, loss = 0.44424743\n",
            "Iteration 198, loss = 0.44063791\n",
            "Iteration 199, loss = 0.43680130\n",
            "Iteration 200, loss = 0.43369723\n",
            "Iteration 201, loss = 0.43039488\n",
            "Iteration 202, loss = 0.42688332\n",
            "Iteration 203, loss = 0.42308042\n",
            "Iteration 204, loss = 0.41929512\n",
            "Iteration 205, loss = 0.41628326\n",
            "Iteration 206, loss = 0.41277334\n",
            "Iteration 207, loss = 0.40889576\n",
            "Iteration 208, loss = 0.40517502\n",
            "Iteration 209, loss = 0.40173547\n",
            "Iteration 210, loss = 0.39815436\n",
            "Iteration 211, loss = 0.39436853\n",
            "Iteration 212, loss = 0.39097320\n",
            "Iteration 213, loss = 0.38733435\n",
            "Iteration 214, loss = 0.38368377\n",
            "Iteration 215, loss = 0.38005569\n",
            "Iteration 216, loss = 0.37653111\n",
            "Iteration 217, loss = 0.37301576\n",
            "Iteration 218, loss = 0.36934740\n",
            "Iteration 219, loss = 0.36600926\n",
            "Iteration 220, loss = 0.36234020\n",
            "Iteration 221, loss = 0.35868642\n",
            "Iteration 222, loss = 0.35543128\n",
            "Iteration 223, loss = 0.35172595\n",
            "Iteration 224, loss = 0.34830111\n",
            "Iteration 225, loss = 0.34478693\n",
            "Iteration 226, loss = 0.34138308\n",
            "Iteration 227, loss = 0.33794286\n",
            "Iteration 228, loss = 0.33438479\n",
            "Iteration 229, loss = 0.33090795\n",
            "Iteration 230, loss = 0.32753714\n",
            "Iteration 231, loss = 0.32395632\n",
            "Iteration 232, loss = 0.32062276\n",
            "Iteration 233, loss = 0.31727878\n",
            "Iteration 234, loss = 0.31398509\n",
            "Iteration 235, loss = 0.31030336\n",
            "Iteration 236, loss = 0.30729861\n",
            "Iteration 237, loss = 0.30416457\n",
            "Iteration 238, loss = 0.30071381\n",
            "Iteration 239, loss = 0.29714898\n",
            "Iteration 240, loss = 0.29424242\n",
            "Iteration 241, loss = 0.29108025\n",
            "Iteration 242, loss = 0.28771348\n",
            "Iteration 243, loss = 0.28442025\n",
            "Iteration 244, loss = 0.28162308\n",
            "Iteration 245, loss = 0.27854240\n",
            "Iteration 246, loss = 0.27505763\n",
            "Iteration 247, loss = 0.27204689\n",
            "Iteration 248, loss = 0.26918450\n",
            "Iteration 249, loss = 0.26614906\n",
            "Iteration 250, loss = 0.26280275\n",
            "Iteration 251, loss = 0.25974947\n",
            "Iteration 252, loss = 0.25724441\n",
            "Iteration 253, loss = 0.25434163\n",
            "Iteration 254, loss = 0.25112075\n",
            "Iteration 255, loss = 0.24807518\n",
            "Iteration 256, loss = 0.24524232\n",
            "Iteration 257, loss = 0.24238839\n",
            "Iteration 258, loss = 0.23941505\n",
            "Iteration 259, loss = 0.23652406\n",
            "Iteration 260, loss = 0.23382810\n",
            "Iteration 261, loss = 0.23119678\n",
            "Iteration 262, loss = 0.22849577\n",
            "Iteration 263, loss = 0.22583676\n",
            "Iteration 264, loss = 0.22308601\n",
            "Iteration 265, loss = 0.22057864\n",
            "Iteration 266, loss = 0.21788749\n",
            "Iteration 267, loss = 0.21526283\n",
            "Iteration 268, loss = 0.21277598\n",
            "Iteration 269, loss = 0.21027189\n",
            "Iteration 270, loss = 0.20777191\n",
            "Iteration 271, loss = 0.20544485\n",
            "Iteration 272, loss = 0.20297281\n",
            "Iteration 273, loss = 0.20046283\n",
            "Iteration 274, loss = 0.19825925\n",
            "Iteration 275, loss = 0.19584057\n",
            "Iteration 276, loss = 0.19339749\n",
            "Iteration 277, loss = 0.19124581\n",
            "Iteration 278, loss = 0.18898038\n",
            "Iteration 279, loss = 0.18665164\n",
            "Iteration 280, loss = 0.18454102\n",
            "Iteration 281, loss = 0.18246857\n",
            "Iteration 282, loss = 0.18010730\n",
            "Iteration 283, loss = 0.17817080\n",
            "Iteration 284, loss = 0.17611582\n",
            "Iteration 285, loss = 0.17405436\n",
            "Iteration 286, loss = 0.17186202\n",
            "Iteration 287, loss = 0.16966031\n",
            "Iteration 288, loss = 0.16805380\n",
            "Iteration 289, loss = 0.16616763\n",
            "Iteration 290, loss = 0.16401126\n",
            "Iteration 291, loss = 0.16196893\n",
            "Iteration 292, loss = 0.16018787\n",
            "Iteration 293, loss = 0.15840540\n",
            "Iteration 294, loss = 0.15644385\n",
            "Iteration 295, loss = 0.15456222\n",
            "Iteration 296, loss = 0.15278639\n",
            "Iteration 297, loss = 0.15101646\n",
            "Iteration 298, loss = 0.14922884\n",
            "Iteration 299, loss = 0.14750402\n",
            "Iteration 300, loss = 0.14567393\n",
            "Iteration 301, loss = 0.14395199\n",
            "Iteration 302, loss = 0.14221641\n",
            "Iteration 303, loss = 0.14066488\n",
            "Iteration 304, loss = 0.13907105\n",
            "Iteration 305, loss = 0.13746576\n",
            "Iteration 306, loss = 0.13599912\n",
            "Iteration 307, loss = 0.13435444\n",
            "Iteration 308, loss = 0.13271874\n",
            "Iteration 309, loss = 0.13133216\n",
            "Iteration 310, loss = 0.12985452\n",
            "Iteration 311, loss = 0.12837566\n",
            "Iteration 312, loss = 0.12691478\n",
            "Iteration 313, loss = 0.12556471\n",
            "Iteration 314, loss = 0.12411523\n",
            "Iteration 315, loss = 0.12270450\n",
            "Iteration 316, loss = 0.12131716\n",
            "Iteration 317, loss = 0.11996495\n",
            "Iteration 318, loss = 0.11875098\n",
            "Iteration 319, loss = 0.11741788\n",
            "Iteration 320, loss = 0.11601653\n",
            "Iteration 321, loss = 0.11482031\n",
            "Iteration 322, loss = 0.11359181\n",
            "Iteration 323, loss = 0.11226690\n",
            "Iteration 324, loss = 0.11108679\n",
            "Iteration 325, loss = 0.10992804\n",
            "Iteration 326, loss = 0.10867219\n",
            "Iteration 327, loss = 0.10751382\n",
            "Iteration 328, loss = 0.10637804\n",
            "Iteration 329, loss = 0.10519983\n",
            "Iteration 330, loss = 0.10407921\n",
            "Iteration 331, loss = 0.10297307\n",
            "Iteration 332, loss = 0.10188969\n",
            "Iteration 333, loss = 0.10081017\n",
            "Iteration 334, loss = 0.09972631\n",
            "Iteration 335, loss = 0.09873586\n",
            "Iteration 336, loss = 0.09771852\n",
            "Iteration 337, loss = 0.09670730\n",
            "Iteration 338, loss = 0.09574225\n",
            "Iteration 339, loss = 0.09474294\n",
            "Iteration 340, loss = 0.09370623\n",
            "Iteration 341, loss = 0.09279515\n",
            "Iteration 342, loss = 0.09185453\n",
            "Iteration 343, loss = 0.09088602\n",
            "Iteration 344, loss = 0.09003354\n",
            "Iteration 345, loss = 0.08914333\n",
            "Iteration 346, loss = 0.08818909\n",
            "Iteration 347, loss = 0.08727001\n",
            "Iteration 348, loss = 0.08640584\n",
            "Iteration 349, loss = 0.08552505\n",
            "Iteration 350, loss = 0.08466762\n",
            "Iteration 351, loss = 0.08384371\n",
            "Iteration 352, loss = 0.08300771\n",
            "Iteration 353, loss = 0.08223041\n",
            "Iteration 354, loss = 0.08142052\n",
            "Iteration 355, loss = 0.08061935\n",
            "Iteration 356, loss = 0.07984740\n",
            "Iteration 357, loss = 0.07907292\n",
            "Iteration 358, loss = 0.07833376\n",
            "Iteration 359, loss = 0.07756991\n",
            "Iteration 360, loss = 0.07684289\n",
            "Iteration 361, loss = 0.07619213\n",
            "Iteration 362, loss = 0.07547153\n",
            "Iteration 363, loss = 0.07470074\n",
            "Iteration 364, loss = 0.07401147\n",
            "Iteration 365, loss = 0.07334097\n",
            "Iteration 366, loss = 0.07261819\n",
            "Iteration 367, loss = 0.07192292\n",
            "Iteration 368, loss = 0.07129987\n",
            "Iteration 369, loss = 0.07065162\n",
            "Iteration 370, loss = 0.06996834\n",
            "Iteration 371, loss = 0.06933656\n",
            "Iteration 372, loss = 0.06875186\n",
            "Iteration 373, loss = 0.06809181\n",
            "Iteration 374, loss = 0.06744336\n",
            "Iteration 375, loss = 0.06688143\n",
            "Iteration 376, loss = 0.06629819\n",
            "Iteration 377, loss = 0.06568939\n",
            "Iteration 378, loss = 0.06505423\n",
            "Iteration 379, loss = 0.06453529\n",
            "Iteration 380, loss = 0.06400782\n",
            "Iteration 381, loss = 0.06344740\n",
            "Iteration 382, loss = 0.06284542\n",
            "Iteration 383, loss = 0.06228834\n",
            "Iteration 384, loss = 0.06180292\n",
            "Iteration 385, loss = 0.06129412\n",
            "Iteration 386, loss = 0.06077968\n",
            "Iteration 387, loss = 0.06021492\n",
            "Iteration 388, loss = 0.05967275\n",
            "Iteration 389, loss = 0.05917563\n",
            "Iteration 390, loss = 0.05869194\n",
            "Iteration 391, loss = 0.05821869\n",
            "Iteration 392, loss = 0.05769227\n",
            "Iteration 393, loss = 0.05716022\n",
            "Iteration 394, loss = 0.05671904\n",
            "Iteration 395, loss = 0.05623242\n",
            "Iteration 396, loss = 0.05576113\n",
            "Iteration 397, loss = 0.05531086\n",
            "Iteration 398, loss = 0.05486926\n",
            "Iteration 399, loss = 0.05439940\n",
            "Iteration 400, loss = 0.05394418\n",
            "Iteration 401, loss = 0.05351590\n",
            "Iteration 402, loss = 0.05308133\n",
            "Iteration 403, loss = 0.05266301\n",
            "Iteration 404, loss = 0.05224408\n",
            "Iteration 405, loss = 0.05180828\n",
            "Iteration 406, loss = 0.05140088\n",
            "Iteration 407, loss = 0.05098894\n",
            "Iteration 408, loss = 0.05060864\n",
            "Iteration 409, loss = 0.05019978\n",
            "Iteration 410, loss = 0.04982405\n",
            "Iteration 411, loss = 0.04944224\n",
            "Iteration 412, loss = 0.04903165\n",
            "Iteration 413, loss = 0.04866355\n",
            "Iteration 414, loss = 0.04828831\n",
            "Iteration 415, loss = 0.04792390\n",
            "Iteration 416, loss = 0.04756487\n",
            "Iteration 417, loss = 0.04720927\n",
            "Iteration 418, loss = 0.04686271\n",
            "Iteration 419, loss = 0.04649317\n",
            "Iteration 420, loss = 0.04611575\n",
            "Iteration 421, loss = 0.04581173\n",
            "Iteration 422, loss = 0.04548766\n",
            "Iteration 423, loss = 0.04513927\n",
            "Iteration 424, loss = 0.04479121\n",
            "Iteration 425, loss = 0.04443941\n",
            "Iteration 426, loss = 0.04413440\n",
            "Iteration 427, loss = 0.04381149\n",
            "Iteration 428, loss = 0.04349955\n",
            "Iteration 429, loss = 0.04318362\n",
            "Iteration 430, loss = 0.04286881\n",
            "Iteration 431, loss = 0.04254341\n",
            "Iteration 432, loss = 0.04222137\n",
            "Iteration 433, loss = 0.04193785\n",
            "Iteration 434, loss = 0.04164870\n",
            "Iteration 435, loss = 0.04133646\n",
            "Iteration 436, loss = 0.04104682\n",
            "Iteration 437, loss = 0.04074013\n",
            "Iteration 438, loss = 0.04045092\n",
            "Iteration 439, loss = 0.04017601\n",
            "Iteration 440, loss = 0.03988989\n",
            "Iteration 441, loss = 0.03962843\n",
            "Iteration 442, loss = 0.03934841\n",
            "Iteration 443, loss = 0.03906219\n",
            "Iteration 444, loss = 0.03879229\n",
            "Iteration 445, loss = 0.03853706\n",
            "Iteration 446, loss = 0.03826036\n",
            "Iteration 447, loss = 0.03800273\n",
            "Iteration 448, loss = 0.03774091\n",
            "Iteration 449, loss = 0.03749733\n",
            "Iteration 450, loss = 0.03724403\n",
            "Iteration 451, loss = 0.03698849\n",
            "Iteration 452, loss = 0.03675046\n",
            "Iteration 453, loss = 0.03650111\n",
            "Iteration 454, loss = 0.03625453\n",
            "Iteration 455, loss = 0.03602126\n",
            "Iteration 456, loss = 0.03576982\n",
            "Iteration 457, loss = 0.03554957\n",
            "Iteration 458, loss = 0.03531133\n",
            "Iteration 459, loss = 0.03507007\n",
            "Iteration 460, loss = 0.03486036\n",
            "Iteration 461, loss = 0.03463248\n",
            "Iteration 462, loss = 0.03440505\n",
            "Iteration 463, loss = 0.03418641\n",
            "Iteration 464, loss = 0.03397249\n",
            "Iteration 465, loss = 0.03375212\n",
            "Iteration 466, loss = 0.03352971\n",
            "Iteration 467, loss = 0.03331544\n",
            "Iteration 468, loss = 0.03309730\n",
            "Iteration 469, loss = 0.03289828\n",
            "Iteration 470, loss = 0.03269881\n",
            "Iteration 471, loss = 0.03248290\n",
            "Iteration 472, loss = 0.03229468\n",
            "Iteration 473, loss = 0.03208734\n",
            "Iteration 474, loss = 0.03188778\n",
            "Iteration 475, loss = 0.03169436\n",
            "Iteration 476, loss = 0.03148815\n",
            "Iteration 477, loss = 0.03129790\n",
            "Iteration 478, loss = 0.03110252\n",
            "Iteration 479, loss = 0.03090925\n",
            "Iteration 480, loss = 0.03072854\n",
            "Iteration 481, loss = 0.03054813\n",
            "Iteration 482, loss = 0.03036368\n",
            "Iteration 483, loss = 0.03018476\n",
            "Iteration 484, loss = 0.03000219\n",
            "Iteration 485, loss = 0.02981950\n",
            "Iteration 486, loss = 0.02964230\n",
            "Iteration 487, loss = 0.02947305\n",
            "Iteration 488, loss = 0.02930017\n",
            "Iteration 489, loss = 0.02911478\n",
            "Iteration 490, loss = 0.02895003\n",
            "Iteration 491, loss = 0.02877463\n",
            "Iteration 492, loss = 0.02861436\n",
            "Iteration 493, loss = 0.02844862\n",
            "Iteration 494, loss = 0.02827376\n",
            "Iteration 495, loss = 0.02812120\n",
            "Iteration 496, loss = 0.02795529\n",
            "Iteration 497, loss = 0.02779302\n",
            "Iteration 498, loss = 0.02763154\n",
            "Iteration 499, loss = 0.02747935\n",
            "Iteration 500, loss = 0.02732412\n",
            "Iteration 501, loss = 0.02717430\n",
            "Iteration 502, loss = 0.02701860\n",
            "Iteration 503, loss = 0.02686216\n",
            "Iteration 504, loss = 0.02671332\n",
            "Iteration 505, loss = 0.02655995\n",
            "Iteration 506, loss = 0.02640976\n",
            "Iteration 507, loss = 0.02626826\n",
            "Iteration 508, loss = 0.02611577\n",
            "Iteration 509, loss = 0.02597330\n",
            "Iteration 510, loss = 0.02583473\n",
            "Iteration 511, loss = 0.02569281\n",
            "Iteration 512, loss = 0.02554980\n",
            "Iteration 513, loss = 0.02541350\n",
            "Iteration 514, loss = 0.02527342\n",
            "Iteration 515, loss = 0.02513745\n",
            "Iteration 516, loss = 0.02500242\n",
            "Iteration 517, loss = 0.02487393\n",
            "Iteration 518, loss = 0.02473916\n",
            "Iteration 519, loss = 0.02460729\n",
            "Iteration 520, loss = 0.02448255\n",
            "Iteration 521, loss = 0.02435130\n",
            "Iteration 522, loss = 0.02421842\n",
            "Iteration 523, loss = 0.02408590\n",
            "Iteration 524, loss = 0.02396428\n",
            "Iteration 525, loss = 0.02383634\n",
            "Iteration 526, loss = 0.02371551\n",
            "Iteration 527, loss = 0.02359295\n",
            "Iteration 528, loss = 0.02346955\n",
            "Iteration 529, loss = 0.02333894\n",
            "Iteration 530, loss = 0.02321968\n",
            "Iteration 531, loss = 0.02309571\n",
            "Iteration 532, loss = 0.02298288\n",
            "Iteration 533, loss = 0.02286788\n",
            "Iteration 534, loss = 0.02275122\n",
            "Iteration 535, loss = 0.02262757\n",
            "Iteration 536, loss = 0.02251725\n",
            "Iteration 537, loss = 0.02240420\n",
            "Iteration 538, loss = 0.02228503\n",
            "Iteration 539, loss = 0.02217678\n",
            "Iteration 540, loss = 0.02206423\n",
            "Iteration 541, loss = 0.02194623\n",
            "Iteration 542, loss = 0.02184817\n",
            "Iteration 543, loss = 0.02174928\n",
            "Iteration 544, loss = 0.02163430\n",
            "Iteration 545, loss = 0.02151748\n",
            "Iteration 546, loss = 0.02142162\n",
            "Iteration 547, loss = 0.02131527\n",
            "Iteration 548, loss = 0.02120828\n",
            "Iteration 549, loss = 0.02110019\n",
            "Iteration 550, loss = 0.02099271\n",
            "Iteration 551, loss = 0.02089638\n",
            "Iteration 552, loss = 0.02080188\n",
            "Iteration 553, loss = 0.02070146\n",
            "Iteration 554, loss = 0.02059497\n",
            "Iteration 555, loss = 0.02049846\n",
            "Iteration 556, loss = 0.02040436\n",
            "Iteration 557, loss = 0.02030775\n",
            "Iteration 558, loss = 0.02020711\n",
            "Iteration 559, loss = 0.02010628\n",
            "Iteration 560, loss = 0.02000635\n",
            "Iteration 561, loss = 0.01991493\n",
            "Iteration 562, loss = 0.01981776\n",
            "Iteration 563, loss = 0.01972728\n",
            "Iteration 564, loss = 0.01963570\n",
            "Iteration 565, loss = 0.01954018\n",
            "Iteration 566, loss = 0.01945434\n",
            "Iteration 567, loss = 0.01937052\n",
            "Iteration 568, loss = 0.01927645\n",
            "Iteration 569, loss = 0.01918183\n",
            "Iteration 570, loss = 0.01909460\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[0]\n",
            "[1]\n",
            "[1]\n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im7fp4WC1DHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60283074-e67b-4fe8-9f60-1e3dfbef17f7"
      },
      "source": [
        "dataset_wine = load_wine(return_X_y=True)\n",
        "dataset_wine, load_wine().target_names\n",
        "\n",
        "X_wine, y_wine = dataset_wine\n",
        "\n",
        "model_wine = MLPClassifier(hidden_layer_sizes=(X_wine.shape[0] + 1), activation='logistic', learning_rate_init=0.01, verbose=True, max_iter=2000)\n",
        "model_wine.fit(X_wine, y_wine)\n",
        "\n",
        "ye = model_wine.predict(X_wine)\n",
        "ye, accuracy_score(y_wine, ye)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.08616656\n",
            "Iteration 2, loss = 1.33519543\n",
            "Iteration 3, loss = 1.06930398\n",
            "Iteration 4, loss = 1.12788166\n",
            "Iteration 5, loss = 1.11986898\n",
            "Iteration 6, loss = 1.03455377\n",
            "Iteration 7, loss = 0.95103037\n",
            "Iteration 8, loss = 0.96027668\n",
            "Iteration 9, loss = 0.97868372\n",
            "Iteration 10, loss = 0.91130735\n",
            "Iteration 11, loss = 0.87732164\n",
            "Iteration 12, loss = 0.82662053\n",
            "Iteration 13, loss = 0.80329662\n",
            "Iteration 14, loss = 0.82255259\n",
            "Iteration 15, loss = 0.78579428\n",
            "Iteration 16, loss = 0.76317271\n",
            "Iteration 17, loss = 0.73979191\n",
            "Iteration 18, loss = 0.72308022\n",
            "Iteration 19, loss = 0.69672647\n",
            "Iteration 20, loss = 0.68259436\n",
            "Iteration 21, loss = 0.66022094\n",
            "Iteration 22, loss = 0.64323720\n",
            "Iteration 23, loss = 0.62521499\n",
            "Iteration 24, loss = 0.62925950\n",
            "Iteration 25, loss = 0.62670079\n",
            "Iteration 26, loss = 0.59095226\n",
            "Iteration 27, loss = 0.57251750\n",
            "Iteration 28, loss = 0.56303667\n",
            "Iteration 29, loss = 0.56351689\n",
            "Iteration 30, loss = 0.57730721\n",
            "Iteration 31, loss = 0.53150231\n",
            "Iteration 32, loss = 0.56309567\n",
            "Iteration 33, loss = 0.54860305\n",
            "Iteration 34, loss = 0.51692873\n",
            "Iteration 35, loss = 0.53176019\n",
            "Iteration 36, loss = 0.49097819\n",
            "Iteration 37, loss = 0.50117882\n",
            "Iteration 38, loss = 0.47316194\n",
            "Iteration 39, loss = 0.46940463\n",
            "Iteration 40, loss = 0.46108615\n",
            "Iteration 41, loss = 0.44407292\n",
            "Iteration 42, loss = 0.43437221\n",
            "Iteration 43, loss = 0.42758456\n",
            "Iteration 44, loss = 0.40686266\n",
            "Iteration 45, loss = 0.39131881\n",
            "Iteration 46, loss = 0.37620190\n",
            "Iteration 47, loss = 0.36416956\n",
            "Iteration 48, loss = 0.35681070\n",
            "Iteration 49, loss = 0.38349709\n",
            "Iteration 50, loss = 0.39865647\n",
            "Iteration 51, loss = 0.35908891\n",
            "Iteration 52, loss = 0.33140286\n",
            "Iteration 53, loss = 0.35343833\n",
            "Iteration 54, loss = 0.30214993\n",
            "Iteration 55, loss = 0.29768352\n",
            "Iteration 56, loss = 0.32876876\n",
            "Iteration 57, loss = 0.30602413\n",
            "Iteration 58, loss = 0.27344812\n",
            "Iteration 59, loss = 0.29604506\n",
            "Iteration 60, loss = 0.25967717\n",
            "Iteration 61, loss = 0.24667765\n",
            "Iteration 62, loss = 0.24701279\n",
            "Iteration 63, loss = 0.23961987\n",
            "Iteration 64, loss = 0.23618502\n",
            "Iteration 65, loss = 0.22295893\n",
            "Iteration 66, loss = 0.20469206\n",
            "Iteration 67, loss = 0.20670954\n",
            "Iteration 68, loss = 0.20992848\n",
            "Iteration 69, loss = 0.19682256\n",
            "Iteration 70, loss = 0.18475342\n",
            "Iteration 71, loss = 0.18745156\n",
            "Iteration 72, loss = 0.18376878\n",
            "Iteration 73, loss = 0.17267705\n",
            "Iteration 74, loss = 0.16902171\n",
            "Iteration 75, loss = 0.17053531\n",
            "Iteration 76, loss = 0.16233898\n",
            "Iteration 77, loss = 0.15433738\n",
            "Iteration 78, loss = 0.15270263\n",
            "Iteration 79, loss = 0.15524871\n",
            "Iteration 80, loss = 0.15494400\n",
            "Iteration 81, loss = 0.14712997\n",
            "Iteration 82, loss = 0.13934353\n",
            "Iteration 83, loss = 0.14207127\n",
            "Iteration 84, loss = 0.14183082\n",
            "Iteration 85, loss = 0.13657914\n",
            "Iteration 86, loss = 0.12982397\n",
            "Iteration 87, loss = 0.12786790\n",
            "Iteration 88, loss = 0.12947320\n",
            "Iteration 89, loss = 0.12953434\n",
            "Iteration 90, loss = 0.12898838\n",
            "Iteration 91, loss = 0.12304725\n",
            "Iteration 92, loss = 0.11931180\n",
            "Iteration 93, loss = 0.11687121\n",
            "Iteration 94, loss = 0.11976100\n",
            "Iteration 95, loss = 0.12435242\n",
            "Iteration 96, loss = 0.12793021\n",
            "Iteration 97, loss = 0.13135106\n",
            "Iteration 98, loss = 0.12139077\n",
            "Iteration 99, loss = 0.11476319\n",
            "Iteration 100, loss = 0.11037288\n",
            "Iteration 101, loss = 0.11934302\n",
            "Iteration 102, loss = 0.11905302\n",
            "Iteration 103, loss = 0.11569198\n",
            "Iteration 104, loss = 0.10604160\n",
            "Iteration 105, loss = 0.10334789\n",
            "Iteration 106, loss = 0.10276851\n",
            "Iteration 107, loss = 0.10347849\n",
            "Iteration 108, loss = 0.10526221\n",
            "Iteration 109, loss = 0.10378117\n",
            "Iteration 110, loss = 0.10370437\n",
            "Iteration 111, loss = 0.10144338\n",
            "Iteration 112, loss = 0.10050844\n",
            "Iteration 113, loss = 0.09599211\n",
            "Iteration 114, loss = 0.09461330\n",
            "Iteration 115, loss = 0.09424098\n",
            "Iteration 116, loss = 0.09411387\n",
            "Iteration 117, loss = 0.09400437\n",
            "Iteration 118, loss = 0.09301856\n",
            "Iteration 119, loss = 0.09388844\n",
            "Iteration 120, loss = 0.09432220\n",
            "Iteration 121, loss = 0.09474653\n",
            "Iteration 122, loss = 0.09426442\n",
            "Iteration 123, loss = 0.09226010\n",
            "Iteration 124, loss = 0.08856892\n",
            "Iteration 125, loss = 0.08594987\n",
            "Iteration 126, loss = 0.08358718\n",
            "Iteration 127, loss = 0.08380659\n",
            "Iteration 128, loss = 0.08515543\n",
            "Iteration 129, loss = 0.08949928\n",
            "Iteration 130, loss = 0.09617128\n",
            "Iteration 131, loss = 0.10575881\n",
            "Iteration 132, loss = 0.09805142\n",
            "Iteration 133, loss = 0.08345523\n",
            "Iteration 134, loss = 0.08197180\n",
            "Iteration 135, loss = 0.09371865\n",
            "Iteration 136, loss = 0.10654935\n",
            "Iteration 137, loss = 0.10262845\n",
            "Iteration 138, loss = 0.09040231\n",
            "Iteration 139, loss = 0.07772172\n",
            "Iteration 140, loss = 0.08751792\n",
            "Iteration 141, loss = 0.10076788\n",
            "Iteration 142, loss = 0.09992283\n",
            "Iteration 143, loss = 0.08548182\n",
            "Iteration 144, loss = 0.07825038\n",
            "Iteration 145, loss = 0.08045084\n",
            "Iteration 146, loss = 0.08532464\n",
            "Iteration 147, loss = 0.08368587\n",
            "Iteration 148, loss = 0.07879513\n",
            "Iteration 149, loss = 0.07509319\n",
            "Iteration 150, loss = 0.07169539\n",
            "Iteration 151, loss = 0.07245752\n",
            "Iteration 152, loss = 0.07599290\n",
            "Iteration 153, loss = 0.07801767\n",
            "Iteration 154, loss = 0.07584990\n",
            "Iteration 155, loss = 0.07114187\n",
            "Iteration 156, loss = 0.06969908\n",
            "Iteration 157, loss = 0.07043432\n",
            "Iteration 158, loss = 0.07189034\n",
            "Iteration 159, loss = 0.07167624\n",
            "Iteration 160, loss = 0.06893641\n",
            "Iteration 161, loss = 0.06635205\n",
            "Iteration 162, loss = 0.06679738\n",
            "Iteration 163, loss = 0.06926345\n",
            "Iteration 164, loss = 0.07066066\n",
            "Iteration 165, loss = 0.07045414\n",
            "Iteration 166, loss = 0.07026153\n",
            "Iteration 167, loss = 0.06710730\n",
            "Iteration 168, loss = 0.06417806\n",
            "Iteration 169, loss = 0.06515735\n",
            "Iteration 170, loss = 0.06713898\n",
            "Iteration 171, loss = 0.06865600\n",
            "Iteration 172, loss = 0.06751996\n",
            "Iteration 173, loss = 0.06541966\n",
            "Iteration 174, loss = 0.06261233\n",
            "Iteration 175, loss = 0.06092103\n",
            "Iteration 176, loss = 0.06114011\n",
            "Iteration 177, loss = 0.06251088\n",
            "Iteration 178, loss = 0.06415028\n",
            "Iteration 179, loss = 0.06501390\n",
            "Iteration 180, loss = 0.06488290\n",
            "Iteration 181, loss = 0.06299727\n",
            "Iteration 182, loss = 0.06007085\n",
            "Iteration 183, loss = 0.05824740\n",
            "Iteration 184, loss = 0.05837325\n",
            "Iteration 185, loss = 0.06011896\n",
            "Iteration 186, loss = 0.06273904\n",
            "Iteration 187, loss = 0.06660843\n",
            "Iteration 188, loss = 0.07090230\n",
            "Iteration 189, loss = 0.07036115\n",
            "Iteration 190, loss = 0.06272312\n",
            "Iteration 191, loss = 0.05699939\n",
            "Iteration 192, loss = 0.06098710\n",
            "Iteration 193, loss = 0.06719654\n",
            "Iteration 194, loss = 0.07301734\n",
            "Iteration 195, loss = 0.07861256\n",
            "Iteration 196, loss = 0.07486125\n",
            "Iteration 197, loss = 0.06058403\n",
            "Iteration 198, loss = 0.05745976\n",
            "Iteration 199, loss = 0.06747011\n",
            "Iteration 200, loss = 0.06762624\n",
            "Iteration 201, loss = 0.05968301\n",
            "Iteration 202, loss = 0.05345686\n",
            "Iteration 203, loss = 0.05539960\n",
            "Iteration 204, loss = 0.06209989\n",
            "Iteration 205, loss = 0.06923049\n",
            "Iteration 206, loss = 0.07015678\n",
            "Iteration 207, loss = 0.05915028\n",
            "Iteration 208, loss = 0.05401937\n",
            "Iteration 209, loss = 0.06021652\n",
            "Iteration 210, loss = 0.05768537\n",
            "Iteration 211, loss = 0.05228858\n",
            "Iteration 212, loss = 0.05135533\n",
            "Iteration 213, loss = 0.05548265\n",
            "Iteration 214, loss = 0.06334452\n",
            "Iteration 215, loss = 0.06832872\n",
            "Iteration 216, loss = 0.06462813\n",
            "Iteration 217, loss = 0.05274379\n",
            "Iteration 218, loss = 0.05756577\n",
            "Iteration 219, loss = 0.06322710\n",
            "Iteration 220, loss = 0.05398015\n",
            "Iteration 221, loss = 0.04897310\n",
            "Iteration 222, loss = 0.05512940\n",
            "Iteration 223, loss = 0.06656814\n",
            "Iteration 224, loss = 0.07692609\n",
            "Iteration 225, loss = 0.07156500\n",
            "Iteration 226, loss = 0.05554719\n",
            "Iteration 227, loss = 0.05969960\n",
            "Iteration 228, loss = 0.06774965\n",
            "Iteration 229, loss = 0.05739190\n",
            "Iteration 230, loss = 0.04976406\n",
            "Iteration 231, loss = 0.05996097\n",
            "Iteration 232, loss = 0.06577496\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2]), 0.9775280898876404)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JilSN-5x3CXd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e5850190-1137-4bbc-d11f-93d5a59a1a62"
      },
      "source": [
        "plt.plot(y_wine, 'bo')\n",
        "plt.plot(ye, 'ro')\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j8rwrx13cRW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "f672e95c-ed63-4ecf-ef01-34f971cf9fba"
      },
      "source": [
        "plt.plot(model_wine.loss_curve_, 'bo')\n",
        "plt.show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wd+u3LxouClRAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}